{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc50806-36bf-46f3-bbf9-e0e5c3f8b56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Detected] Target in train: 'WeightCategory', Label in sample_sub: 'WeightCategory'\n",
      "[Detected] ID in train: 'id', ID in test: 'id'\n",
      "[Info] Train male rows: 8851 | female rows: 8793\n",
      "[Info] Test  male rows: 2553 | female rows: 2672\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['NObeyesdad', '__source__'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 359\u001b[0m\n\u001b[1;32m    356\u001b[0m y_male_enc \u001b[38;5;241m=\u001b[39m y_enc[male_mask]\n\u001b[1;32m    357\u001b[0m test_male \u001b[38;5;241m=\u001b[39m test_features[test_male_mask]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 359\u001b[0m male_oof_labels, male_test_votes \u001b[38;5;241m=\u001b[39m train_group_and_predict(X_male, y_male_enc, test_male, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMALE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# -------- Run female model --------\u001b[39;00m\n\u001b[1;32m    362\u001b[0m X_female \u001b[38;5;241m=\u001b[39m X[female_mask]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 250\u001b[0m, in \u001b[0;36mtrain_group_and_predict\u001b[0;34m(X_grp, y_enc_grp, test_grp, group_name)\u001b[0m\n\u001b[1;32m    248\u001b[0m cols_to_use \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m X_grp\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;241m!=\u001b[39m gender_col]\n\u001b[1;32m    249\u001b[0m Xg \u001b[38;5;241m=\u001b[39m X_grp[cols_to_use]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 250\u001b[0m Xtestg \u001b[38;5;241m=\u001b[39m test_grp[cols_to_use]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# feature types\u001b[39;00m\n\u001b[1;32m    253\u001b[0m num_cols, cat_cols \u001b[38;5;241m=\u001b[39m infer_feature_types(Xg)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['NObeyesdad', '__source__'] not in index\""
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# End-to-end (Gender-Specific Models) + BMI feature:\n",
    "# Load → Detect ID/Target/Gender → Drop MTRANS/SMOKE → +BMI →\n",
    "# Split by Gender → Per-gender 5-Fold XGB (ES, multi:softmax) → Vote-avg → submission.csv\n",
    "# ==============================================\n",
    "\n",
    "# -------- Imports --------\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.base import clone\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# -------- Paths --------\n",
    "TRAIN_PATH = \"train_combined.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "SAMPLE_SUB_PATH = \"sample_submission.csv\"\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "N_JOBS = -1\n",
    "NUM_CLASSES_EXPECTED = 7   # used only for a sanity warning\n",
    "\n",
    "# -------- Your best params (EXACT, using multi:softmax) --------\n",
    "best_params2 = {\n",
    "    'booster' : 'gbtree',\n",
    "    'objective': 'multi:softmax',     # keep exactly as provided\n",
    "    'verbosity' : 0,\n",
    "    'tree_method' : \"hist\",\n",
    "    'grow_policy' : 'depthwise',\n",
    "    'max_depth' : 23,\n",
    "    'learning_rate': 0.050053726931263504,\n",
    "    'n_estimators': 982,\n",
    "    'gamma': 0.5354391952653927,\n",
    "    'min_child_weight': 2.561508074944465e-07,\n",
    "    'subsample': 0.7060590452456204,\n",
    "    'colsample_bytree': 0.37939433412123275,\n",
    "    'reg_alpha': 9.150224029846654e-08,\n",
    "    'reg_lambda': 5.671063656994295e-08,\n",
    "    'eval_metric': 'mlogloss'\n",
    "}\n",
    "\n",
    "# Map to xgb.train param names\n",
    "FIXED_XGB_PARAMS = {\n",
    "    \"booster\": best_params2[\"booster\"],\n",
    "    \"objective\": best_params2[\"objective\"],\n",
    "    \"eval_metric\": best_params2[\"eval_metric\"],\n",
    "    \"tree_method\": best_params2[\"tree_method\"],\n",
    "    \"grow_policy\": best_params2[\"grow_policy\"],\n",
    "    \"max_depth\": int(best_params2[\"max_depth\"]),\n",
    "    \"eta\": float(best_params2[\"learning_rate\"]),\n",
    "    \"gamma\": float(best_params2[\"gamma\"]),\n",
    "    \"min_child_weight\": float(best_params2[\"min_child_weight\"]),\n",
    "    \"subsample\": float(best_params2[\"subsample\"]),\n",
    "    \"colsample_bytree\": float(best_params2[\"colsample_bytree\"]),\n",
    "    \"alpha\": float(best_params2[\"reg_alpha\"]),\n",
    "    \"lambda\": float(best_params2[\"reg_lambda\"]),\n",
    "    \"verbosity\": int(best_params2[\"verbosity\"]),\n",
    "    \"nthread\": N_JOBS,\n",
    "    \"seed\": RANDOM_STATE,\n",
    "}\n",
    "NUM_BOOST_ROUND = int(best_params2[\"n_estimators\"])\n",
    "EARLY_STOP = 200\n",
    "\n",
    "# -------- Helpers --------\n",
    "def norm_col(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    return str(s).replace(\"\\ufeff\", \"\").strip().lower()\n",
    "\n",
    "def build_norm_map(cols):\n",
    "    fwd = {c: norm_col(c) for c in cols}\n",
    "    rev = {}\n",
    "    for orig, n in fwd.items():\n",
    "        if n not in rev:\n",
    "            rev[n] = orig\n",
    "    return fwd, rev\n",
    "\n",
    "def find_id_and_label(sample_sub, train, test):\n",
    "    ss_fwd, ss_rev = build_norm_map(sample_sub.columns)\n",
    "    tr_fwd, tr_rev = build_norm_map(train.columns)\n",
    "    te_fwd, te_rev = build_norm_map(test.columns)\n",
    "\n",
    "    ss_norm_cols = [ss_fwd[c] for c in sample_sub.columns]\n",
    "    tr_norm_cols = [tr_fwd[c] for c in train.columns]\n",
    "    te_norm_cols = [te_fwd[c] for c in test.columns]\n",
    "\n",
    "    id_norm, label_norm = None, None\n",
    "    if len(ss_norm_cols) == 2:\n",
    "        c1, c2 = ss_norm_cols\n",
    "        if c1 in te_norm_cols and c2 not in te_norm_cols:\n",
    "            id_norm, label_norm = c1, c2\n",
    "        elif c2 in te_norm_cols and c1 not in te_norm_cols:\n",
    "            id_norm, label_norm = c2, c1\n",
    "        else:\n",
    "            if c1 in te_norm_cols and c1 in tr_norm_cols:\n",
    "                id_norm, label_norm = c1, c2\n",
    "            elif c2 in te_norm_cols and c2 in tr_norm_cols:\n",
    "                id_norm, label_norm = c2, c1\n",
    "\n",
    "    if id_norm is None:\n",
    "        for cand in [\"id\", \"row_id\", \"index\", \"sample_id\"]:\n",
    "            if cand in te_norm_cols and cand in tr_norm_cols:\n",
    "                id_norm = cand\n",
    "                break\n",
    "\n",
    "    if label_norm is None:\n",
    "        candidates = [c for c in ss_norm_cols if c != id_norm]\n",
    "        if len(candidates) == 1:\n",
    "            label_norm = candidates[0]\n",
    "\n",
    "    if label_norm is None:\n",
    "        for cand in [\"label\", \"target\", \"class\", \"y\", \"weightcategory\", \"nobeyesdad\"]:\n",
    "            if cand in tr_norm_cols and cand != id_norm:\n",
    "                label_norm = cand\n",
    "                break\n",
    "\n",
    "    if label_norm is None:\n",
    "        for c in reversed(tr_norm_cols):\n",
    "            if c != id_norm:\n",
    "                label_norm = c\n",
    "                break\n",
    "\n",
    "    return {\n",
    "        \"id_norm\": id_norm,\n",
    "        \"label_norm\": label_norm,\n",
    "        \"id_in_train\": build_norm_map(train.columns)[1].get(id_norm, None),\n",
    "        \"id_in_test\": build_norm_map(test.columns)[1].get(id_norm, None),\n",
    "        \"id_in_sample\": build_norm_map(sample_sub.columns)[1].get(id_norm, None),\n",
    "        \"label_in_train\": build_norm_map(train.columns)[1].get(label_norm, None),\n",
    "        \"label_in_sample\": build_norm_map(sample_sub.columns)[1].get(label_norm, None),\n",
    "    }\n",
    "\n",
    "def infer_feature_types(df):\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def detect_gender_column(df):\n",
    "    candidates = [c for c in df.columns if norm_col(c) in {\"gender\",\"sex\"}]\n",
    "    if candidates:\n",
    "        return candidates[0]\n",
    "    for c in df.columns:\n",
    "        vals = pd.Series(df[c].dropna().astype(str).str.lower().str.strip()).unique()\n",
    "        if len(vals) in (2, 3):\n",
    "            if any(v.startswith(\"m\") for v in vals) and any(v.startswith(\"f\") for v in vals):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def split_by_gender(series):\n",
    "    s = series.astype(str).str.lower().str.strip()\n",
    "    male_mask = s.str.startswith((\"m\",\"1\",\"true\"))\n",
    "    female_mask = s.str.startswith((\"f\",\"0\",\"false\"))\n",
    "    if male_mask.sum()==0 and female_mask.sum()==0:\n",
    "        top = s.value_counts().index.tolist()\n",
    "        if len(top)>=2:\n",
    "            male_mask = s==top[0]\n",
    "            female_mask = s==top[1]\n",
    "    return male_mask, female_mask\n",
    "\n",
    "def add_bmi(df):\n",
    "    \"\"\"Compute BMI = Weight / (Height_m^2) with robust height-unit detection.\"\"\"\n",
    "    if (\"Weight\" in df.columns) and (\"Height\" in df.columns):\n",
    "        h = df[\"Height\"].astype(float)\n",
    "        height_m = np.where(h.median() > 3.0, h / 100.0, h)\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            bmi = df[\"Weight\"].astype(float) / (np.power(height_m, 2) + 1e-12)\n",
    "        df[\"BMI\"] = bmi.replace([np.inf, -np.inf], np.nan)\n",
    "    return df\n",
    "\n",
    "# -------- Load data --------\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "# Drop not-used columns\n",
    "for c in [\"MTRANS\",\"SMOKE\"]:\n",
    "    if c in train.columns: train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:  test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# >>> Add BMI feature (train & test) <<<\n",
    "train = add_bmi(train)\n",
    "test  = add_bmi(test)\n",
    "\n",
    "info = find_id_and_label(sample_sub, train, test)\n",
    "\n",
    "ID_COL_TRAIN   = info[\"id_in_train\"]\n",
    "ID_COL_TEST    = info[\"id_in_test\"]\n",
    "ID_COL_SAMPLE  = info[\"id_in_sample\"]\n",
    "TARGET_COL     = info[\"label_in_train\"]\n",
    "LABEL_COL_SAMP = info[\"label_in_sample\"]\n",
    "\n",
    "if TARGET_COL is None:\n",
    "    raise ValueError(\"Could not detect the target column. Please ensure sample_submission and train headers align.\")\n",
    "if LABEL_COL_SAMP is None:\n",
    "    ss_cols = list(sample_sub.columns)\n",
    "    others = [c for c in ss_cols if c != ID_COL_SAMPLE]\n",
    "    if len(others)==1:\n",
    "        LABEL_COL_SAMP = others[0]\n",
    "    else:\n",
    "        raise ValueError(\"Could not detect label header in sample_submission.csv\")\n",
    "\n",
    "print(f\"[Detected] Target in train: '{TARGET_COL}', Label in sample_sub: '{LABEL_COL_SAMP}'\")\n",
    "if ID_COL_TRAIN and ID_COL_TEST:\n",
    "    print(f\"[Detected] ID in train: '{ID_COL_TRAIN}', ID in test: '{ID_COL_TEST}'\")\n",
    "\n",
    "# -------- Target / Features --------\n",
    "y = train[TARGET_COL].copy()\n",
    "X = train.drop(columns=[TARGET_COL]).copy()\n",
    "if ID_COL_TRAIN in X.columns:\n",
    "    X.drop(columns=[ID_COL_TRAIN], inplace=True)\n",
    "\n",
    "test_features = test.copy()\n",
    "if ID_COL_TEST in test_features.columns:\n",
    "    test_ids = test_features[ID_COL_TEST].copy()\n",
    "    test_features.drop(columns=[ID_COL_TEST], inplace=True)\n",
    "else:\n",
    "    test_ids = pd.Series(np.arange(len(test_features)), name=\"id\")\n",
    "\n",
    "# -------- Label encode target --------\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "classes = list(le.classes_)\n",
    "if len(classes) != NUM_CLASSES_EXPECTED:\n",
    "    print(f\"[Warn] Expected {NUM_CLASSES_EXPECTED} classes but found {len(classes)}. Proceeding with {len(classes)}.\")\n",
    "\n",
    "# -------- Detect gender column and split --------\n",
    "gender_col = detect_gender_column(pd.concat([X, test_features], axis=0))\n",
    "if gender_col is None:\n",
    "    raise ValueError(\"Could not detect a gender column (e.g., 'Gender' or 'SEX'). Please confirm the column name.\")\n",
    "\n",
    "male_mask, female_mask = split_by_gender(train[gender_col])\n",
    "test_male_mask, test_female_mask = split_by_gender(test_features[gender_col])\n",
    "\n",
    "print(f\"[Info] Train male rows: {int(male_mask.sum())} | female rows: {int(female_mask.sum())}\")\n",
    "print(f\"[Info] Test  male rows: {int(test_male_mask.sum())} | female rows: {int(test_female_mask.sum())}\")\n",
    "\n",
    "# We drop gender col inside each group (it's constant after split)\n",
    "def train_group_and_predict(X_grp, y_enc_grp, test_grp, group_name):\n",
    "    # remove gender from features\n",
    "    cols_to_use = [c for c in X_grp.columns if c != gender_col]\n",
    "    Xg = X_grp[cols_to_use].copy()\n",
    "    Xtestg = test_grp[cols_to_use].copy()\n",
    "\n",
    "    # feature types\n",
    "    num_cols, cat_cols = infer_feature_types(Xg)\n",
    "\n",
    "    # Preprocessor: sparse OHE + scaling (sparse for safety with wide OHE)\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", ohe)\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=1.0\n",
    "    )\n",
    "\n",
    "    # ---- Use your best params here (multi:softmax + voting) ----\n",
    "    xgb_params = FIXED_XGB_PARAMS.copy()\n",
    "    xgb_params.update({\n",
    "        \"num_class\": len(classes),\n",
    "    })\n",
    "    local_num_boost_round = NUM_BOOST_ROUND\n",
    "    early_stop = EARLY_STOP\n",
    "\n",
    "    print(f\"\\n[{group_name}] Using best-found params:\")\n",
    "    pretty = {k: (round(v,6) if isinstance(v, float) else v) for k,v in best_params2.items()}\n",
    "    print(pretty)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    # OOF labels (since softmax returns class IDs)\n",
    "    oof_labels_group = np.full(len(Xg), -1, dtype=int)\n",
    "    # Test vote accumulator (class counts)\n",
    "    test_votes = np.zeros((len(Xtestg), len(classes)), dtype=np.float32)\n",
    "    fold_best = []\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(Xg, y_enc_grp), start=1):\n",
    "        print(f\"\\n[{group_name}] Fold {fold}/{N_FOLDS}\")\n",
    "        X_tr, X_va = Xg.iloc[tr_idx], Xg.iloc[va_idx]\n",
    "        y_tr, y_va = y_enc_grp[tr_idx], y_enc_grp[va_idx]\n",
    "\n",
    "        prep = clone(preprocessor)\n",
    "        Xtr = prep.fit_transform(X_tr)\n",
    "        Xva = prep.transform(X_va)\n",
    "\n",
    "        dtrain = xgb.DMatrix(Xtr, label=y_tr)\n",
    "        dval   = xgb.DMatrix(Xva, label=y_va)\n",
    "\n",
    "        bst = xgb.train(\n",
    "            params=xgb_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=local_num_boost_round,\n",
    "            evals=[(dtrain, \"train\"), (dval, \"valid\")],\n",
    "            early_stopping_rounds=early_stop,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        best_round = int(bst.best_iteration + 1)\n",
    "        fold_best.append(best_round)\n",
    "        print(f\"[{group_name}] Best iteration: {best_round}\")\n",
    "\n",
    "        # OOF predictions: class IDs directly\n",
    "        va_pred_ids = bst.predict(dval, iteration_range=(0, best_round)).astype(int)\n",
    "        oof_labels_group[va_idx] = va_pred_ids\n",
    "\n",
    "        acc = accuracy_score(y_va, va_pred_ids)\n",
    "        f1m = f1_score(y_va, va_pred_ids, average=\"macro\")\n",
    "        fold_metrics.append((acc, f1m))\n",
    "        print(f\"[{group_name}] Acc: {acc:.4f} | Macro F1: {f1m:.4f}\")\n",
    "\n",
    "        # test preds for this fold: vote-accumulate class IDs\n",
    "        Xtest_tf = prep.transform(Xtestg)\n",
    "        dtest = xgb.DMatrix(Xtest_tf)\n",
    "        test_pred_ids = bst.predict(dtest, iteration_range=(0, best_round)).astype(int)\n",
    "        # accumulate votes (as 1/N_FOLDS for tie-breaking consistency)\n",
    "        for cls in range(len(classes)):\n",
    "            test_votes[:, cls] += (test_pred_ids == cls).astype(np.float32) / N_FOLDS\n",
    "\n",
    "    # Group OOF summary\n",
    "    acc_g = accuracy_score(y_enc_grp, oof_labels_group)\n",
    "    f1_g = f1_score(y_enc_grp, oof_labels_group, average=\"macro\")\n",
    "    print(f\"\\n[{group_name}] OOF Accuracy: {acc_g:.4f} | Macro F1: {f1_g:.4f}\")\n",
    "    print(f\"[{group_name}] Best iterations: {fold_best} | Median: {int(np.median(fold_best))}\")\n",
    "\n",
    "    # Return OOF labels and test vote distribution\n",
    "    return oof_labels_group, test_votes\n",
    "\n",
    "# -------- Detect gender masks (after defining splitter) --------\n",
    "male_mask, female_mask = split_by_gender(train[gender_col])\n",
    "test_male_mask, test_female_mask = split_by_gender(test_features[gender_col])\n",
    "\n",
    "# -------- Run male model --------\n",
    "X_male = X[male_mask].reset_index(drop=True)\n",
    "y_male_enc = y_enc[male_mask]\n",
    "test_male = test_features[test_male_mask].reset_index(drop=True)\n",
    "\n",
    "male_oof_labels, male_test_votes = train_group_and_predict(X_male, y_male_enc, test_male, \"MALE\")\n",
    "\n",
    "# -------- Run female model --------\n",
    "X_female = X[female_mask].reset_index(drop=True)\n",
    "y_female_enc = y_enc[female_mask]\n",
    "test_female = test_features[test_female_mask].reset_index(drop=True)\n",
    "\n",
    "female_oof_labels, female_test_votes = train_group_and_predict(X_female, y_female_enc, test_female, \"FEMALE\")\n",
    "\n",
    "# -------- Combine OOF for overall report --------\n",
    "oof_labels_full = np.full(len(X), -1, dtype=int)\n",
    "oof_labels_full[male_mask.values] = male_oof_labels\n",
    "oof_labels_full[female_mask.values] = female_oof_labels\n",
    "\n",
    "oof_acc = accuracy_score(y_enc, oof_labels_full)\n",
    "oof_f1 = f1_score(y_enc, oof_labels_full, average=\"macro\")\n",
    "print(\"\\n========== OVERALL OOF ==========\")\n",
    "print(f\"OOF Accuracy: {oof_acc:.4f} | OOF Macro F1: {oof_f1:.4f}\")\n",
    "try:\n",
    "    print(\"\\nOOF Classification Report:\\n\",\n",
    "          classification_report(y_enc, oof_labels_full, target_names=classes))\n",
    "except Exception as e:\n",
    "    print(f\"[Info] Could not print classification report: {e}\")\n",
    "\n",
    "# -------- Build full test predictions by placing group votes back to original order --------\n",
    "test_votes_full = np.zeros((len(test_features), len(classes)), dtype=np.float32)\n",
    "test_votes_full[test_male_mask.values] = male_test_votes\n",
    "test_votes_full[test_female_mask.values] = female_test_votes\n",
    "\n",
    "test_pred_int = np.argmax(test_votes_full, axis=1)\n",
    "test_pred_labels = le.inverse_transform(test_pred_int)\n",
    "\n",
    "# -------- Build submission --------\n",
    "ss_cols = list(sample_sub.columns)\n",
    "ID_HEADER = ID_COL_SAMPLE if ID_COL_SAMPLE in sample_sub.columns else None\n",
    "LABEL_HEADER = LABEL_COL_SAMP\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "if ID_HEADER is not None and ID_COL_TEST in test.columns:\n",
    "    sub[ID_HEADER] = test[ID_COL_TEST].values\n",
    "elif ID_HEADER is not None:\n",
    "    sub[ID_HEADER] = np.arange(len(test_features))\n",
    "sub[LABEL_HEADER] = test_pred_labels\n",
    "\n",
    "# Reorder/complete to match sample_sub exactly\n",
    "for c in ss_cols:\n",
    "    if c not in sub.columns:\n",
    "        sub[c] = sample_sub[c].iloc[0] if len(sample_sub[c]) else None\n",
    "sub = sub[ss_cols]\n",
    "\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSaved submission.csv\")\n",
    "print(sub.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31273f48-5ffd-467a-ad61-14492d31980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Why vote-averaging? With multi:softmax, XGBoost’s predict returns class IDs. \n",
    "## We therefore aggregate per-fold votes for each class and take the final argmax. \n",
    "## Your OOF and test predictions remain fully cross-validated and gender-split, but now aligned with your exact hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
