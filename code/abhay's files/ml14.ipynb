{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54680a63-5e66-457c-8016-c1756aa4aa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Detected] Target in train: 'WeightCategory', Label in sample_sub: 'WeightCategory'\n",
      "[Detected] ID in train: 'id', ID in test: 'id'\n",
      "[Info] Train male rows: 7783 | female rows: 7750\n",
      "[Info] Test  male rows: 2553 | female rows: 2672\n",
      "\n",
      "[MALE] Fold 1/5\n",
      "[MALE] Best iteration: 302\n",
      "[MALE] Acc: 0.8915 | Macro F1: 0.7553\n",
      "\n",
      "[MALE] Fold 2/5\n",
      "[MALE] Best iteration: 316\n",
      "[MALE] Acc: 0.8915 | Macro F1: 0.7552\n",
      "\n",
      "[MALE] Fold 3/5\n",
      "[MALE] Best iteration: 390\n",
      "[MALE] Acc: 0.8838 | Macro F1: 0.7476\n",
      "\n",
      "[MALE] Fold 4/5\n",
      "[MALE] Best iteration: 353\n",
      "[MALE] Acc: 0.8869 | Macro F1: 0.7491\n",
      "\n",
      "[MALE] Fold 5/5\n",
      "[MALE] Best iteration: 309\n",
      "[MALE] Acc: 0.8830 | Macro F1: 0.7465\n",
      "\n",
      "[MALE] OOF Accuracy: 0.8873 | Macro F1: 0.7508\n",
      "[MALE] Best iterations: [302, 316, 390, 353, 309] | Median: 316\n",
      "\n",
      "[FEMALE] Fold 1/5\n",
      "[FEMALE] Best iteration: 318\n",
      "[FEMALE] Acc: 0.9174 | Macro F1: 0.7515\n",
      "\n",
      "[FEMALE] Fold 2/5\n",
      "[FEMALE] Best iteration: 314\n",
      "[FEMALE] Acc: 0.9219 | Macro F1: 0.7522\n",
      "\n",
      "[FEMALE] Fold 3/5\n",
      "[FEMALE] Best iteration: 298\n",
      "[FEMALE] Acc: 0.9090 | Macro F1: 0.7381\n",
      "\n",
      "[FEMALE] Fold 4/5\n",
      "[FEMALE] Best iteration: 341\n",
      "[FEMALE] Acc: 0.9161 | Macro F1: 0.7498\n",
      "\n",
      "[FEMALE] Fold 5/5\n",
      "[FEMALE] Best iteration: 333\n",
      "[FEMALE] Acc: 0.9239 | Macro F1: 0.7592\n",
      "\n",
      "[FEMALE] OOF Accuracy: 0.9177 | Macro F1: 0.7503\n",
      "[FEMALE] Best iterations: [318, 314, 298, 341, 333] | Median: 318\n",
      "\n",
      "========== OVERALL OOF ==========\n",
      "OOF Accuracy: 0.9025 | OOF Macro F1: 0.8927\n",
      "\n",
      "OOF Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.93      0.93      0.93      1870\n",
      "      Normal_Weight       0.88      0.89      0.88      2345\n",
      "     Obesity_Type_I       0.89      0.88      0.88      2207\n",
      "    Obesity_Type_II       0.97      0.97      0.97      2403\n",
      "   Obesity_Type_III       1.00      1.00      1.00      2983\n",
      " Overweight_Level_I       0.80      0.77      0.78      1844\n",
      "Overweight_Level_II       0.80      0.82      0.81      1881\n",
      "\n",
      "           accuracy                           0.90     15533\n",
      "          macro avg       0.89      0.89      0.89     15533\n",
      "       weighted avg       0.90      0.90      0.90     15533\n",
      "\n",
      "\n",
      "Saved submission.csv\n",
      "      id       WeightCategory\n",
      "0  15533     Obesity_Type_III\n",
      "1  15534   Overweight_Level_I\n",
      "2  15535  Overweight_Level_II\n",
      "3  15536      Obesity_Type_II\n",
      "4  15537        Normal_Weight\n",
      "5  15538  Insufficient_Weight\n",
      "6  15539       Obesity_Type_I\n",
      "7  15540     Obesity_Type_III\n",
      "8  15541   Overweight_Level_I\n",
      "9  15542       Obesity_Type_I\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# End-to-end (Gender-Specific Models) + BMI feature:\n",
    "# Load → Detect ID/Target/Gender → Drop MTRANS/SMOKE → +BMI →\n",
    "# Split by Gender → Per-gender 5-Fold XGB (ES) → Predict → submission.csv\n",
    "# ==============================================\n",
    "\n",
    "# -------- Imports --------\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.base import clone\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# -------- Paths --------\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "SAMPLE_SUB_PATH = \"sample_submission.csv\"\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "N_JOBS = -1\n",
    "NUM_CLASSES_EXPECTED = 7   # used only for a sanity warning\n",
    "\n",
    "# -------- Helpers --------\n",
    "def norm_col(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    return str(s).replace(\"\\ufeff\", \"\").strip().lower()\n",
    "\n",
    "def build_norm_map(cols):\n",
    "    fwd = {c: norm_col(c) for c in cols}\n",
    "    rev = {}\n",
    "    for orig, n in fwd.items():\n",
    "        if n not in rev:\n",
    "            rev[n] = orig\n",
    "    return fwd, rev\n",
    "\n",
    "def find_id_and_label(sample_sub, train, test):\n",
    "    ss_fwd, ss_rev = build_norm_map(sample_sub.columns)\n",
    "    tr_fwd, tr_rev = build_norm_map(train.columns)\n",
    "    te_fwd, te_rev = build_norm_map(test.columns)\n",
    "\n",
    "    ss_norm_cols = [ss_fwd[c] for c in sample_sub.columns]\n",
    "    tr_norm_cols = [tr_fwd[c] for c in train.columns]\n",
    "    te_norm_cols = [te_fwd[c] for c in test.columns]\n",
    "\n",
    "    id_norm, label_norm = None, None\n",
    "    if len(ss_norm_cols) == 2:\n",
    "        c1, c2 = ss_norm_cols\n",
    "        if c1 in te_norm_cols and c2 not in te_norm_cols:\n",
    "            id_norm, label_norm = c1, c2\n",
    "        elif c2 in te_norm_cols and c1 not in te_norm_cols:\n",
    "            id_norm, label_norm = c2, c1\n",
    "        else:\n",
    "            if c1 in te_norm_cols and c1 in tr_norm_cols:\n",
    "                id_norm, label_norm = c1, c2\n",
    "            elif c2 in te_norm_cols and c2 in tr_norm_cols:\n",
    "                id_norm, label_norm = c2, c1\n",
    "\n",
    "    if id_norm is None:\n",
    "        for cand in [\"id\", \"row_id\", \"index\", \"sample_id\"]:\n",
    "            if cand in te_norm_cols and cand in tr_norm_cols:\n",
    "                id_norm = cand\n",
    "                break\n",
    "\n",
    "    if label_norm is None:\n",
    "        candidates = [c for c in ss_norm_cols if c != id_norm]\n",
    "        if len(candidates) == 1:\n",
    "            label_norm = candidates[0]\n",
    "\n",
    "    if label_norm is None:\n",
    "        for cand in [\"label\", \"target\", \"class\", \"y\", \"weightcategory\", \"nobeyesdad\"]:\n",
    "            if cand in tr_norm_cols and cand != id_norm:\n",
    "                label_norm = cand\n",
    "                break\n",
    "\n",
    "    if label_norm is None:\n",
    "        for c in reversed(tr_norm_cols):\n",
    "            if c != id_norm:\n",
    "                label_norm = c\n",
    "                break\n",
    "\n",
    "    return {\n",
    "        \"id_norm\": id_norm,\n",
    "        \"label_norm\": label_norm,\n",
    "        \"id_in_train\": build_norm_map(train.columns)[1].get(id_norm, None),\n",
    "        \"id_in_test\": build_norm_map(test.columns)[1].get(id_norm, None),\n",
    "        \"id_in_sample\": build_norm_map(sample_sub.columns)[1].get(id_norm, None),\n",
    "        \"label_in_train\": build_norm_map(train.columns)[1].get(label_norm, None),\n",
    "        \"label_in_sample\": build_norm_map(sample_sub.columns)[1].get(label_norm, None),\n",
    "    }\n",
    "\n",
    "def infer_feature_types(df):\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def detect_gender_column(df):\n",
    "    # try common names\n",
    "    candidates = [c for c in df.columns if norm_col(c) in {\"gender\",\"sex\"}]\n",
    "    if candidates:\n",
    "        return candidates[0]\n",
    "    # weak fallback: any column with two unique values that looks like M/F\n",
    "    for c in df.columns:\n",
    "        vals = pd.Series(df[c].dropna().astype(str).str.lower().str.strip()).unique()\n",
    "        if len(vals) in (2, 3):\n",
    "            if any(v.startswith(\"m\") for v in vals) and any(v.startswith(\"f\") for v in vals):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def split_by_gender(series):\n",
    "    s = series.astype(str).str.lower().str.strip()\n",
    "    male_mask = s.str.startswith((\"m\",\"1\",\"true\"))\n",
    "    female_mask = s.str.startswith((\"f\",\"0\",\"false\"))\n",
    "    if male_mask.sum()==0 and female_mask.sum()==0:\n",
    "        top = s.value_counts().index.tolist()\n",
    "        if len(top)>=2:\n",
    "            male_mask = s==top[0]\n",
    "            female_mask = s==top[1]\n",
    "    return male_mask, female_mask\n",
    "\n",
    "def add_bmi(df):\n",
    "    \"\"\"Compute BMI = Weight / (Height_m^2) with robust height-unit detection.\"\"\"\n",
    "    if (\"Weight\" in df.columns) and (\"Height\" in df.columns):\n",
    "        h = df[\"Height\"].astype(float)\n",
    "        # If median height > 3 assume cm and convert to meters\n",
    "        height_m = np.where(h.median() > 3.0, h / 100.0, h)\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            bmi = df[\"Weight\"].astype(float) / (np.power(height_m, 2) + 1e-12)\n",
    "        df[\"BMI\"] = bmi.replace([np.inf, -np.inf], np.nan)\n",
    "    return df\n",
    "\n",
    "# -------- Load data --------\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "# Drop not-used columns\n",
    "for c in [\"MTRANS\",\"SMOKE\"]:\n",
    "    if c in train.columns: train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:  test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# >>> Add BMI feature (train & test) <<<\n",
    "train = add_bmi(train)\n",
    "test  = add_bmi(test)\n",
    "\n",
    "info = find_id_and_label(sample_sub, train, test)\n",
    "\n",
    "ID_COL_TRAIN   = info[\"id_in_train\"]\n",
    "ID_COL_TEST    = info[\"id_in_test\"]\n",
    "ID_COL_SAMPLE  = info[\"id_in_sample\"]\n",
    "TARGET_COL     = info[\"label_in_train\"]\n",
    "LABEL_COL_SAMP = info[\"label_in_sample\"]\n",
    "\n",
    "if TARGET_COL is None:\n",
    "    raise ValueError(\"Could not detect the target column. Please ensure sample_submission and train headers align.\")\n",
    "if LABEL_COL_SAMP is None:\n",
    "    ss_cols = list(sample_sub.columns)\n",
    "    others = [c for c in ss_cols if c != ID_COL_SAMPLE]\n",
    "    if len(others)==1:\n",
    "        LABEL_COL_SAMP = others[0]\n",
    "    else:\n",
    "        raise ValueError(\"Could not detect label header in sample_submission.csv\")\n",
    "\n",
    "print(f\"[Detected] Target in train: '{TARGET_COL}', Label in sample_sub: '{LABEL_COL_SAMP}'\")\n",
    "if ID_COL_TRAIN and ID_COL_TEST:\n",
    "    print(f\"[Detected] ID in train: '{ID_COL_TRAIN}', ID in test: '{ID_COL_TEST}'\")\n",
    "\n",
    "# -------- Target / Features --------\n",
    "y = train[TARGET_COL].copy()\n",
    "X = train.drop(columns=[TARGET_COL]).copy()\n",
    "if ID_COL_TRAIN in X.columns:\n",
    "    X.drop(columns=[ID_COL_TRAIN], inplace=True)\n",
    "\n",
    "test_features = test.copy()\n",
    "if ID_COL_TEST in test_features.columns:\n",
    "    test_ids = test_features[ID_COL_TEST].copy()\n",
    "    test_features.drop(columns=[ID_COL_TEST], inplace=True)\n",
    "else:\n",
    "    test_ids = pd.Series(np.arange(len(test_features)), name=\"id\")\n",
    "\n",
    "# -------- Label encode target --------\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "classes = list(le.classes_)\n",
    "if len(classes) != NUM_CLASSES_EXPECTED:\n",
    "    print(f\"[Warn] Expected {NUM_CLASSES_EXPECTED} classes but found {len(classes)}. Proceeding.\")\n",
    "\n",
    "# -------- Detect gender column and split --------\n",
    "gender_col = detect_gender_column(pd.concat([X, test_features], axis=0))\n",
    "if gender_col is None:\n",
    "    raise ValueError(\"Could not detect a gender column (e.g., 'Gender' or 'SEX'). Please confirm the column name.\")\n",
    "\n",
    "male_mask, female_mask = split_by_gender(train[gender_col])\n",
    "test_male_mask, test_female_mask = split_by_gender(test_features[gender_col])\n",
    "\n",
    "print(f\"[Info] Train male rows: {int(male_mask.sum())} | female rows: {int(female_mask.sum())}\")\n",
    "print(f\"[Info] Test  male rows: {int(test_male_mask.sum())} | female rows: {int(test_female_mask.sum())}\")\n",
    "\n",
    "# We drop gender col inside each group (it's constant after split)\n",
    "def train_group_and_predict(X_grp, y_enc_grp, test_grp, group_name):\n",
    "    # remove gender from features\n",
    "    cols_to_use = [c for c in X_grp.columns if c != gender_col]\n",
    "    Xg = X_grp[cols_to_use].copy()\n",
    "    Xtestg = test_grp[cols_to_use].copy()\n",
    "\n",
    "    # feature types\n",
    "    num_cols, cat_cols = infer_feature_types(Xg)\n",
    "\n",
    "    # Preprocessor: sparse OHE (works with xgb DMatrix)\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", ohe)\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=1.0\n",
    "    )\n",
    "\n",
    "    # XGBoost params\n",
    "    xgb_params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": len(classes),\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"max_depth\": 6,\n",
    "        \"min_child_weight\": 2,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"lambda\": 1.0,\n",
    "        \"alpha\": 0.0,\n",
    "        \"eta\": 0.03,\n",
    "        \"nthread\": N_JOBS,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "    }\n",
    "    NUM_BOOST_ROUND = 20000\n",
    "    EARLY_STOP = 200\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof_group = np.zeros((len(Xg), len(classes)), dtype=np.float32)\n",
    "    test_group_pred = np.zeros((len(Xtestg), len(classes)), dtype=np.float32)\n",
    "    fold_best = []\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(Xg, y_enc_grp), start=1):\n",
    "        print(f\"\\n[{group_name}] Fold {fold}/{N_FOLDS}\")\n",
    "        X_tr, X_va = Xg.iloc[tr_idx], Xg.iloc[va_idx]\n",
    "        y_tr, y_va = y_enc_grp[tr_idx], y_enc_grp[va_idx]\n",
    "\n",
    "        prep = clone(preprocessor)\n",
    "        Xtr = prep.fit_transform(X_tr)\n",
    "        Xva = prep.transform(X_va)\n",
    "\n",
    "        dtrain = xgb.DMatrix(Xtr, label=y_tr)\n",
    "        dval   = xgb.DMatrix(Xva, label=y_va)\n",
    "\n",
    "        bst = xgb.train(\n",
    "            params=xgb_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=NUM_BOOST_ROUND,\n",
    "            evals=[(dtrain, \"train\"), (dval, \"valid\")],\n",
    "            early_stopping_rounds=EARLY_STOP,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        best_round = int(bst.best_iteration + 1)\n",
    "        fold_best.append(best_round)\n",
    "        print(f\"[{group_name}] Best iteration: {best_round}\")\n",
    "\n",
    "        oof_proba = bst.predict(dval, iteration_range=(0, best_round))\n",
    "        oof_group[va_idx] = oof_proba\n",
    "        oof_labels = np.argmax(oof_proba, axis=1)\n",
    "        acc = accuracy_score(y_va, oof_labels)\n",
    "        f1m = f1_score(y_va, oof_labels, average=\"macro\")\n",
    "        fold_metrics.append((acc, f1m))\n",
    "        print(f\"[{group_name}] Acc: {acc:.4f} | Macro F1: {f1m:.4f}\")\n",
    "\n",
    "        # test preds for this fold\n",
    "        Xtest_tf = prep.transform(Xtestg)\n",
    "        dtest = xgb.DMatrix(Xtest_tf)\n",
    "        test_group_pred += bst.predict(dtest, iteration_range=(0, best_round)) / N_FOLDS\n",
    "\n",
    "    # OOF summary for the group\n",
    "    oof_argmax = np.argmax(oof_group, axis=1)\n",
    "    acc_g = accuracy_score(y_enc_grp, oof_argmax)\n",
    "    f1_g = f1_score(y_enc_grp, oof_argmax, average=\"macro\")\n",
    "    print(f\"\\n[{group_name}] OOF Accuracy: {acc_g:.4f} | Macro F1: {f1_g:.4f}\")\n",
    "    print(f\"[{group_name}] Best iterations: {fold_best} | Median: {int(np.median(fold_best))}\")\n",
    "\n",
    "    return oof_group, test_group_pred\n",
    "\n",
    "# -------- Run male model --------\n",
    "X_male = X[male_mask].reset_index(drop=True)\n",
    "y_male_enc = y_enc[male_mask]\n",
    "test_male = test_features[test_male_mask].reset_index(drop=True)\n",
    "\n",
    "male_oof, male_test_pred = train_group_and_predict(X_male, y_male_enc, test_male, \"MALE\")\n",
    "\n",
    "# -------- Run female model --------\n",
    "X_female = X[female_mask].reset_index(drop=True)\n",
    "y_female_enc = y_enc[female_mask]\n",
    "test_female = test_features[test_female_mask].reset_index(drop=True)\n",
    "\n",
    "female_oof, female_test_pred = train_group_and_predict(X_female, y_female_enc, test_female, \"FEMALE\")\n",
    "\n",
    "# -------- Combine OOF for overall report --------\n",
    "oof_full = np.zeros((len(X), len(classes)), dtype=np.float32)\n",
    "oof_full[male_mask.values] = male_oof\n",
    "oof_full[female_mask.values] = female_oof\n",
    "\n",
    "oof_labels = np.argmax(oof_full, axis=1)\n",
    "oof_acc = accuracy_score(y_enc, oof_labels)\n",
    "oof_f1 = f1_score(y_enc, oof_labels, average=\"macro\")\n",
    "print(\"\\n========== OVERALL OOF ==========\")\n",
    "print(f\"OOF Accuracy: {oof_acc:.4f} | OOF Macro F1: {oof_f1:.4f}\")\n",
    "try:\n",
    "    print(\"\\nOOF Classification Report:\\n\",\n",
    "          classification_report(y_enc, oof_labels, target_names=classes))\n",
    "except Exception as e:\n",
    "    print(f\"[Info] Could not print classification report: {e}\")\n",
    "\n",
    "# -------- Build full test predictions by placing group preds back to original order --------\n",
    "test_pred_proba = np.zeros((len(test_features), len(classes)), dtype=np.float32)\n",
    "test_pred_proba[test_male_mask.values] = male_test_pred\n",
    "test_pred_proba[test_female_mask.values] = female_test_pred\n",
    "\n",
    "test_pred_int = np.argmax(test_pred_proba, axis=1)\n",
    "test_pred_labels = le.inverse_transform(test_pred_int)\n",
    "\n",
    "# -------- Build submission --------\n",
    "ss_cols = list(sample_sub.columns)\n",
    "ID_HEADER = ID_COL_SAMPLE if ID_COL_SAMPLE in sample_sub.columns else None\n",
    "LABEL_HEADER = LABEL_COL_SAMP\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "if ID_HEADER is not None and ID_COL_TEST in test.columns:\n",
    "    sub[ID_HEADER] = test[ID_COL_TEST].values\n",
    "elif ID_HEADER is not None:\n",
    "    sub[ID_HEADER] = np.arange(len(test_features))\n",
    "sub[LABEL_HEADER] = test_pred_labels\n",
    "\n",
    "# Reorder/complete to match sample_sub exactly\n",
    "for c in ss_cols:\n",
    "    if c not in sub.columns:\n",
    "        sub[c] = sample_sub[c].iloc[0] if len(sample_sub[c]) else None\n",
    "sub = sub[ss_cols]\n",
    "\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSaved submission.csv\")\n",
    "print(sub.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06f86972-0766-4e01-b975-cc4257fb1a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 14:36:15,894] A new study created in memory with name: no-name-b02d92ff-280b-47c7-8868-6387fde293d8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: males=7783, females=7750\n",
      "Test:  males=2553, females=2672\n",
      "\n",
      "[MALE] Robust tuning: 30 trials × 3 seeds × 5-fold CV\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2301b0996f274dd6b01e58b7852234f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 14:36:54,931] Trial 0 finished with value: 0.8880036477329076 and parameters: {'eta': 0.03144161771601719, 'max_depth': 5, 'min_child_weight': 2.7951119663071418, 'subsample': 0.8422573187846457, 'colsample_bytree': 0.908021857824381, 'reg_lambda': 1.1384175276155883, 'reg_alpha': 0.21000918009944766, 'gamma': 0.1504247093899292, 'max_delta_step': 1.7038570039360976, 'num_boost_round': 3000}. Best is trial 0 with value: 0.8880036477329076.\n",
      "[I 2025-10-23 14:37:21,757] Trial 1 finished with value: 0.88894612549456 and parameters: {'eta': 0.04797006861249111, 'max_depth': 7, 'min_child_weight': 2.3862238644362206, 'subsample': 0.7975882567788476, 'colsample_bytree': 0.7690008946376732, 'reg_lambda': 1.094292488954696, 'reg_alpha': 0.008815840511338857, 'gamma': 0.25669800208742877, 'max_delta_step': 1.2436470318358337, 'num_boost_round': 4000}. Best is trial 1 with value: 0.88894612549456.\n",
      "[I 2025-10-23 14:39:00,817] Trial 2 finished with value: 0.8885601636526639 and parameters: {'eta': 0.033606655918346905, 'max_depth': 8, 'min_child_weight': 1.3459171162061252, 'subsample': 0.7913309392007026, 'colsample_bytree': 0.8214617362130544, 'reg_lambda': 1.9512307018620043, 'reg_alpha': 0.468310213602027, 'gamma': 0.34542493159194054, 'max_delta_step': 0.10587413845376115, 'num_boost_round': 4000}. Best is trial 1 with value: 0.88894612549456.\n",
      "[I 2025-10-23 14:39:27,637] Trial 3 finished with value: 0.8882181198985811 and parameters: {'eta': 0.05690060966582504, 'max_depth': 8, 'min_child_weight': 4.147024134316719, 'subsample': 0.9263934894349444, 'colsample_bytree': 0.7022056818743544, 'reg_lambda': 0.8261909610447703, 'reg_alpha': 0.062243716949504854, 'gamma': 0.052101230816002625, 'max_delta_step': 1.8707577962842703, 'num_boost_round': 10000}. Best is trial 1 with value: 0.88894612549456.\n",
      "[I 2025-10-23 14:40:22,026] Trial 4 finished with value: 0.8888176458253877 and parameters: {'eta': 0.021298058662448424, 'max_depth': 8, 'min_child_weight': 4.221208580656999, 'subsample': 0.9020525445274606, 'colsample_bytree': 0.8386139372629683, 'reg_lambda': 2.238447301528719, 'reg_alpha': 0.295945100213747, 'gamma': 0.259499548871146, 'max_delta_step': 0.6537439859666749, 'num_boost_round': 12000}. Best is trial 1 with value: 0.88894612549456.\n",
      "[I 2025-10-23 14:40:37,244] Trial 5 pruned. \n",
      "[I 2025-10-23 14:40:48,229] Trial 6 pruned. \n",
      "[I 2025-10-23 14:41:04,791] Trial 7 pruned. \n",
      "[I 2025-10-23 14:41:18,793] Trial 8 pruned. \n",
      "[I 2025-10-23 14:41:27,698] Trial 9 pruned. \n",
      "[I 2025-10-23 14:41:38,124] Trial 10 pruned. \n",
      "[I 2025-10-23 14:42:12,043] Trial 11 finished with value: 0.8887747458887332 and parameters: {'eta': 0.0421534114003866, 'max_depth': 7, 'min_child_weight': 3.3764998816797216, 'subsample': 0.8091699365701024, 'colsample_bytree': 0.8206406791125388, 'reg_lambda': 1.530926422520302, 'reg_alpha': 0.3330603039026249, 'gamma': 0.21846424701493142, 'max_delta_step': 0.4542366513546532, 'num_boost_round': 6000}. Best is trial 1 with value: 0.88894612549456.\n",
      "[I 2025-10-23 14:42:21,973] Trial 12 pruned. \n",
      "[I 2025-10-23 14:43:09,686] Trial 13 finished with value: 0.8894170616817987 and parameters: {'eta': 0.024953858999340765, 'max_depth': 7, 'min_child_weight': 3.7482960384081707, 'subsample': 0.7793179058458695, 'colsample_bytree': 0.785607966601573, 'reg_lambda': 2.3756202453605435, 'reg_alpha': 0.32243979387125415, 'gamma': 0.14945799737133894, 'max_delta_step': 0.6141920705262955, 'num_boost_round': 2000}. Best is trial 13 with value: 0.8894170616817987.\n",
      "[I 2025-10-23 14:43:40,306] Trial 14 pruned. \n",
      "[I 2025-10-23 14:43:52,792] Trial 15 pruned. \n",
      "[I 2025-10-23 14:44:22,471] Trial 16 finished with value: 0.8895886063932187 and parameters: {'eta': 0.04846505000215754, 'max_depth': 7, 'min_child_weight': 2.918361644831322, 'subsample': 0.8132267926559592, 'colsample_bytree': 0.7559932838553789, 'reg_lambda': 1.6449663834471437, 'reg_alpha': 0.27502101084477665, 'gamma': 0.14721930211811146, 'max_delta_step': 0.624651143585174, 'num_boost_round': 5000}. Best is trial 16 with value: 0.8895886063932187.\n",
      "[I 2025-10-23 14:44:31,431] Trial 17 pruned. \n",
      "[I 2025-10-23 14:45:28,229] Trial 18 finished with value: 0.8884750242017283 and parameters: {'eta': 0.023566161669811958, 'max_depth': 7, 'min_child_weight': 2.970282841333884, 'subsample': 0.7800356337057687, 'colsample_bytree': 0.8086516324376432, 'reg_lambda': 2.4015185625850726, 'reg_alpha': 0.39570658826409666, 'gamma': 0.08691344531465794, 'max_delta_step': 0.3366986738982023, 'num_boost_round': 8000}. Best is trial 16 with value: 0.8895886063932187.\n",
      "[I 2025-10-23 14:45:38,352] Trial 19 pruned. \n",
      "[I 2025-10-23 14:46:44,914] Trial 20 finished with value: 0.88885985782207 and parameters: {'eta': 0.028434946633099513, 'max_depth': 7, 'min_child_weight': 3.6438010442231397, 'subsample': 0.8591793807409553, 'colsample_bytree': 0.7536347639424053, 'reg_lambda': 1.8203760029802325, 'reg_alpha': 0.13444838268725023, 'gamma': 0.09553647567735261, 'max_delta_step': 0.20948022315216386, 'num_boost_round': 2000}. Best is trial 16 with value: 0.8895886063932187.\n",
      "[I 2025-10-23 14:46:54,424] Trial 21 pruned. \n",
      "[I 2025-10-23 14:47:03,777] Trial 22 pruned. \n",
      "[I 2025-10-23 14:47:13,882] Trial 23 pruned. \n",
      "[I 2025-10-23 14:47:26,587] Trial 24 pruned. \n",
      "[I 2025-10-23 14:47:36,471] Trial 25 pruned. \n",
      "[I 2025-10-23 14:47:45,136] Trial 26 pruned. \n",
      "[I 2025-10-23 14:47:57,194] Trial 27 pruned. \n",
      "[I 2025-10-23 14:48:06,863] Trial 28 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 14:48:20,162] A new study created in memory with name: no-name-a9b2c401-980a-4014-b60e-79ea6e83a6f3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 14:48:20,146] Trial 29 pruned. \n",
      "[MALE] Best CV Acc: 0.88959\n",
      "[MALE] Best params:\n",
      "{'eta': 0.04846505000215754, 'max_depth': 7, 'min_child_weight': 2.918361644831322, 'subsample': 0.8132267926559592, 'colsample_bytree': 0.7559932838553789, 'reg_lambda': 1.6449663834471437, 'reg_alpha': 0.27502101084477665, 'gamma': 0.14721930211811146, 'max_delta_step': 0.624651143585174, 'num_boost_round': 5000}\n",
      "\n",
      "[FEMALE] Robust tuning: 30 trials × 3 seeds × 5-fold CV\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21649ecbc2e5474a8449a39368c3d5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 14:48:50,332] Trial 0 finished with value: 0.916258064516129 and parameters: {'eta': 0.048688319264963274, 'max_depth': 7, 'min_child_weight': 4.282458257461285, 'subsample': 0.880604318698849, 'colsample_bytree': 0.8254782605511541, 'reg_lambda': 1.6682280071337459, 'reg_alpha': 0.22086085323422852, 'gamma': 0.14432055917399045, 'max_delta_step': 2.9614566154582986, 'num_boost_round': 6000}. Best is trial 0 with value: 0.916258064516129.\n",
      "[I 2025-10-23 14:49:32,215] Trial 1 finished with value: 0.9164731182795699 and parameters: {'eta': 0.02571027148691823, 'max_depth': 8, 'min_child_weight': 2.7489816751332086, 'subsample': 0.7657517572063801, 'colsample_bytree': 0.7827436783781312, 'reg_lambda': 0.946215979389272, 'reg_alpha': 0.5907662282782293, 'gamma': 0.3385902365745781, 'max_delta_step': 2.7321993822920803, 'num_boost_round': 8000}. Best is trial 1 with value: 0.9164731182795699.\n",
      "[I 2025-10-23 14:49:57,426] Trial 2 finished with value: 0.9183225806451613 and parameters: {'eta': 0.05407598799308484, 'max_depth': 5, 'min_child_weight': 3.599924168441532, 'subsample': 0.8343705571353311, 'colsample_bytree': 0.7135498423588592, 'reg_lambda': 2.422207760688385, 'reg_alpha': 0.16796998220071638, 'gamma': 0.37925125854000435, 'max_delta_step': 2.6115018898868687, 'num_boost_round': 7000}. Best is trial 2 with value: 0.9183225806451613.\n",
      "[I 2025-10-23 14:50:26,888] Trial 3 finished with value: 0.9187956989247312 and parameters: {'eta': 0.05527430729020161, 'max_depth': 5, 'min_child_weight': 2.4199496273046437, 'subsample': 0.82259094898557, 'colsample_bytree': 0.8419333818822574, 'reg_lambda': 1.916718152401423, 'reg_alpha': 0.2676341929638112, 'gamma': 0.01316871814073961, 'max_delta_step': 0.9112719629036926, 'num_boost_round': 12000}. Best is trial 3 with value: 0.9187956989247312.\n",
      "[I 2025-10-23 14:51:00,996] Trial 4 finished with value: 0.9171182795698924 and parameters: {'eta': 0.036373358589318, 'max_depth': 6, 'min_child_weight': 3.422849402950933, 'subsample': 0.8863987514102385, 'colsample_bytree': 0.938739756492034, 'reg_lambda': 1.8366473242459957, 'reg_alpha': 0.1067143032388429, 'gamma': 0.25718123832607553, 'max_delta_step': 0.4146983387174734, 'num_boost_round': 4000}. Best is trial 3 with value: 0.9187956989247312.\n",
      "[I 2025-10-23 14:51:10,326] Trial 5 pruned. \n",
      "[I 2025-10-23 14:51:24,568] Trial 6 pruned. \n",
      "[I 2025-10-23 14:52:49,116] Trial 7 finished with value: 0.9171182795698923 and parameters: {'eta': 0.025262035599771283, 'max_depth': 8, 'min_child_weight': 3.5529690093442876, 'subsample': 0.8082124661064685, 'colsample_bytree': 0.7455296477512287, 'reg_lambda': 1.237419056746686, 'reg_alpha': 0.1581550805423266, 'gamma': 0.16830483793454853, 'max_delta_step': 0.16468458470248903, 'num_boost_round': 7000}. Best is trial 3 with value: 0.9187956989247312.\n",
      "[I 2025-10-23 14:53:03,491] Trial 8 pruned. \n",
      "[I 2025-10-23 14:53:44,163] Trial 9 finished with value: 0.9171182795698926 and parameters: {'eta': 0.02703849794331647, 'max_depth': 7, 'min_child_weight': 2.2814056183851927, 'subsample': 0.7718113913419536, 'colsample_bytree': 0.8146421330729284, 'reg_lambda': 1.2751628156039285, 'reg_alpha': 0.5874375786710095, 'gamma': 0.3053199589631246, 'max_delta_step': 1.3062751503250576, 'num_boost_round': 2000}. Best is trial 3 with value: 0.9187956989247312.\n",
      "[I 2025-10-23 14:53:53,566] Trial 10 pruned. \n",
      "[I 2025-10-23 14:54:14,175] Trial 11 finished with value: 0.9186236559139784 and parameters: {'eta': 0.05933393678227981, 'max_depth': 5, 'min_child_weight': 2.0215707231341993, 'subsample': 0.8261236177978183, 'colsample_bytree': 0.8749832469023278, 'reg_lambda': 2.4705695819438716, 'reg_alpha': 0.0016592051752524761, 'gamma': 0.3976431130405147, 'max_delta_step': 2.2101537249800876, 'num_boost_round': 11000}. Best is trial 3 with value: 0.9187956989247312.\n",
      "[I 2025-10-23 14:54:22,804] Trial 12 pruned. \n",
      "[I 2025-10-23 14:54:34,795] Trial 13 pruned. \n",
      "[I 2025-10-23 14:55:24,682] Trial 14 finished with value: 0.917505376344086 and parameters: {'eta': 0.02125443427981119, 'max_depth': 5, 'min_child_weight': 2.5472813117667243, 'subsample': 0.80508406678694, 'colsample_bytree': 0.8487115776566821, 'reg_lambda': 1.5413188751458193, 'reg_alpha': 0.3970055461201383, 'gamma': 0.24627965997336165, 'max_delta_step': 0.8222162797069117, 'num_boost_round': 11000}. Best is trial 3 with value: 0.9187956989247312.\n",
      "[I 2025-10-23 14:55:57,570] Trial 15 finished with value: 0.9185806451612902 and parameters: {'eta': 0.03730438108728428, 'max_depth': 5, 'min_child_weight': 1.9889920894830495, 'subsample': 0.8828761647020308, 'colsample_bytree': 0.9116763747324341, 'reg_lambda': 1.9399659691967752, 'reg_alpha': 0.09204908941764228, 'gamma': 0.04804804602762687, 'max_delta_step': 1.7340339929277782, 'num_boost_round': 10000}. Best is trial 3 with value: 0.9187956989247312.\n",
      "[I 2025-10-23 14:56:06,080] Trial 16 pruned. \n",
      "[I 2025-10-23 14:56:42,077] Trial 17 finished with value: 0.918752688172043 and parameters: {'eta': 0.030814438037049786, 'max_depth': 5, 'min_child_weight': 3.0489596310713196, 'subsample': 0.8360044239748506, 'colsample_bytree': 0.8970136546589822, 'reg_lambda': 1.385977356772138, 'reg_alpha': 0.3542699614374739, 'gamma': 0.2345938051820858, 'max_delta_step': 2.1709833978186492, 'num_boost_round': 9000}. Best is trial 3 with value: 0.9187956989247312.\n",
      "[I 2025-10-23 14:56:54,287] Trial 18 pruned. \n",
      "[I 2025-10-23 14:57:06,260] Trial 19 pruned. \n",
      "[I 2025-10-23 14:57:37,453] Trial 20 finished with value: 0.916731182795699 and parameters: {'eta': 0.03952458569033473, 'max_depth': 6, 'min_child_weight': 3.2113161015931633, 'subsample': 0.8638252150815248, 'colsample_bytree': 0.9444546232193185, 'reg_lambda': 1.5032610556602317, 'reg_alpha': 0.2533590966331631, 'gamma': 0.1945434449573834, 'max_delta_step': 0.6498734110032962, 'num_boost_round': 12000}. Best is trial 3 with value: 0.9187956989247312.\n",
      "[I 2025-10-23 14:57:46,460] Trial 21 pruned. \n",
      "[I 2025-10-23 14:57:59,455] Trial 22 pruned. \n",
      "[I 2025-10-23 14:58:07,538] Trial 23 pruned. \n",
      "[I 2025-10-23 14:58:51,410] Trial 24 finished with value: 0.9183225806451615 and parameters: {'eta': 0.022674127613228342, 'max_depth': 5, 'min_child_weight': 2.164499993885222, 'subsample': 0.8623647928667661, 'colsample_bytree': 0.7910441120567011, 'reg_lambda': 2.036441478572352, 'reg_alpha': 0.07723061801332401, 'gamma': 0.28734753153636694, 'max_delta_step': 2.3448922635293674, 'num_boost_round': 11000}. Best is trial 3 with value: 0.9187956989247312.\n",
      "[I 2025-10-23 14:59:01,174] Trial 25 pruned. \n",
      "[I 2025-10-23 14:59:11,731] Trial 26 pruned. \n",
      "[I 2025-10-23 14:59:34,904] Trial 27 finished with value: 0.9174623655913979 and parameters: {'eta': 0.0569806397453014, 'max_depth': 6, 'min_child_weight': 1.4550607838484164, 'subsample': 0.8370278810198072, 'colsample_bytree': 0.8907641959892748, 'reg_lambda': 2.226848388440284, 'reg_alpha': 0.21417093628328693, 'gamma': 0.30524835603698125, 'max_delta_step': 1.5259406170739915, 'num_boost_round': 8000}. Best is trial 3 with value: 0.9187956989247312.\n",
      "[I 2025-10-23 15:00:02,354] Trial 28 finished with value: 0.9180215053763442 and parameters: {'eta': 0.049119457057102925, 'max_depth': 5, 'min_child_weight': 2.41841877527881, 'subsample': 0.9037987779690362, 'colsample_bytree': 0.9283350548911198, 'reg_lambda': 1.7350709376935303, 'reg_alpha': 0.13806883917452928, 'gamma': 0.19653252441757776, 'max_delta_step': 2.9498465617084006, 'num_boost_round': 11000}. Best is trial 3 with value: 0.9187956989247312.\n",
      "[I 2025-10-23 15:00:27,484] Trial 29 finished with value: 0.9188387096774194 and parameters: {'eta': 0.0490189080645557, 'max_depth': 5, 'min_child_weight': 3.891896860531353, 'subsample': 0.8742618563850837, 'colsample_bytree': 0.8659018142039979, 'reg_lambda': 1.5618565796702324, 'reg_alpha': 0.24415862265325777, 'gamma': 0.12933152718632024, 'max_delta_step': 2.5373862427019613, 'num_boost_round': 4000}. Best is trial 29 with value: 0.9188387096774194.\n",
      "[FEMALE] Best CV Acc: 0.91884\n",
      "[FEMALE] Best params:\n",
      "{'eta': 0.0490189080645557, 'max_depth': 5, 'min_child_weight': 3.891896860531353, 'subsample': 0.8742618563850837, 'colsample_bytree': 0.8659018142039979, 'reg_lambda': 1.5618565796702324, 'reg_alpha': 0.24415862265325777, 'gamma': 0.12933152718632024, 'max_delta_step': 2.5373862427019613, 'num_boost_round': 4000}\n",
      "[MALE] Fold 1/5\n",
      "[MALE] Fold 2/5\n",
      "[MALE] Fold 3/5\n",
      "[MALE] Fold 4/5\n",
      "[MALE] Fold 5/5\n",
      "[MALE] OOF Acc: 0.88989 | OOF F1: 0.75287 | median best_iter: 210\n",
      "[FEMALE] Fold 1/5\n",
      "[FEMALE] Fold 2/5\n",
      "[FEMALE] Fold 3/5\n",
      "[FEMALE] Fold 4/5\n",
      "[FEMALE] Fold 5/5\n",
      "[FEMALE] OOF Acc: 0.91897 | OOF F1: 0.75200 | median best_iter: 274\n",
      "\n",
      "Saved submission.csv ✅\n",
      "      id       WeightCategory\n",
      "0  15533     Obesity_Type_III\n",
      "1  15534   Overweight_Level_I\n",
      "2  15535  Overweight_Level_II\n",
      "3  15536      Obesity_Type_II\n",
      "4  15537        Normal_Weight\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# Robust tuning for your winning setup (pure XGB, no ensembles)\n",
    "# Gender-split XGBoost + BMI + OHE + EarlyStopping\n",
    "# Optuna (tight ranges) + multi-seed CV stability\n",
    "# Produces: submission.csv\n",
    "# ==============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH  = \"test.csv\"\n",
    "SAMPLE_SUB_PATH = \"sample_submission.csv\"\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_JOBS = -1\n",
    "N_FOLDS = 5\n",
    "EARLY_STOP = 200\n",
    "\n",
    "# Tuning budget (tight but effective)\n",
    "TRIALS_MALE = 30\n",
    "TRIALS_FEMALE = 30\n",
    "\n",
    "# Use multiple CV shuffles during tuning for stability\n",
    "CV_SEEDS = [42, 2027, 1337]   # add/remove seeds to trade speed vs stability\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def add_bmi(df):\n",
    "    if (\"Weight\" in df.columns) and (\"Height\" in df.columns):\n",
    "        h = df[\"Height\"].astype(float)\n",
    "        hm = np.where(h.median() > 3.0, h/100.0, h)\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            bmi = df[\"Weight\"].astype(float) / (hm**2 + 1e-12)\n",
    "        df[\"BMI\"] = pd.Series(bmi).replace([np.inf, -np.inf], np.nan).clip(10, 80)\n",
    "    return df\n",
    "\n",
    "def make_preprocessor(num_cols, cat_cols):\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", ohe),\n",
    "    ])\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, num_cols),\n",
    "            (\"cat\", cat_pipe, cat_cols)\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=1.0\n",
    "    )\n",
    "\n",
    "def detect_gender(df):\n",
    "    for c in df.columns:\n",
    "        if c.lower() in (\"gender\",\"sex\"):\n",
    "            return c\n",
    "    raise ValueError(\"Could not detect gender column (expected Gender or Sex).\")\n",
    "\n",
    "def detect_id_and_target(sample_sub):\n",
    "    return sample_sub.columns[0], sample_sub.columns[1]\n",
    "\n",
    "def infer_cols(df):\n",
    "    cat_cols = df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "# Baseline params you’ve been using (we’ll tune around these)\n",
    "BASELINE = {\n",
    "    \"eta\": 0.03,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 2.0,\n",
    "    \"subsample\": 0.90,\n",
    "    \"colsample_bytree\": 0.90,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \"reg_alpha\": 0.0,\n",
    "    \"gamma\": 0.0,\n",
    "    \"max_delta_step\": 0.0,\n",
    "    \"num_boost_round\": 20000,  # with ES it will stop earlier\n",
    "}\n",
    "\n",
    "def suggest_params(trial):\n",
    "    # Tight ranges around the baseline to avoid CV-overfit drift\n",
    "    return {\n",
    "        \"eta\": trial.suggest_float(\"eta\", 0.02, 0.06, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 8),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 4.5),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.75, 0.95),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.70, 0.95),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.8, 2.5, log=True),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 0.6),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 0.4),\n",
    "        \"max_delta_step\": trial.suggest_float(\"max_delta_step\", 0.0, 3.0),\n",
    "        # We’ll let early stopping pick the effective iteration; still bound a reasonable cap:\n",
    "        \"num_boost_round\": trial.suggest_int(\"num_boost_round\", 2000, 12000, step=1000),\n",
    "    }\n",
    "\n",
    "def tune_group(Xg, yg, label, num_class, trials):\n",
    "    num_cols, cat_cols = infer_cols(Xg)\n",
    "\n",
    "    def objective(trial):\n",
    "        hp = suggest_params(trial)\n",
    "\n",
    "        scores_across_seeds = []\n",
    "        for cv_seed in CV_SEEDS:\n",
    "            skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=cv_seed)\n",
    "            fold_scores = []\n",
    "\n",
    "            for tr_idx, va_idx in skf.split(Xg, yg):\n",
    "                X_tr, X_va = Xg.iloc[tr_idx], Xg.iloc[va_idx]\n",
    "                y_tr, y_va = yg[tr_idx], yg[va_idx]\n",
    "\n",
    "                pre = make_preprocessor(num_cols, cat_cols)\n",
    "                Xtr = pre.fit_transform(X_tr)\n",
    "                Xva = pre.transform(X_va)\n",
    "\n",
    "                dtrain = xgb.DMatrix(Xtr, label=y_tr)\n",
    "                dvalid = xgb.DMatrix(Xva, label=y_va)\n",
    "\n",
    "                params = {\n",
    "                    \"objective\": \"multi:softprob\",\n",
    "                    \"num_class\": num_class,\n",
    "                    \"eval_metric\": \"mlogloss\",\n",
    "                    \"tree_method\": \"hist\",\n",
    "                    \"eta\": hp[\"eta\"],\n",
    "                    \"max_depth\": int(hp[\"max_depth\"]),\n",
    "                    \"min_child_weight\": float(hp[\"min_child_weight\"]),\n",
    "                    \"subsample\": float(hp[\"subsample\"]),\n",
    "                    \"colsample_bytree\": float(hp[\"colsample_bytree\"]),\n",
    "                    \"reg_lambda\": float(hp[\"reg_lambda\"]),\n",
    "                    \"reg_alpha\": float(hp[\"reg_alpha\"]),\n",
    "                    \"gamma\": float(hp[\"gamma\"]),\n",
    "                    \"max_delta_step\": float(hp[\"max_delta_step\"]),\n",
    "                    \"nthread\": N_JOBS,\n",
    "                    \"seed\": cv_seed,\n",
    "                    \"verbosity\": 0,\n",
    "                }\n",
    "\n",
    "                bst = xgb.train(\n",
    "                    params=params,\n",
    "                    dtrain=dtrain,\n",
    "                    num_boost_round=hp[\"num_boost_round\"],\n",
    "                    evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "                    early_stopping_rounds=EARLY_STOP,\n",
    "                    verbose_eval=False\n",
    "                )\n",
    "\n",
    "                pred_va = bst.predict(dvalid, iteration_range=(0, bst.best_iteration + 1))\n",
    "                y_hat = np.argmax(pred_va, axis=1)\n",
    "                fold_scores.append(accuracy_score(y_va, y_hat))\n",
    "\n",
    "            scores_across_seeds.append(np.mean(fold_scores))\n",
    "\n",
    "            # report progress so TPE can prune bad regions\n",
    "            trial.report(np.mean(scores_across_seeds), step=len(scores_across_seeds))\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        return float(np.mean(scores_across_seeds))\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    print(f\"\\n[{label}] Robust tuning: {trials} trials × {len(CV_SEEDS)} seeds × {N_FOLDS}-fold CV\")\n",
    "    study.optimize(objective, n_trials=trials, show_progress_bar=True)\n",
    "\n",
    "    print(f\"[{label}] Best CV Acc: {study.best_value:.5f}\")\n",
    "    print(f\"[{label}] Best params:\\n{study.best_params}\")\n",
    "    return study.best_params\n",
    "\n",
    "def train_cv_predict(Xg, yg, Xtestg, params, num_class, label):\n",
    "    num_cols, cat_cols = infer_cols(Xg)\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    oof = np.zeros((len(Xg), num_class), dtype=np.float32)\n",
    "    test_pred = np.zeros((len(Xtestg), num_class), dtype=np.float32)\n",
    "    best_iters = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(Xg, yg), start=1):\n",
    "        print(f\"[{label}] Fold {fold}/{N_FOLDS}\")\n",
    "        X_tr, X_va = Xg.iloc[tr_idx], Xg.iloc[va_idx]\n",
    "        y_tr, y_va = yg[tr_idx], yg[va_idx]\n",
    "\n",
    "        pre = make_preprocessor(num_cols, cat_cols)\n",
    "        Xtr = pre.fit_transform(X_tr)\n",
    "        Xva = pre.transform(X_va)\n",
    "        Xte = pre.transform(Xtestg)\n",
    "\n",
    "        dtrain = xgb.DMatrix(Xtr, label=y_tr)\n",
    "        dvalid = xgb.DMatrix(Xva, label=y_va)\n",
    "        dtest  = xgb.DMatrix(Xte)\n",
    "\n",
    "        train_params = {\n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"num_class\": num_class,\n",
    "            \"eval_metric\": \"mlogloss\",\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"nthread\": N_JOBS,\n",
    "            \"seed\": RANDOM_STATE,\n",
    "            \"verbosity\": 0,\n",
    "            \"eta\": float(params[\"eta\"]),\n",
    "            \"max_depth\": int(params[\"max_depth\"]),\n",
    "            \"min_child_weight\": float(params[\"min_child_weight\"]),\n",
    "            \"subsample\": float(params[\"subsample\"]),\n",
    "            \"colsample_bytree\": float(params[\"colsample_bytree\"]),\n",
    "            \"reg_lambda\": float(params[\"reg_lambda\"]),\n",
    "            \"reg_alpha\": float(params[\"reg_alpha\"]),\n",
    "            \"gamma\": float(params[\"gamma\"]),\n",
    "            \"max_delta_step\": float(params[\"max_delta_step\"]),\n",
    "        }\n",
    "\n",
    "        bst = xgb.train(\n",
    "            params=train_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=int(params[\"num_boost_round\"]),\n",
    "            evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "            early_stopping_rounds=EARLY_STOP,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        best_iters.append(int(bst.best_iteration + 1))\n",
    "\n",
    "        oof[va_idx] = bst.predict(dvalid, iteration_range=(0, bst.best_iteration + 1))\n",
    "        test_pred += bst.predict(dtest, iteration_range=(0, bst.best_iteration + 1)) / N_FOLDS\n",
    "\n",
    "    y_oof = np.argmax(oof, axis=1)\n",
    "    print(f\"[{label}] OOF Acc: {accuracy_score(yg, y_oof):.5f} | OOF F1: {f1_score(yg, y_oof, average='macro'):.5f} | median best_iter: {int(np.median(best_iters))}\")\n",
    "    return test_pred\n",
    "\n",
    "# ==============================================================\n",
    "# Load & Prepare\n",
    "# ==============================================================\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "# Drop columns you don’t want\n",
    "for c in [\"MTRANS\",\"SMOKE\"]:\n",
    "    if c in train.columns: train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:  test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# Add BMI (only engineered feature you kept)\n",
    "train = add_bmi(train)\n",
    "test  = add_bmi(test)\n",
    "\n",
    "# Detect ID/Target\n",
    "ID_COL, TARGET_COL = detect_id_and_target(sample_sub)\n",
    "y = train[TARGET_COL].copy()\n",
    "X = train.drop(columns=[TARGET_COL, ID_COL], errors=\"ignore\")\n",
    "test_ids = test[ID_COL]\n",
    "test_X = test.drop(columns=[ID_COL], errors=\"ignore\")\n",
    "\n",
    "# Label encode target\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "classes = le.classes_\n",
    "num_class = len(classes)\n",
    "\n",
    "# Gender split\n",
    "gender_col = detect_gender(X)\n",
    "male_mask   = X[gender_col].astype(str).str.lower().str.startswith((\"m\"))\n",
    "female_mask = X[gender_col].astype(str).str.lower().str.startswith((\"f\"))\n",
    "test_male_mask   = test_X[gender_col].astype(str).str.lower().str.startswith((\"m\"))\n",
    "test_female_mask = test_X[gender_col].astype(str).str.lower().str.startswith((\"f\"))\n",
    "\n",
    "print(f\"Train: males={male_mask.sum()}, females={female_mask.sum()}\")\n",
    "print(f\"Test:  males={test_male_mask.sum()}, females={test_female_mask.sum()}\")\n",
    "\n",
    "X_male,   y_male   = X[male_mask].reset_index(drop=True),   y_enc[male_mask]\n",
    "X_female, y_female = X[female_mask].reset_index(drop=True), y_enc[female_mask]\n",
    "test_male   = test_X[test_male_mask].reset_index(drop=True)\n",
    "test_female = test_X[test_female_mask].reset_index(drop=True)\n",
    "\n",
    "# ==============================================================\n",
    "# Tune (tight ranges, multi-seed stability) per gender\n",
    "# ==============================================================\n",
    "\n",
    "best_male   = tune_group(X_male, y_male, \"MALE\",   num_class, TRIALS_MALE)\n",
    "best_female = tune_group(X_female, y_female, \"FEMALE\", num_class, TRIALS_FEMALE)\n",
    "\n",
    "# ==============================================================\n",
    "# Train with best params and predict (5-fold avg)\n",
    "# ==============================================================\n",
    "\n",
    "pred_male   = train_cv_predict(X_male, y_male, test_male,   best_male,   num_class, \"MALE\")\n",
    "pred_female = train_cv_predict(X_female, y_female, test_female, best_female, num_class, \"FEMALE\")\n",
    "\n",
    "# Merge predictions and build submission\n",
    "final_proba = np.zeros((len(test_X), num_class), dtype=np.float32)\n",
    "final_proba[test_male_mask.values]   = pred_male\n",
    "final_proba[test_female_mask.values] = pred_female\n",
    "final_pred = le.inverse_transform(np.argmax(final_proba, axis=1))\n",
    "\n",
    "sub = pd.DataFrame({ID_COL: test_ids, TARGET_COL: final_pred})\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSaved submission.csv ✅\")\n",
    "print(sub.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb3d9ce0-5e43-4927-8304-107c155f20c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Detected] Target in train: 'WeightCategory', Label in sample_sub: 'WeightCategory'\n",
      "[Detected] ID in train: 'id', ID in test: 'id'\n",
      "[Info] Train male rows: 8851 | female rows: 8793\n",
      "[Info] Test  male rows: 2553 | female rows: 2672\n",
      "\n",
      "[MALE] Fold 1/5\n",
      "[MALE] Best iteration: 409\n",
      "[MALE] Acc: 0.8984 | Macro F1: 0.7595\n",
      "\n",
      "[MALE] Fold 2/5\n",
      "[MALE] Best iteration: 349\n",
      "[MALE] Acc: 0.8881 | Macro F1: 0.7526\n",
      "\n",
      "[MALE] Fold 3/5\n",
      "[MALE] Best iteration: 275\n",
      "[MALE] Acc: 0.8825 | Macro F1: 0.7455\n",
      "\n",
      "[MALE] Fold 4/5\n",
      "[MALE] Best iteration: 336\n",
      "[MALE] Acc: 0.8960 | Macro F1: 0.7605\n",
      "\n",
      "[MALE] Fold 5/5\n",
      "[MALE] Best iteration: 354\n",
      "[MALE] Acc: 0.9000 | Macro F1: 0.7617\n",
      "\n",
      "[MALE] OOF Accuracy: 0.8930 | Macro F1: 0.7560\n",
      "[MALE] Best iterations: [409, 349, 275, 336, 354] | Median: 349\n",
      "\n",
      "[FEMALE] Fold 1/5\n",
      "[FEMALE] Best iteration: 334\n",
      "[FEMALE] Acc: 0.9176 | Macro F1: 0.7531\n",
      "\n",
      "[FEMALE] Fold 2/5\n",
      "[FEMALE] Best iteration: 363\n",
      "[FEMALE] Acc: 0.9346 | Macro F1: 0.7769\n",
      "\n",
      "[FEMALE] Fold 3/5\n",
      "[FEMALE] Best iteration: 326\n",
      "[FEMALE] Acc: 0.9267 | Macro F1: 0.7643\n",
      "\n",
      "[FEMALE] Fold 4/5\n",
      "[FEMALE] Best iteration: 331\n",
      "[FEMALE] Acc: 0.9255 | Macro F1: 0.7634\n",
      "\n",
      "[FEMALE] Fold 5/5\n",
      "[FEMALE] Best iteration: 368\n",
      "[FEMALE] Acc: 0.9278 | Macro F1: 0.7671\n",
      "\n",
      "[FEMALE] OOF Accuracy: 0.9264 | Macro F1: 0.7649\n",
      "[FEMALE] Best iterations: [334, 363, 326, 331, 368] | Median: 334\n",
      "\n",
      "========== OVERALL OOF ==========\n",
      "OOF Accuracy: 0.9097 | OOF Macro F1: 0.9017\n",
      "\n",
      "OOF Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.94      0.94      0.94      2142\n",
      "      Normal_Weight       0.88      0.89      0.89      2632\n",
      "     Obesity_Type_I       0.90      0.89      0.90      2558\n",
      "    Obesity_Type_II       0.97      0.97      0.97      2700\n",
      "   Obesity_Type_III       1.00      1.00      1.00      3307\n",
      " Overweight_Level_I       0.81      0.79      0.80      2134\n",
      "Overweight_Level_II       0.82      0.83      0.82      2171\n",
      "\n",
      "           accuracy                           0.91     17644\n",
      "          macro avg       0.90      0.90      0.90     17644\n",
      "       weighted avg       0.91      0.91      0.91     17644\n",
      "\n",
      "\n",
      "Saved submission.csv\n",
      "      id       WeightCategory\n",
      "0  15533     Obesity_Type_III\n",
      "1  15534   Overweight_Level_I\n",
      "2  15535  Overweight_Level_II\n",
      "3  15536      Obesity_Type_II\n",
      "4  15537        Normal_Weight\n",
      "5  15538  Insufficient_Weight\n",
      "6  15539       Obesity_Type_I\n",
      "7  15540     Obesity_Type_III\n",
      "8  15541   Overweight_Level_I\n",
      "9  15542       Obesity_Type_I\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b132bf9d-2c05-49f4-810b-486769bbb5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
