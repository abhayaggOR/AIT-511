{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efdc8388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# Gender-specific XGB + BMI + requested feature engineering + Kaggle_test eval\n",
    "# ==============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.base import clone\n",
    "import xgboost as xgb\n",
    "\n",
    "# -------- Paths --------\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "SAMPLE_SUB_PATH = \"sample_submission.csv\"\n",
    "KAGGLE_TEST_PATH = \"Kaggle_test.csv\"  # has WeightCategory ground truth\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "N_JOBS = -1\n",
    "\n",
    "# -------- Helpers --------\n",
    "def norm_col(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    return str(s).replace(\"\\ufeff\", \"\").strip().lower()\n",
    "\n",
    "def infer_feature_types(df):\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def detect_gender_column(df):\n",
    "    # common names\n",
    "    for c in df.columns:\n",
    "        if norm_col(c) in {\"gender\", \"sex\"}:\n",
    "            return c\n",
    "    # fallback: column that looks like M/F\n",
    "    for c in df.columns:\n",
    "        vals = pd.Series(df[c].dropna().astype(str).str.lower().str.strip()).unique()\n",
    "        if len(vals) in (2, 3):\n",
    "            if any(v.startswith(\"m\") for v in vals) and any(v.startswith(\"f\") for v in vals):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def split_by_gender(series):\n",
    "    s = series.astype(str).str.lower().str.strip()\n",
    "    male_mask = s.str.startswith((\"m\",\"1\",\"true\"))\n",
    "    female_mask = s.str.startswith((\"f\",\"0\",\"false\"))\n",
    "    if male_mask.sum()==0 and female_mask.sum()==0:\n",
    "        top = s.value_counts().index.tolist()\n",
    "        if len(top)>=2:\n",
    "            male_mask = s==top[0]\n",
    "            female_mask = s==top[1]\n",
    "    return male_mask, female_mask\n",
    "\n",
    "def add_bmi(df):\n",
    "    \"\"\"Compute BMI = Weight / (Height_m^2).\n",
    "       If median height > 3 assume cm → convert to meters.\"\"\"\n",
    "    if (\"Weight\" in df.columns) and (\"Height\" in df.columns):\n",
    "        h = pd.to_numeric(df[\"Height\"], errors=\"coerce\")\n",
    "        height_m = np.where(np.nanmedian(h) > 3.0, h / 100.0, h)\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            w = pd.to_numeric(df[\"Weight\"], errors=\"coerce\")\n",
    "            bmi = w / (np.power(height_m, 2) + 1e-12)\n",
    "        df[\"BMI\"] = pd.Series(bmi).replace([np.inf, -np.inf], np.nan)\n",
    "    return df\n",
    "\n",
    "def safe_lower_str_col(s):\n",
    "    \"\"\"Return string-lowered series (preserve NaNs).\"\"\"\n",
    "    return s.astype(str).str.strip().str.lower().where(s.notna(), other=np.nan)\n",
    "\n",
    "# -------- Load data --------\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe21299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Engineered features added: favc_fcvc_interaction, activity_balance, meal_balance\n",
      "[Info] Dropped: SMOKE, MTRANS, FAVC, FCVC, TUE, NCP, CAEC\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# Feature Engineering (as requested)\n",
    "# ===========================================================\n",
    "\n",
    "# 1) Drop columns we don’t want\n",
    "for c in [\"MTRANS\",\"SMOKE\"]:\n",
    "    if c in train.columns: train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:  test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# 2) FAVC * FCVC interaction, then drop FAVC/FCVC\n",
    "# Normalize FAVC to numeric (yes=1, no=0), robust to capitalization/whitespace\n",
    "if \"FAVC\" in train.columns:\n",
    "    favc_map = {\"yes\":1, \"no\":0}\n",
    "    tr_favc = safe_lower_str_col(train[\"FAVC\"])\n",
    "    te_favc = safe_lower_str_col(test[\"FAVC\"]) if \"FAVC\" in test.columns else pd.Series(index=test.index, dtype=object)\n",
    "    train[\"FAVC_num\"] = tr_favc.map(favc_map)\n",
    "    if \"FAVC\" in test.columns:\n",
    "        test[\"FAVC_num\"] = te_favc.map(favc_map)\n",
    "\n",
    "# If FCVC present, create interaction\n",
    "if (\"FAVC_num\" in train.columns) and (\"FCVC\" in train.columns):\n",
    "    train[\"favc_fcvc_interaction\"] = pd.to_numeric(train[\"FAVC_num\"], errors=\"coerce\") * pd.to_numeric(train[\"FCVC\"], errors=\"coerce\")\n",
    "if (\"FAVC_num\" in test.columns) and (\"FCVC\" in test.columns):\n",
    "    test[\"favc_fcvc_interaction\"]  = pd.to_numeric(test[\"FAVC_num\"], errors=\"coerce\")  * pd.to_numeric(test[\"FCVC\"], errors=\"coerce\")\n",
    "\n",
    "# Drop originals (FAVC, FCVC, and intermediate FAVC_num)\n",
    "for c in [\"FAVC\",\"FAVC_num\",\"FCVC\"]:\n",
    "    if c in train.columns: train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:  test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# 3) activity_balance = FAF / (TUE + 1e-6); keep FAF, drop TUE\n",
    "if (\"FAF\" in train.columns) and (\"TUE\" in train.columns):\n",
    "    train[\"activity_balance\"] = pd.to_numeric(train[\"FAF\"], errors=\"coerce\") / (pd.to_numeric(train[\"TUE\"], errors=\"coerce\") + 1e-6)\n",
    "if (\"FAF\" in test.columns) and (\"TUE\" in test.columns):\n",
    "    test[\"activity_balance\"]  = pd.to_numeric(test[\"FAF\"], errors=\"coerce\")  / (pd.to_numeric(test[\"TUE\"], errors=\"coerce\")  + 1e-6)\n",
    "\n",
    "# Drop TUE only (keep FAF)\n",
    "for c in [\"TUE\"]:\n",
    "    if c in train.columns: train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:  test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# 4) meal_balance = NCP * CAEC_num; map CAEC via provided mapping, then drop NCP & CAEC\n",
    "# Provided mapping (note: 'no' is lowercase, others have capital first letter)\n",
    "_caec_map_exact = {'no': 0, 'Sometimes': 1, 'Frequently': 2, 'Always': 3}\n",
    "# To be robust, we’ll normalize train/test strings, then map back to the exact keys:\n",
    "def map_caec(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s_lower = s.str.lower()\n",
    "    # build normalized map\n",
    "    norm_map = {'no':0, 'sometimes':1, 'frequently':2, 'always':3}\n",
    "    return s_lower.map(norm_map)\n",
    "\n",
    "if \"CAEC\" in train.columns:\n",
    "    train[\"CAEC_num\"] = map_caec(train[\"CAEC\"])\n",
    "if \"CAEC\" in test.columns:\n",
    "    test[\"CAEC_num\"]  = map_caec(test[\"CAEC\"])\n",
    "\n",
    "if (\"NCP\" in train.columns) and (\"CAEC_num\" in train.columns):\n",
    "    train[\"meal_balance\"] = pd.to_numeric(train[\"NCP\"], errors=\"coerce\") * pd.to_numeric(train[\"CAEC_num\"], errors=\"coerce\")\n",
    "if (\"NCP\" in test.columns) and (\"CAEC_num\" in test.columns):\n",
    "    test[\"meal_balance\"]  = pd.to_numeric(test[\"NCP\"], errors=\"coerce\")  * pd.to_numeric(test[\"CAEC_num\"], errors=\"coerce\")\n",
    "\n",
    "for c in [\"NCP\",\"CAEC\"]:\n",
    "    if c in train.columns: train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:  test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# 5) BMI (keep as in your original)\n",
    "train = add_bmi(train)\n",
    "test  = add_bmi(test)\n",
    "\n",
    "print(\"[Info] Engineered features added: favc_fcvc_interaction, activity_balance, meal_balance\")\n",
    "print(\"[Info] Dropped: SMOKE, MTRANS, FAVC, FCVC, TUE, NCP, CAEC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea09f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Classes: ['Insufficient_Weight', 'Normal_Weight', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III', 'Overweight_Level_I', 'Overweight_Level_II']\n",
      "[Info] Train male=7783, female=7750\n",
      "[Info] Test  male=10336, female=10422\n"
     ]
    }
   ],
   "source": [
    "# -------- Detect ID/Target from files (simple logic)\n",
    "id_col = None\n",
    "for cand in [\"id\", \"row_id\", \"index\", \"sample_id\"]:\n",
    "    if cand in train.columns and cand in test.columns:\n",
    "        id_col = cand\n",
    "        break\n",
    "\n",
    "target_col = None\n",
    "for cand in [\"WeightCategory\", \"NObeyesdad\", \"label\", \"target\", \"class\", \"y\"]:\n",
    "    if cand in train.columns:\n",
    "        target_col = cand\n",
    "        break\n",
    "if target_col is None:\n",
    "    raise ValueError(\"Could not detect target column in train.csv\")\n",
    "\n",
    "# Build X/y\n",
    "y = train[target_col].copy()\n",
    "X = train.drop(columns=[target_col]).copy()\n",
    "if id_col and id_col in X.columns:\n",
    "    X.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "test_features = test.copy()\n",
    "if id_col and id_col in test_features.columns:\n",
    "    test_ids = test_features[id_col].copy()\n",
    "    test_features.drop(columns=[id_col], inplace=True)\n",
    "else:\n",
    "    test_ids = pd.Series(np.arange(len(test_features)), name=\"id\")\n",
    "\n",
    "# Label encode target\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "classes = list(le.classes_)\n",
    "print(f\"[Info] Classes: {classes}\")\n",
    "\n",
    "# Detect gender and split\n",
    "gender_col = detect_gender_column(pd.concat([X, test_features], axis=0))\n",
    "if gender_col is None:\n",
    "    raise ValueError(\"Could not detect a gender column (e.g., 'Gender'/'SEX').\")\n",
    "male_mask, female_mask = split_by_gender(train[gender_col])\n",
    "test_male_mask, test_female_mask = split_by_gender(test_features[gender_col])\n",
    "print(f\"[Info] Train male={int(male_mask.sum())}, female={int(female_mask.sum())}\")\n",
    "print(f\"[Info] Test  male={int(test_male_mask.sum())}, female={int(test_female_mask.sum())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92153073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------- Training function (gender-specific) with class boosting for two classes --------\n",
    "def train_group_and_predict(X_grp, y_enc_grp, test_grp, group_name,\n",
    "                            boost_targets=(\"Overweight_Level_I\",\"Overweight_Level_II\"),\n",
    "                            base_boost=1.50, jitter_amp=0.10):\n",
    "    # Drop gender column inside a group (constant after split)\n",
    "    cols_to_use = [c for c in X_grp.columns if c != gender_col]\n",
    "    Xg = X_grp[cols_to_use].copy()\n",
    "    Xtestg = test_grp[cols_to_use].copy()\n",
    "\n",
    "    num_cols, cat_cols = infer_feature_types(Xg)\n",
    "\n",
    "    # Preprocessor\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", ohe)\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=1.0\n",
    "    )\n",
    "\n",
    "    # XGB params\n",
    "    xgb_params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": len(classes),\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"max_depth\": 6,\n",
    "        \"min_child_weight\": 2,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"lambda\": 1.0,\n",
    "        \"alpha\": 0.0,\n",
    "        \"eta\": 0.03,\n",
    "        \"nthread\": N_JOBS,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "    }\n",
    "    NUM_BOOST_ROUND = 20000\n",
    "    EARLY_STOP = 200\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof_group = np.zeros((len(Xg), len(classes)), dtype=np.float32)\n",
    "    test_group_pred = np.zeros((len(Xtestg), len(classes)), dtype=np.float32)\n",
    "    fold_best = []\n",
    "\n",
    "    # map class name -> index\n",
    "    cls_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(Xg, y_enc_grp), start=1):\n",
    "        print(f\"\\n[{group_name}] Fold {fold}/{N_FOLDS}\")\n",
    "        X_tr, X_va = Xg.iloc[tr_idx], Xg.iloc[va_idx]\n",
    "        y_tr, y_va = y_enc_grp[tr_idx], y_enc_grp[va_idx]\n",
    "\n",
    "        prep = clone(preprocessor)\n",
    "        Xtr = prep.fit_transform(X_tr)\n",
    "        Xva = prep.transform(X_va)\n",
    "\n",
    "        # ---- RANDOM (non-count) WEIGHTS to gently boost two classes ----\n",
    "        w_tr = np.ones_like(y_tr, dtype=float)\n",
    "        rng = np.random.default_rng(RANDOM_STATE + fold)  # deterministic per fold\n",
    "        for t in boost_targets:\n",
    "            if t in cls_to_idx:\n",
    "                cls_id = cls_to_idx[t]\n",
    "                idx_t = np.where(y_tr == cls_id)[0]\n",
    "                if idx_t.size > 0:\n",
    "                    jitter = rng.uniform(-jitter_amp, jitter_amp, size=idx_t.size)\n",
    "                    w_tr[idx_t] = base_boost + jitter\n",
    "        w_va = np.ones_like(y_va, dtype=float)\n",
    "\n",
    "        dtrain = xgb.DMatrix(Xtr, label=y_tr, weight=w_tr)\n",
    "        dval   = xgb.DMatrix(Xva, label=y_va, weight=w_va)\n",
    "\n",
    "        bst = xgb.train(\n",
    "            params=xgb_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=NUM_BOOST_ROUND,\n",
    "            evals=[(dtrain, \"train\"), (dval, \"valid\")],\n",
    "            feval=None,\n",
    "            early_stopping_rounds=EARLY_STOP,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        best_round = int(bst.best_iteration + 1)\n",
    "        fold_best.append(best_round)\n",
    "        print(f\"[{group_name}] Best iteration: {best_round}\")\n",
    "\n",
    "        oof_proba = bst.predict(dval, iteration_range=(0, best_round))\n",
    "        oof_group[va_idx] = oof_proba\n",
    "\n",
    "        # test preds for this fold\n",
    "        Xtest_tf = prep.transform(Xtestg)\n",
    "        dtest = xgb.DMatrix(Xtest_tf)\n",
    "        test_group_pred += bst.predict(dtest, iteration_range=(0, best_round)) / N_FOLDS\n",
    "\n",
    "    # OOF summary for the group\n",
    "    oof_labels = np.argmax(oof_group, axis=1)\n",
    "    acc_g = accuracy_score(y_enc_grp, oof_labels)\n",
    "    f1_g = f1_score(y_enc_grp, oof_labels, average=\"macro\")\n",
    "    print(f\"\\n[{group_name}] OOF Accuracy: {acc_g:.4f} | Macro F1: {f1_g:.4f}\")\n",
    "    print(f\"[{group_name}] Best iterations: {fold_best} | Median: {int(np.median(fold_best))}\")\n",
    "\n",
    "    return oof_group, test_group_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65f1424f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MALE] Fold 1/5\n",
      "[MALE] Best iteration: 305\n",
      "\n",
      "[MALE] Fold 2/5\n",
      "[MALE] Best iteration: 311\n",
      "\n",
      "[MALE] Fold 3/5\n",
      "[MALE] Best iteration: 320\n",
      "\n",
      "[MALE] Fold 4/5\n",
      "[MALE] Best iteration: 315\n",
      "\n",
      "[MALE] Fold 5/5\n",
      "[MALE] Best iteration: 287\n",
      "\n",
      "[MALE] OOF Accuracy: 0.8855 | Macro F1: 0.7491\n",
      "[MALE] Best iterations: [305, 311, 320, 315, 287] | Median: 311\n",
      "\n",
      "[FEMALE] Fold 1/5\n",
      "[FEMALE] Best iteration: 311\n",
      "\n",
      "[FEMALE] Fold 2/5\n",
      "[FEMALE] Best iteration: 311\n",
      "\n",
      "[FEMALE] Fold 3/5\n",
      "[FEMALE] Best iteration: 292\n",
      "\n",
      "[FEMALE] Fold 4/5\n",
      "[FEMALE] Best iteration: 319\n",
      "\n",
      "[FEMALE] Fold 5/5\n",
      "[FEMALE] Best iteration: 362\n",
      "\n",
      "[FEMALE] OOF Accuracy: 0.9166 | Macro F1: 0.7494\n",
      "[FEMALE] Best iterations: [311, 311, 292, 319, 362] | Median: 311\n",
      "\n",
      "========== OVERALL OOF ==========\n",
      "OOF Accuracy: 0.9010 | OOF Macro F1: 0.8916\n",
      "\n",
      "OOF Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.93      0.93      0.93      1870\n",
      "      Normal_Weight       0.88      0.88      0.88      2345\n",
      "     Obesity_Type_I       0.90      0.86      0.88      2207\n",
      "    Obesity_Type_II       0.96      0.97      0.97      2403\n",
      "   Obesity_Type_III       1.00      1.00      1.00      2983\n",
      " Overweight_Level_I       0.80      0.78      0.79      1844\n",
      "Overweight_Level_II       0.78      0.83      0.80      1881\n",
      "\n",
      "           accuracy                           0.90     15533\n",
      "          macro avg       0.89      0.89      0.89     15533\n",
      "       weighted avg       0.90      0.90      0.90     15533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------- Train per-gender and predict full test --------\n",
    "X_male = X[male_mask].reset_index(drop=True)\n",
    "y_male_enc = y_enc[male_mask]\n",
    "test_male = test_features[test_male_mask].reset_index(drop=True)\n",
    "\n",
    "X_female = X[female_mask].reset_index(drop=True)\n",
    "y_female_enc = y_enc[female_mask]\n",
    "test_female = test_features[test_female_mask].reset_index(drop=True)\n",
    "\n",
    "male_oof, male_test_pred = train_group_and_predict(X_male, y_male_enc, test_male, \"MALE\")\n",
    "female_oof, female_test_pred = train_group_and_predict(X_female, y_female_enc, test_female, \"FEMALE\")\n",
    "\n",
    "# Combine OOF\n",
    "oof_full = np.zeros((len(X), len(classes)), dtype=np.float32)\n",
    "oof_full[male_mask.values] = male_oof\n",
    "oof_full[female_mask.values] = female_oof\n",
    "\n",
    "oof_labels = np.argmax(oof_full, axis=1)\n",
    "oof_acc = accuracy_score(y_enc, oof_labels)\n",
    "oof_f1 = f1_score(y_enc, oof_labels, average=\"macro\")\n",
    "print(\"\\n========== OVERALL OOF ==========\")\n",
    "print(f\"OOF Accuracy: {oof_acc:.4f} | OOF Macro F1: {oof_f1:.4f}\")\n",
    "try:\n",
    "    print(\"\\nOOF Classification Report:\\n\",\n",
    "          classification_report(y_enc, oof_labels, target_names=classes))\n",
    "except Exception as e:\n",
    "    print(f\"[Info] Could not print classification report: {e}\")\n",
    "\n",
    "# Build full test predictions (for Kaggle submission use-case)\n",
    "test_pred_proba = np.zeros((len(test_features), len(classes)), dtype=np.float32)\n",
    "test_pred_proba[test_male_mask.values] = male_test_pred\n",
    "test_pred_proba[test_female_mask.values] = female_test_pred\n",
    "\n",
    "test_pred_int = np.argmax(test_pred_proba, axis=1)\n",
    "test_pred_labels = le.inverse_transform(test_pred_int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62dbc10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved submission.csv\n",
      "   id       WeightCategory\n",
      "0   0  Overweight_Level_II\n",
      "1   1        Normal_Weight\n",
      "2   2  Insufficient_Weight\n",
      "3   3     Obesity_Type_III\n",
      "4   4  Overweight_Level_II\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Submission\n",
    "ss_cols = list(sample_sub.columns)\n",
    "ID_HEADER = None\n",
    "LABEL_HEADER = None\n",
    "if len(ss_cols) == 2:\n",
    "    # detect which is ID by presence in test\n",
    "    c1, c2 = ss_cols\n",
    "    if c1 in test.columns and c2 not in test.columns:\n",
    "        ID_HEADER, LABEL_HEADER = c1, c2\n",
    "    elif c2 in test.columns and c1 not in test.columns:\n",
    "        ID_HEADER, LABEL_HEADER = c2, c1\n",
    "if ID_HEADER is None:\n",
    "    # fallback\n",
    "    ID_HEADER = ss_cols[0]\n",
    "    LABEL_HEADER = ss_cols[1]\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "if ID_HEADER in test.columns:\n",
    "    sub[ID_HEADER] = test[ID_HEADER].values\n",
    "else:\n",
    "    sub[ID_HEADER] = np.arange(len(test_features))\n",
    "sub[LABEL_HEADER] = test_pred_labels\n",
    "\n",
    "# Ensure column order\n",
    "for c in ss_cols:\n",
    "    if c not in sub.columns:\n",
    "        sub[c] = sample_sub[c].iloc[0] if len(sample_sub[c]) else None\n",
    "sub = sub[ss_cols]\n",
    "\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSaved submission.csv\")\n",
    "print(sub.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ecafa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MALE (Kaggle)] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE (Kaggle)] Best iteration: 305\n",
      "\n",
      "[MALE (Kaggle)] Fold 2/5\n",
      "[MALE (Kaggle)] Best iteration: 311\n",
      "\n",
      "[MALE (Kaggle)] Fold 3/5\n",
      "[MALE (Kaggle)] Best iteration: 320\n",
      "\n",
      "[MALE (Kaggle)] Fold 4/5\n",
      "[MALE (Kaggle)] Best iteration: 315\n",
      "\n",
      "[MALE (Kaggle)] Fold 5/5\n",
      "[MALE (Kaggle)] Best iteration: 287\n",
      "\n",
      "[MALE (Kaggle)] OOF Accuracy: 0.8855 | Macro F1: 0.7491\n",
      "[MALE (Kaggle)] Best iterations: [305, 311, 320, 315, 287] | Median: 311\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 1/5\n",
      "[FEMALE (Kaggle)] Best iteration: 311\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 2/5\n",
      "[FEMALE (Kaggle)] Best iteration: 311\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 3/5\n",
      "[FEMALE (Kaggle)] Best iteration: 292\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 4/5\n",
      "[FEMALE (Kaggle)] Best iteration: 319\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 5/5\n",
      "[FEMALE (Kaggle)] Best iteration: 362\n",
      "\n",
      "[FEMALE (Kaggle)] OOF Accuracy: 0.9166 | Macro F1: 0.7494\n",
      "[FEMALE (Kaggle)] Best iterations: [311, 311, 292, 319, 362] | Median: 311\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Evaluate on Kaggle_test.csv (with ground truth)\n",
    "# ==============================================\n",
    "kdf = pd.read_csv(KAGGLE_TEST_PATH)\n",
    "if \"WeightCategory\" not in kdf.columns:\n",
    "    raise KeyError(\"Kaggle_test.csv must contain 'WeightCategory'.\")\n",
    "\n",
    "y_true = kdf[\"WeightCategory\"].copy()\n",
    "X_k = kdf.drop(columns=[\"WeightCategory\"], errors=\"ignore\").copy()\n",
    "if id_col and id_col in X_k.columns:\n",
    "    X_k.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "# same drops + engineered features + BMI\n",
    "for c in [\"MTRANS\",\"SMOKE\"]:\n",
    "    if c in X_k.columns: X_k.drop(columns=[c], inplace=True)\n",
    "\n",
    "# Recreate the same engineered features for Kaggle_test\n",
    "# FAVC/FCVC -> favc_fcvc_interaction\n",
    "if \"FAVC\" in X_k.columns:\n",
    "    favc_map = {\"yes\":1, \"no\":0}\n",
    "    favc_num = X_k[\"FAVC\"].astype(str).str.strip().str.lower().map(favc_map)\n",
    "    X_k[\"FAVC_num\"] = favc_num\n",
    "if (\"FAVC_num\" in X_k.columns) and (\"FCVC\" in X_k.columns):\n",
    "    X_k[\"favc_fcvc_interaction\"] = pd.to_numeric(X_k[\"FAVC_num\"], errors=\"coerce\") * pd.to_numeric(X_k[\"FCVC\"], errors=\"coerce\")\n",
    "for c in [\"FAVC\",\"FAVC_num\",\"FCVC\"]:\n",
    "    if c in X_k.columns: X_k.drop(columns=[c], inplace=True)\n",
    "\n",
    "# activity_balance = FAF/(TUE+1e-6); drop TUE\n",
    "if (\"FAF\" in X_k.columns) and (\"TUE\" in X_k.columns):\n",
    "    X_k[\"activity_balance\"] = pd.to_numeric(X_k[\"FAF\"], errors=\"coerce\") / (pd.to_numeric(X_k[\"TUE\"], errors=\"coerce\") + 1e-6)\n",
    "if \"TUE\" in X_k.columns: X_k.drop(columns=[\"TUE\"], inplace=True)\n",
    "\n",
    "# meal_balance = NCP * CAEC_num; drop NCP, CAEC\n",
    "if \"CAEC\" in X_k.columns:\n",
    "    s = X_k[\"CAEC\"].astype(str).str.strip().str.lower()\n",
    "    caec_num = s.map({'no':0, 'sometimes':1, 'frequently':2, 'always':3})\n",
    "    X_k[\"CAEC_num\"] = caec_num\n",
    "if (\"NCP\" in X_k.columns) and (\"CAEC_num\" in X_k.columns):\n",
    "    X_k[\"meal_balance\"] = pd.to_numeric(X_k[\"NCP\"], errors=\"coerce\") * pd.to_numeric(X_k[\"CAEC_num\"], errors=\"coerce\")\n",
    "for c in [\"NCP\",\"CAEC\"]:\n",
    "    if c in X_k.columns: X_k.drop(columns=[c], inplace=True)\n",
    "\n",
    "# BMI\n",
    "X_k = add_bmi(X_k)\n",
    "\n",
    "# detect gender and split for Kaggle set\n",
    "gender_col_k = detect_gender_column(X_k)\n",
    "if gender_col_k is None:\n",
    "    raise ValueError(\"Could not detect a gender column in Kaggle_test.csv\")\n",
    "km_k, kf_k = split_by_gender(X_k[gender_col_k])\n",
    "\n",
    "# Predict on Kaggle by reusing the same training procedure (per gender)\n",
    "kaggle_pred_proba = np.zeros((len(X_k), len(classes)), dtype=np.float32)\n",
    "\n",
    "# Build group frames aligned with training columns\n",
    "def align_cols_for_inference(df_like_train, df_to_align):\n",
    "    use_cols = [c for c in df_like_train.columns if c in df_to_align.columns]\n",
    "    missing = [c for c in df_like_train.columns if c not in df_to_align.columns]\n",
    "    tmp = df_to_align[use_cols].copy()\n",
    "    for m in missing:\n",
    "        tmp[m] = np.nan\n",
    "    return tmp[df_like_train.columns]\n",
    "\n",
    "if X_male.shape[0] > 0 and km_k.sum() > 0:\n",
    "    X_k_m = align_cols_for_inference(X_male, X_k[km_k].reset_index(drop=True))\n",
    "    _, male_k_pred = train_group_and_predict(X_male, y_male_enc, X_k_m, \"MALE (Kaggle)\")\n",
    "    kaggle_pred_proba[km_k.values] = male_k_pred\n",
    "if X_female.shape[0] > 0 and kf_k.sum() > 0:\n",
    "    X_k_f = align_cols_for_inference(X_female, X_k[kf_k].reset_index(drop=True))\n",
    "    _, female_k_pred = train_group_and_predict(X_female, y_female_enc, X_k_f, \"FEMALE (Kaggle)\")\n",
    "    kaggle_pred_proba[kf_k.values] = female_k_pred\n",
    "\n",
    "kaggle_pred_idx = np.argmax(kaggle_pred_proba, axis=1)\n",
    "y_pred = le.inverse_transform(kaggle_pred_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce237dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Overall Accuracy on Kaggle_test: 0.90928\n",
      "\n",
      "=== Confusion Matrix (counts) ===\n",
      "Predicted →\n",
      "True ↓\n",
      "Insufficient_Weight   :  619 |   31 |    3 |    0 |    0 |    0 |    0\n",
      "Normal_Weight         :   42 |  651 |   37 |    6 |    1 |    0 |    0\n",
      "Overweight_Level_I    :    4 |   52 |  449 |   69 |    9 |    0 |    0\n",
      "Overweight_Level_II   :    0 |   16 |   52 |  526 |   42 |    5 |    0\n",
      "Obesity_Type_I        :    1 |    1 |   10 |   50 |  624 |   15 |    2\n",
      "Obesity_Type_II       :    0 |    0 |    2 |    5 |   17 |  821 |    0\n",
      "Obesity_Type_III      :    0 |    0 |    1 |    0 |    1 |    0 | 1061\n",
      "\n",
      "=== Confusion Matrix (row-normalized) ===\n",
      "Insufficient_Weight   : 0.95 | 0.05 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00\n",
      "Normal_Weight         : 0.06 | 0.88 | 0.05 | 0.01 | 0.00 | 0.00 | 0.00\n",
      "Overweight_Level_I    : 0.01 | 0.09 | 0.77 | 0.12 | 0.02 | 0.00 | 0.00\n",
      "Overweight_Level_II   : 0.00 | 0.02 | 0.08 | 0.82 | 0.07 | 0.01 | 0.00\n",
      "Obesity_Type_I        : 0.00 | 0.00 | 0.01 | 0.07 | 0.89 | 0.02 | 0.00\n",
      "Obesity_Type_II       : 0.00 | 0.00 | 0.00 | 0.01 | 0.02 | 0.97 | 0.00\n",
      "Obesity_Type_III      : 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00\n",
      "\n",
      "=== Per-class metrics ===\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight     0.9294    0.9479    0.9386       653\n",
      "      Normal_Weight     0.8668    0.8833    0.8750       737\n",
      " Overweight_Level_I     0.8105    0.7702    0.7898       583\n",
      "Overweight_Level_II     0.8018    0.8206    0.8111       641\n",
      "     Obesity_Type_I     0.8991    0.8876    0.8933       703\n",
      "    Obesity_Type_II     0.9762    0.9716    0.9739       845\n",
      "   Obesity_Type_III     0.9981    0.9981    0.9981      1063\n",
      "\n",
      "           accuracy                         0.9093      5225\n",
      "          macro avg     0.8974    0.8970    0.8971      5225\n",
      "       weighted avg     0.9091    0.9093    0.9091      5225\n",
      "\n",
      "\n",
      "=== Per-class accuracy (diagonal/row total) ===\n",
      "Insufficient_Weight    | Correct: 619 / 653 |  94.79%\n",
      "Normal_Weight          | Correct: 651 / 737 |  88.33%\n",
      "Overweight_Level_I     | Correct: 449 / 583 |  77.02%\n",
      "Overweight_Level_II    | Correct: 526 / 641 |  82.06%\n",
      "Obesity_Type_I         | Correct: 624 / 703 |  88.76%\n",
      "Obesity_Type_II        | Correct: 821 / 845 |  97.16%\n",
      "Obesity_Type_III       | Correct: 1061 / 1063 |  99.81%\n",
      "\n",
      "=== Most common confusions (true → predicted) ===\n",
      "Overweight_Level_I        → Overweight_Level_II       | Count:  69 | Row%:  11.8\n",
      "Overweight_Level_I        → Normal_Weight             | Count:  52 | Row%:   8.9\n",
      "Overweight_Level_II       → Overweight_Level_I        | Count:  52 | Row%:   8.1\n",
      "Obesity_Type_I            → Overweight_Level_II       | Count:  50 | Row%:   7.1\n",
      "Overweight_Level_II       → Obesity_Type_I            | Count:  42 | Row%:   6.6\n",
      "Normal_Weight             → Insufficient_Weight       | Count:  42 | Row%:   5.7\n",
      "Normal_Weight             → Overweight_Level_I        | Count:  37 | Row%:   5.0\n",
      "Insufficient_Weight       → Normal_Weight             | Count:  31 | Row%:   4.7\n",
      "Obesity_Type_II           → Obesity_Type_I            | Count:  17 | Row%:   2.0\n",
      "Overweight_Level_II       → Normal_Weight             | Count:  16 | Row%:   2.5\n",
      "\n",
      "=== Sample of misclassified rows (first 10) ===\n",
      "Row    9: true=Overweight_Level_II    pred=Obesity_Type_I         conf=0.893 2nd=Overweight_Level_II   (0.056)\n",
      "Row   16: true=Overweight_Level_I     pred=Normal_Weight          conf=0.938 2nd=Overweight_Level_I    (0.048)\n",
      "Row   28: true=Normal_Weight          pred=Insufficient_Weight    conf=0.811 2nd=Normal_Weight         (0.180)\n",
      "Row   30: true=Overweight_Level_I     pred=Overweight_Level_II    conf=0.729 2nd=Overweight_Level_I    (0.213)\n",
      "Row   33: true=Overweight_Level_I     pred=Normal_Weight          conf=0.934 2nd=Overweight_Level_I    (0.046)\n",
      "Row   43: true=Obesity_Type_II        pred=Obesity_Type_I         conf=0.702 2nd=Obesity_Type_II       (0.279)\n",
      "Row   61: true=Obesity_Type_I         pred=Overweight_Level_II    conf=0.830 2nd=Obesity_Type_I        (0.137)\n",
      "Row   65: true=Overweight_Level_I     pred=Overweight_Level_II    conf=0.765 2nd=Obesity_Type_I        (0.160)\n",
      "Row   75: true=Overweight_Level_I     pred=Normal_Weight          conf=0.922 2nd=Overweight_Level_I    (0.072)\n",
      "Row   90: true=Obesity_Type_I         pred=Overweight_Level_II    conf=0.777 2nd=Obesity_Type_I        (0.181)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------- Overall accuracy to 5 decimals --------\n",
    "overall_acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\n✅ Overall Accuracy on Kaggle_test: {overall_acc:.5f}\")\n",
    "\n",
    "# -------- Text-only error analysis (custom order) --------\n",
    "order = [\n",
    "    'Insufficient_Weight',\n",
    "    'Normal_Weight',\n",
    "    'Overweight_Level_I',\n",
    "    'Overweight_Level_II',\n",
    "    'Obesity_Type_I',\n",
    "    'Obesity_Type_II',\n",
    "    'Obesity_Type_III'\n",
    "]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=order)\n",
    "cm_norm = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (counts) ===\")\n",
    "print(\"Predicted →\")\n",
    "print(\"True ↓\")\n",
    "for i, true_class in enumerate(order):\n",
    "    row = \" | \".join(f\"{cm[i, j]:4d}\" for j in range(len(order)))\n",
    "    print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (row-normalized) ===\")\n",
    "for i, true_class in enumerate(order):\n",
    "    row = \" | \".join(f\"{cm_norm[i, j]:.2f}\" for j in range(len(order)))\n",
    "    print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "print(\"\\n=== Per-class metrics ===\")\n",
    "try:\n",
    "    print(classification_report(y_true, y_pred, labels=order, target_names=order, digits=4, zero_division=0))\n",
    "except Exception as e:\n",
    "    print(f\"[Info] classification_report fallback: {e}\")\n",
    "    print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "print(\"\\n=== Per-class accuracy (diagonal/row total) ===\")\n",
    "for i, c in enumerate(order):\n",
    "    total = cm[i].sum()\n",
    "    correct = cm[i, i]\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"{c:<22} | Correct: {correct:3d} / {total:3d} | {acc*100:6.2f}%\")\n",
    "\n",
    "print(\"\\n=== Most common confusions (true → predicted) ===\")\n",
    "pairs = []\n",
    "for i, t in enumerate(order):\n",
    "    for j, p in enumerate(order):\n",
    "        if i == j or cm[i, j] == 0:\n",
    "            continue\n",
    "        pairs.append((cm[i, j], t, p, cm_norm[i, j]))\n",
    "pairs = sorted(pairs, key=lambda x: (-x[0], -x[3]))\n",
    "for cnt, true_label, pred_label, norm_val in pairs[:10]:\n",
    "    print(f\"{true_label:25} → {pred_label:25} | Count: {cnt:3d} | Row%: {norm_val*100:5.1f}\")\n",
    "\n",
    "print(\"\\n=== Sample of misclassified rows (first 10) ===\")\n",
    "mis_idx = np.where(np.asarray(y_true) != np.asarray(y_pred))[0]\n",
    "if len(mis_idx) == 0:\n",
    "    print(\"🎉 No misclassifications!\")\n",
    "else:\n",
    "    for idx in mis_idx[:10]:\n",
    "        true_lab = y_true.iloc[idx] if hasattr(y_true, \"iloc\") else y_true[idx]\n",
    "        pred_lab = y_pred[idx]\n",
    "        conf = float(np.max(kaggle_pred_proba[idx]))\n",
    "        rank = np.argsort(-kaggle_pred_proba[idx])\n",
    "        second_idx = rank[1] if rank.size > 1 else rank[0]\n",
    "        second_lab = le.inverse_transform([second_idx])[0]\n",
    "        second_conf = float(kaggle_pred_proba[idx][second_idx])\n",
    "        print(f\"Row {idx:4d}: true={true_lab:<22} pred={pred_lab:<22} conf={conf:.3f} 2nd={second_lab:<22}({second_conf:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e217904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b2dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24c04db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Kaggle_test] Normal_Weight misclassified as Insufficient_Weight: 42 rows\n",
      "         id  Gender        Age    Height     Weight  \\\n",
      "28    15561  Female  25.000000  1.600000  45.000000   \n",
      "426   15959    Male  18.000000  1.800000  55.000000   \n",
      "804   16337    Male  20.000000  1.610000  53.000000   \n",
      "839   16372    Male  18.000000  1.720000  52.000000   \n",
      "913   16446  Female  26.000000  1.640000  49.000000   \n",
      "920   16453    Male  20.000000  1.830000  49.000000   \n",
      "1198  16731    Male  17.000000  1.800000  60.000000   \n",
      "1291  16824    Male  18.000000  1.700000  52.000000   \n",
      "1396  16929    Male  19.000000  1.800000  58.000000   \n",
      "1620  17153  Female  20.979254  1.546665  41.890204   \n",
      "\n",
      "     family_history_with_overweight FAVC      FCVC  NCP        CAEC  ...  \\\n",
      "28                               no  yes  3.000000  3.0  Frequently  ...   \n",
      "426                             yes  yes  3.000000  3.0  Frequently  ...   \n",
      "804                             yes  yes  2.000000  3.0  Frequently  ...   \n",
      "839                             yes  yes  1.000000  3.0   Sometimes  ...   \n",
      "913                              no  yes  3.000000  3.0   Sometimes  ...   \n",
      "920                             yes  yes  2.000000  3.0   Sometimes  ...   \n",
      "1198                             no  yes  2.000000  4.0      Always  ...   \n",
      "1291                            yes  yes  2.000000  3.0  Frequently  ...   \n",
      "1396                            yes  yes  2.000000  3.0   Sometimes  ...   \n",
      "1620                             no  yes  2.027574  1.0  Frequently  ...   \n",
      "\n",
      "     WeightCategory     true_label           pred_label    p_pred    p_true  \\\n",
      "28    Normal_Weight  Normal_Weight  Insufficient_Weight  0.810680  0.180275   \n",
      "426   Normal_Weight  Normal_Weight  Insufficient_Weight  0.906548  0.084405   \n",
      "804   Normal_Weight  Normal_Weight  Insufficient_Weight  0.626068  0.367894   \n",
      "839   Normal_Weight  Normal_Weight  Insufficient_Weight  0.976643  0.012054   \n",
      "913   Normal_Weight  Normal_Weight  Insufficient_Weight  0.494541  0.491967   \n",
      "920   Normal_Weight  Normal_Weight  Insufficient_Weight  0.905480  0.086063   \n",
      "1198  Normal_Weight  Normal_Weight  Insufficient_Weight  0.593720  0.380583   \n",
      "1291  Normal_Weight  Normal_Weight  Insufficient_Weight  0.682562  0.306809   \n",
      "1396  Normal_Weight  Normal_Weight  Insufficient_Weight  0.555409  0.397426   \n",
      "1620  Normal_Weight  Normal_Weight  Insufficient_Weight  0.995690  0.002388   \n",
      "\n",
      "                     top1           top2 top1_prob top2_prob    margin  \n",
      "28    Insufficient_Weight  Normal_Weight  0.810680  0.180275  0.630405  \n",
      "426   Insufficient_Weight  Normal_Weight  0.906548  0.084405  0.822143  \n",
      "804   Insufficient_Weight  Normal_Weight  0.626068  0.367894  0.258174  \n",
      "839   Insufficient_Weight  Normal_Weight  0.976643  0.012054  0.964590  \n",
      "913   Insufficient_Weight  Normal_Weight  0.494541  0.491967  0.002574  \n",
      "920   Insufficient_Weight  Normal_Weight  0.905480  0.086063  0.819417  \n",
      "1198  Insufficient_Weight  Normal_Weight  0.593720  0.380583  0.213137  \n",
      "1291  Insufficient_Weight  Normal_Weight  0.682562  0.306809  0.375753  \n",
      "1396  Insufficient_Weight  Normal_Weight  0.555409  0.397426  0.157983  \n",
      "1620  Insufficient_Weight  Normal_Weight  0.995690  0.002388  0.993302  \n",
      "\n",
      "[10 rows x 27 columns]\n",
      "[Kaggle_test] Saved: miscls_kaggle_normal_to_insufficient.csv\n"
     ]
    }
   ],
   "source": [
    "# ---- Misclassifications on Kaggle_test: Normal_Weight -> Insufficient_Weight ----\n",
    "target_true = \"Normal_Weight\"\n",
    "target_pred = \"Insufficient_Weight\"\n",
    "\n",
    "mask_k = (y_true == target_true) & (y_pred == target_pred)\n",
    "idx_k = np.where(mask_k)[0]\n",
    "\n",
    "print(f\"\\n[Kaggle_test] {target_true} misclassified as {target_pred}: {len(idx_k)} rows\")\n",
    "\n",
    "if len(idx_k) > 0:\n",
    "    # Grab original rows (with all original columns) from kdf\n",
    "    mis_k_df = kdf.iloc[idx_k].copy()\n",
    "\n",
    "    # Attach prediction diagnostics\n",
    "    pred_probs = kaggle_pred_proba[idx_k]\n",
    "    pred_idx = np.argmax(pred_probs, axis=1)\n",
    "    pred_lab = le.inverse_transform(pred_idx)\n",
    "\n",
    "    # probability of predicted and true classes\n",
    "    cls_to_idx = {c: i for i, c in enumerate(le.classes_)}\n",
    "    p_pred = pred_probs[np.arange(len(idx_k)), [cls_to_idx[l] for l in pred_lab]]\n",
    "    p_true = pred_probs[:, cls_to_idx[target_true]]\n",
    "\n",
    "    # top-2 info\n",
    "    top2_idx = np.argsort(-pred_probs, axis=1)[:, :2]\n",
    "    top2_labels = np.column_stack([le.inverse_transform(top2_idx[:, 0]),\n",
    "                                   le.inverse_transform(top2_idx[:, 1])])\n",
    "    top1_prob = pred_probs[np.arange(len(idx_k)), top2_idx[:, 0]]\n",
    "    top2_prob = pred_probs[np.arange(len(idx_k)), top2_idx[:, 1]]\n",
    "    margin = top1_prob - top2_prob\n",
    "\n",
    "    mis_k_df[\"true_label\"] = target_true\n",
    "    mis_k_df[\"pred_label\"] = pred_lab\n",
    "    mis_k_df[\"p_pred\"] = p_pred\n",
    "    mis_k_df[\"p_true\"] = p_true\n",
    "    mis_k_df[\"top1\"] = top2_labels[:, 0]\n",
    "    mis_k_df[\"top2\"] = top2_labels[:, 1]\n",
    "    mis_k_df[\"top1_prob\"] = top1_prob\n",
    "    mis_k_df[\"top2_prob\"] = top2_prob\n",
    "    mis_k_df[\"margin\"] = margin\n",
    "\n",
    "    # Show a few\n",
    "    print(mis_k_df.head(10))\n",
    "\n",
    "    # Save them for inspection\n",
    "    mis_k_path = \"miscls_kaggle_normal_to_insufficient.csv\"\n",
    "    mis_k_df.to_csv(mis_k_path, index=False)\n",
    "    print(f\"[Kaggle_test] Saved: {mis_k_path}\")\n",
    "else:\n",
    "    print(\"[Kaggle_test] None found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed938619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881d868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3eddb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d6e1bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Engineered features added: favc_fcvc_interaction, activity_balance, meal_balance\n",
      "[Info] Dropped: SMOKE, MTRANS, FAVC, FCVC, TUE, NCP, CAEC\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Gender-specific LightGBM + BMI + requested feature engineering + Kaggle_test eval\n",
    "# ==============================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.base import clone\n",
    "\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "import logging\n",
    "logging.getLogger('lightgbm').setLevel(logging.ERROR)\n",
    "\n",
    "# -------- Paths --------\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "SAMPLE_SUB_PATH = \"sample_submission.csv\"\n",
    "KAGGLE_TEST_PATH = \"Kaggle_test.csv\"  # must contain WeightCategory ground truth\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "N_JOBS = -1\n",
    "\n",
    "# Your LightGBM hyperparameters\n",
    "best_params = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"verbosity\": -1,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"random_state\": 42,\n",
    "    \"num_class\": 7,\n",
    "    \"learning_rate\": 0.030962211546832760,\n",
    "    \"n_estimators\": 500,\n",
    "    \"lambda_l1\": 0.009667446568254372,\n",
    "    \"lambda_l2\": 0.04018641437301800,\n",
    "    \"max_depth\": 10,\n",
    "    \"colsample_bytree\": 0.40977129346872643,\n",
    "    \"subsample\": 0.9535797422450176,\n",
    "    \"min_child_samples\": 26\n",
    "}\n",
    "\n",
    "# -------- Helpers --------\n",
    "def norm_col(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    return str(s).replace(\"\\ufeff\", \"\").strip().lower()\n",
    "\n",
    "def infer_feature_types(df):\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def detect_gender_column(df):\n",
    "    for c in df.columns:\n",
    "        if norm_col(c) in {\"gender\", \"sex\"}:\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        vals = pd.Series(df[c].dropna().astype(str).str.lower().str.strip()).unique()\n",
    "        if len(vals) in (2, 3):\n",
    "            if any(v.startswith(\"m\") for v in vals) and any(v.startswith(\"f\") for v in vals):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def split_by_gender(series):\n",
    "    s = series.astype(str).str.lower().str.strip()\n",
    "    male_mask = s.str.startswith((\"m\",\"1\",\"true\"))\n",
    "    female_mask = s.str.startswith((\"f\",\"0\",\"false\"))\n",
    "    if male_mask.sum()==0 and female_mask.sum()==0:\n",
    "        top = s.value_counts().index.tolist()\n",
    "        if len(top)>=2:\n",
    "            male_mask = s==top[0]\n",
    "            female_mask = s==top[1]\n",
    "    return male_mask, female_mask\n",
    "\n",
    "def add_bmi(df):\n",
    "    \"\"\"BMI = Weight / (Height_m^2). If median height > 3 assume cm → meters.\"\"\"\n",
    "    if (\"Weight\" in df.columns) and (\"Height\" in df.columns):\n",
    "        h = pd.to_numeric(df[\"Height\"], errors=\"coerce\")\n",
    "        height_m = np.where(np.nanmedian(h) > 3.0, h / 100.0, h)\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            w = pd.to_numeric(df[\"Weight\"], errors=\"coerce\")\n",
    "            bmi = w / (np.power(height_m, 2) + 1e-12)\n",
    "        df[\"BMI\"] = pd.Series(bmi).replace([np.inf, -np.inf], np.nan)\n",
    "    return df\n",
    "\n",
    "def safe_lower_str_col(s):\n",
    "    return s.astype(str).str.strip().str.lower().where(s.notna(), other=np.nan)\n",
    "\n",
    "# -------- Load data --------\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "# ===========================================================\n",
    "# Feature Engineering (requested)\n",
    "# ===========================================================\n",
    "# 1) Drop SMOKE, MTRANS\n",
    "for c in [\"MTRANS\",\"SMOKE\"]:\n",
    "    if c in train.columns: train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:  test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# 2) FAVC * FCVC interaction, then drop FAVC/FCVC\n",
    "if \"FAVC\" in train.columns:\n",
    "    favc_map = {\"yes\":1, \"no\":0}\n",
    "    train[\"FAVC_num\"] = safe_lower_str_col(train[\"FAVC\"]).map(favc_map)\n",
    "if \"FAVC\" in test.columns:\n",
    "    favc_map = {\"yes\":1, \"no\":0}\n",
    "    test[\"FAVC_num\"]  = safe_lower_str_col(test[\"FAVC\"]).map(favc_map)\n",
    "\n",
    "if (\"FAVC_num\" in train.columns) and (\"FCVC\" in train.columns):\n",
    "    train[\"favc_fcvc_interaction\"] = pd.to_numeric(train[\"FAVC_num\"], errors=\"coerce\") * pd.to_numeric(train[\"FCVC\"], errors=\"coerce\")\n",
    "if (\"FAVC_num\" in test.columns) and (\"FCVC\" in test.columns):\n",
    "    test[\"favc_fcvc_interaction\"]  = pd.to_numeric(test[\"FAVC_num\"], errors=\"coerce\")  * pd.to_numeric(test[\"FCVC\"], errors=\"coerce\")\n",
    "\n",
    "for c in [\"FAVC\",\"FAVC_num\",\"FCVC\"]:\n",
    "    if c in train.columns: train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:  test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# 3) activity_balance = FAF / (TUE + 1e-6); keep FAF, drop TUE\n",
    "if (\"FAF\" in train.columns) and (\"TUE\" in train.columns):\n",
    "    train[\"activity_balance\"] = pd.to_numeric(train[\"FAF\"], errors=\"coerce\") / (pd.to_numeric(train[\"TUE\"], errors=\"coerce\") + 1e-6)\n",
    "if (\"FAF\" in test.columns) and (\"TUE\" in test.columns):\n",
    "    test[\"activity_balance\"]  = pd.to_numeric(test[\"FAF\"], errors=\"coerce\")  / (pd.to_numeric(test[\"TUE\"], errors=\"coerce\")  + 1e-6)\n",
    "\n",
    "for c in [\"TUE\"]:\n",
    "    if c in train.columns: train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:  test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# 4) meal_balance = NCP * CAEC_num; drop NCP & CAEC\n",
    "def map_caec(series):\n",
    "    s = series.astype(str).str.strip().str.lower()\n",
    "    return s.map({'no':0, 'sometimes':1, 'frequently':2, 'always':3})\n",
    "\n",
    "if \"CAEC\" in train.columns:\n",
    "    train[\"CAEC_num\"] = map_caec(train[\"CAEC\"])\n",
    "if \"CAEC\" in test.columns:\n",
    "    test[\"CAEC_num\"]  = map_caec(test[\"CAEC\"])\n",
    "\n",
    "if (\"NCP\" in train.columns) and (\"CAEC_num\" in train.columns):\n",
    "    train[\"meal_balance\"] = pd.to_numeric(train[\"NCP\"], errors=\"coerce\") * pd.to_numeric(train[\"CAEC_num\"], errors=\"coerce\")\n",
    "if (\"NCP\" in test.columns) and (\"CAEC_num\" in test.columns):\n",
    "    test[\"meal_balance\"]  = pd.to_numeric(test[\"NCP\"], errors=\"coerce\")  * pd.to_numeric(test[\"CAEC_num\"], errors=\"coerce\")\n",
    "\n",
    "for c in [\"NCP\",\"CAEC\"]:\n",
    "    if c in train.columns: train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:  test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# 5) BMI\n",
    "train = add_bmi(train)\n",
    "test  = add_bmi(test)\n",
    "\n",
    "print(\"[Info] Engineered features added: favc_fcvc_interaction, activity_balance, meal_balance\")\n",
    "print(\"[Info] Dropped: SMOKE, MTRANS, FAVC, FCVC, TUE, NCP, CAEC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f894c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Classes: ['Insufficient_Weight', 'Normal_Weight', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III', 'Overweight_Level_I', 'Overweight_Level_II']\n",
      "[Info] Train male=7783, female=7750\n",
      "[Info] Test  male=10336, female=10422\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------- Detect ID/Target\n",
    "id_col = None\n",
    "for cand in [\"id\", \"row_id\", \"index\", \"sample_id\"]:\n",
    "    if cand in train.columns and cand in test.columns:\n",
    "        id_col = cand\n",
    "        break\n",
    "\n",
    "target_col = None\n",
    "for cand in [\"WeightCategory\", \"NObeyesdad\", \"label\", \"target\", \"class\", \"y\"]:\n",
    "    if cand in train.columns:\n",
    "        target_col = cand\n",
    "        break\n",
    "if target_col is None:\n",
    "    raise ValueError(\"Could not detect target column in train.csv\")\n",
    "\n",
    "# Build X/y\n",
    "y = train[target_col].copy()\n",
    "X = train.drop(columns=[target_col]).copy()\n",
    "if id_col and id_col in X.columns:\n",
    "    X.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "test_features = test.copy()\n",
    "if id_col and id_col in test_features.columns:\n",
    "    test_ids = test_features[id_col].copy()\n",
    "    test_features.drop(columns=[id_col], inplace=True)\n",
    "else:\n",
    "    test_ids = pd.Series(np.arange(len(test_features)), name=\"id\")\n",
    "\n",
    "# Label encode target\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "classes = list(le.classes_)\n",
    "print(f\"[Info] Classes: {classes}\")\n",
    "\n",
    "# Detect gender and split\n",
    "gender_col = detect_gender_column(pd.concat([X, test_features], axis=0))\n",
    "if gender_col is None:\n",
    "    raise ValueError(\"Could not detect a gender column (e.g., 'Gender'/'SEX').\")\n",
    "male_mask, female_mask = split_by_gender(train[gender_col])\n",
    "test_male_mask, test_female_mask = split_by_gender(test_features[gender_col])\n",
    "print(f\"[Info] Train male={int(male_mask.sum())}, female={int(female_mask.sum())}\")\n",
    "print(f\"[Info] Test  male={int(test_male_mask.sum())}, female={int(test_female_mask.sum())}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd6758e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Training function (gender-specific) using LightGBM --------\n",
    "def train_group_and_predict_LGBM(X_grp, y_enc_grp, test_grp, group_name):\n",
    "    # drop gender col inside group\n",
    "    cols_to_use = [c for c in X_grp.columns if c != gender_col]\n",
    "    Xg = X_grp[cols_to_use].copy()\n",
    "    Xtestg = test_grp[cols_to_use].copy()\n",
    "\n",
    "    num_cols, cat_cols = infer_feature_types(Xg)\n",
    "\n",
    "    # Preprocessor (OHE for cats; LightGBM can take categorical indices but OHE keeps parity with earlier code)\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))  # scaling not strictly needed for trees, but harmless\n",
    "    ])\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", ohe)\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=1.0\n",
    "    )\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof_group = np.zeros((len(Xg), len(classes)), dtype=np.float32)\n",
    "    test_group_pred = np.zeros((len(Xtestg), len(classes)), dtype=np.float32)\n",
    "    best_iters = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(Xg, y_enc_grp), start=1):\n",
    "        print(f\"\\n[{group_name}] Fold {fold}/{N_FOLDS}\")\n",
    "        X_tr, X_va = Xg.iloc[tr_idx], Xg.iloc[va_idx]\n",
    "        y_tr, y_va = y_enc_grp[tr_idx], y_enc_grp[va_idx]\n",
    "\n",
    "        prep = clone(preprocessor)\n",
    "        Xtr = prep.fit_transform(X_tr)\n",
    "        Xva = prep.transform(X_va)\n",
    "\n",
    "        # init model with provided params\n",
    "        model = LGBMClassifier(**best_params, n_jobs=N_JOBS)\n",
    "        callbacks = [early_stopping(stopping_rounds=200), log_evaluation(period=0)]\n",
    "        model.fit(\n",
    "            Xtr, y_tr,\n",
    "            eval_set=[(Xva, y_va)],\n",
    "            eval_metric=\"multi_logloss\",\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        best_it = getattr(model, \"best_iteration_\", best_params.get(\"n_estimators\", 500))\n",
    "        best_iters.append(int(best_it))\n",
    "\n",
    "        # OOF\n",
    "        proba_va = model.predict_proba(Xva, num_iteration=best_it)\n",
    "        oof_group[va_idx] = proba_va\n",
    "\n",
    "        # Test transform once per fold to match encoding\n",
    "        Xtest_tf = prep.transform(Xtestg)\n",
    "        proba_te = model.predict_proba(Xtest_tf, num_iteration=best_it)\n",
    "        test_group_pred += proba_te / N_FOLDS\n",
    "\n",
    "    # Group OOF summary\n",
    "    oof_labels_g = np.argmax(oof_group, axis=1)\n",
    "    acc_g = accuracy_score(y_enc_grp, oof_labels_g)\n",
    "    f1_g = f1_score(y_enc_grp, oof_labels_g, average=\"macro\")\n",
    "    print(f\"\\n[{group_name}] OOF Accuracy: {acc_g:.4f} | Macro F1: {f1_g:.4f}\")\n",
    "    print(f\"[{group_name}] Best iterations per fold: {best_iters} | Median: {int(np.median(best_iters))}\")\n",
    "\n",
    "    return oof_group, test_group_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40ea3b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MALE] Fold 1/5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[173]\tvalid_0's multi_logloss: 0.335092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MALE] Fold 2/5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[178]\tvalid_0's multi_logloss: 0.312167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MALE] Fold 3/5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's multi_logloss: 0.326967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MALE] Fold 4/5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[195]\tvalid_0's multi_logloss: 0.320197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MALE] Fold 5/5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[166]\tvalid_0's multi_logloss: 0.340591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MALE] OOF Accuracy: 0.8881 | Macro F1: 0.7509\n",
      "[MALE] Best iterations per fold: [173, 178, 204, 195, 166] | Median: 178\n",
      "\n",
      "[FEMALE] Fold 1/5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[172]\tvalid_0's multi_logloss: 0.251829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FEMALE] Fold 2/5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[171]\tvalid_0's multi_logloss: 0.244419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FEMALE] Fold 3/5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[143]\tvalid_0's multi_logloss: 0.277777\n",
      "\n",
      "[FEMALE] Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's multi_logloss: 0.245481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FEMALE] Fold 5/5\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[168]\tvalid_0's multi_logloss: 0.237603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FEMALE] OOF Accuracy: 0.9182 | Macro F1: 0.7514\n",
      "[FEMALE] Best iterations per fold: [172, 171, 143, 174, 168] | Median: 171\n",
      "\n",
      "========== OVERALL OOF ==========\n",
      "OOF Accuracy: 0.9031 | OOF Macro F1: 0.8936\n",
      "\n",
      "OOF Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.93      0.93      0.93      1870\n",
      "      Normal_Weight       0.87      0.88      0.88      2345\n",
      "     Obesity_Type_I       0.89      0.88      0.88      2207\n",
      "    Obesity_Type_II       0.96      0.97      0.97      2403\n",
      "   Obesity_Type_III       1.00      1.00      1.00      2983\n",
      " Overweight_Level_I       0.81      0.77      0.79      1844\n",
      "Overweight_Level_II       0.80      0.82      0.81      1881\n",
      "\n",
      "           accuracy                           0.90     15533\n",
      "          macro avg       0.89      0.89      0.89     15533\n",
      "       weighted avg       0.90      0.90      0.90     15533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------- Train per-gender and predict full test --------\n",
    "X_male = X[male_mask].reset_index(drop=True)\n",
    "y_male_enc = y_enc[male_mask]\n",
    "test_male = test_features[test_male_mask].reset_index(drop=True)\n",
    "\n",
    "X_female = X[female_mask].reset_index(drop=True)\n",
    "y_female_enc = y_enc[female_mask]\n",
    "test_female = test_features[test_female_mask].reset_index(drop=True)\n",
    "\n",
    "male_oof, male_test_pred = train_group_and_predict_LGBM(X_male, y_male_enc, test_male, \"MALE\")\n",
    "female_oof, female_test_pred = train_group_and_predict_LGBM(X_female, y_female_enc, test_female, \"FEMALE\")\n",
    "\n",
    "# Combine OOF\n",
    "oof_full = np.zeros((len(X), len(classes)), dtype=np.float32)\n",
    "oof_full[male_mask.values] = male_oof\n",
    "oof_full[female_mask.values] = female_oof\n",
    "\n",
    "oof_labels = np.argmax(oof_full, axis=1)\n",
    "oof_acc = accuracy_score(y_enc, oof_labels)\n",
    "oof_f1 = f1_score(y_enc, oof_labels, average=\"macro\")\n",
    "print(\"\\n========== OVERALL OOF ==========\")\n",
    "print(f\"OOF Accuracy: {oof_acc:.4f} | OOF Macro F1: {oof_f1:.4f}\")\n",
    "try:\n",
    "    print(\"\\nOOF Classification Report:\\n\",\n",
    "          classification_report(y_enc, oof_labels, target_names=classes, zero_division=0))\n",
    "except Exception as e:\n",
    "    print(f\"[Info] Could not print classification report: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87aff9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved submission.csv\n",
      "   id       WeightCategory\n",
      "0   0  Overweight_Level_II\n",
      "1   1        Normal_Weight\n",
      "2   2  Insufficient_Weight\n",
      "3   3     Obesity_Type_III\n",
      "4   4  Overweight_Level_II\n"
     ]
    }
   ],
   "source": [
    "# -------- Build submission --------\n",
    "test_pred_proba = np.zeros((len(test_features), len(classes)), dtype=np.float32)\n",
    "test_pred_proba[test_male_mask.values] = male_test_pred\n",
    "test_pred_proba[test_female_mask.values] = female_test_pred\n",
    "\n",
    "test_pred_int = np.argmax(test_pred_proba, axis=1)\n",
    "test_pred_labels = le.inverse_transform(test_pred_int)\n",
    "\n",
    "ss_cols = list(sample_sub.columns)\n",
    "ID_HEADER = None\n",
    "LABEL_HEADER = None\n",
    "if len(ss_cols) == 2:\n",
    "    c1, c2 = ss_cols\n",
    "    if c1 in test.columns and c2 not in test.columns:\n",
    "        ID_HEADER, LABEL_HEADER = c1, c2\n",
    "    elif c2 in test.columns and c1 not in test.columns:\n",
    "        ID_HEADER, LABEL_HEADER = c2, c1\n",
    "if ID_HEADER is None:\n",
    "    ID_HEADER = ss_cols[0]\n",
    "    LABEL_HEADER = ss_cols[1]\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "if ID_HEADER in test.columns:\n",
    "    sub[ID_HEADER] = test[ID_HEADER].values\n",
    "else:\n",
    "    sub[ID_HEADER] = np.arange(len(test_features))\n",
    "sub[LABEL_HEADER] = test_pred_labels\n",
    "\n",
    "for c in ss_cols:\n",
    "    if c not in sub.columns:\n",
    "        sub[c] = sample_sub[c].iloc[0] if len(sample_sub[c]) else None\n",
    "sub = sub[ss_cols]\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSaved submission.csv\")\n",
    "print(sub.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63cdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Kaggle] Trained MALE on 7783 rows; infer 2553 rows.\n",
      "[Kaggle] Trained FEMALE on 7750 rows; infer 2672 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================\n",
    "# Evaluate on Kaggle_test.csv (with ground truth)\n",
    "# ==============================================\n",
    "kdf = pd.read_csv(KAGGLE_TEST_PATH)\n",
    "if \"WeightCategory\" not in kdf.columns:\n",
    "    raise KeyError(\"Kaggle_test.csv must contain 'WeightCategory'.\")\n",
    "\n",
    "y_true = kdf[\"WeightCategory\"].copy()\n",
    "X_k = kdf.drop(columns=[\"WeightCategory\"], errors=\"ignore\").copy()\n",
    "if id_col and id_col in X_k.columns:\n",
    "    X_k.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "# Recreate same engineered features on Kaggle_test\n",
    "for c in [\"MTRANS\",\"SMOKE\"]:\n",
    "    if c in X_k.columns: X_k.drop(columns=[c], inplace=True)\n",
    "\n",
    "# FAVC/FCVC -> interaction\n",
    "if \"FAVC\" in X_k.columns:\n",
    "    favc_num = safe_lower_str_col(X_k[\"FAVC\"]).map({\"yes\":1,\"no\":0})\n",
    "    X_k[\"FAVC_num\"] = favc_num\n",
    "if (\"FAVC_num\" in X_k.columns) and (\"FCVC\" in X_k.columns):\n",
    "    X_k[\"favc_fcvc_interaction\"] = pd.to_numeric(X_k[\"FAVC_num\"], errors=\"coerce\") * pd.to_numeric(X_k[\"FCVC\"], errors=\"coerce\")\n",
    "for c in [\"FAVC\",\"FAVC_num\",\"FCVC\"]:\n",
    "    if c in X_k.columns: X_k.drop(columns=[c], inplace=True)\n",
    "\n",
    "# activity_balance\n",
    "if (\"FAF\" in X_k.columns) and (\"TUE\" in X_k.columns):\n",
    "    X_k[\"activity_balance\"] = pd.to_numeric(X_k[\"FAF\"], errors=\"coerce\") / (pd.to_numeric(X_k[\"TUE\"], errors=\"coerce\") + 1e-6)\n",
    "if \"TUE\" in X_k.columns: X_k.drop(columns=[\"TUE\"], inplace=True)\n",
    "\n",
    "# meal_balance\n",
    "if \"CAEC\" in X_k.columns:\n",
    "    X_k[\"CAEC_num\"] = map_caec(X_k[\"CAEC\"])\n",
    "if (\"NCP\" in X_k.columns) and (\"CAEC_num\" in X_k.columns):\n",
    "    X_k[\"meal_balance\"] = pd.to_numeric(X_k[\"NCP\"], errors=\"coerce\") * pd.to_numeric(X_k[\"CAEC_num\"], errors=\"coerce\")\n",
    "for c in [\"NCP\",\"CAEC\"]:\n",
    "    if c in X_k.columns: X_k.drop(columns=[c], inplace=True)\n",
    "\n",
    "# BMI\n",
    "X_k = add_bmi(X_k)\n",
    "\n",
    "# detect gender split for Kaggle set\n",
    "gender_col_k = detect_gender_column(X_k)\n",
    "if gender_col_k is None:\n",
    "    raise ValueError(\"Could not detect a gender column in Kaggle_test.csv\")\n",
    "km_k, kf_k = split_by_gender(X_k[gender_col_k])\n",
    "\n",
    "# ---- Re-train each gender model on full train-group and infer on Kaggle subset ----\n",
    "def align_cols_like(train_like_df, to_align_df):\n",
    "    use_cols = [c for c in train_like_df.columns if c in to_align_df.columns]\n",
    "    missing = [c for c in train_like_df.columns if c not in to_align_df.columns]\n",
    "    tmp = to_align_df[use_cols].copy()\n",
    "    for m in missing:\n",
    "        tmp[m] = np.nan\n",
    "    return tmp[train_like_df.columns]\n",
    "\n",
    "kaggle_pred_proba = np.zeros((len(X_k), len(classes)), dtype=np.float32)\n",
    "\n",
    "def train_full_and_predict_LGBM(X_full, y_full, X_eval, name):\n",
    "    # drop gender col inside group\n",
    "    cols_to_use = [c for c in X_full.columns if c != gender_col]\n",
    "    Xf = X_full[cols_to_use].copy()\n",
    "    Xe = X_eval[cols_to_use].copy()\n",
    "\n",
    "    num_cols, cat_cols = infer_feature_types(Xf)\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", ohe)\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=1.0\n",
    "    )\n",
    "\n",
    "    prep = clone(preprocessor)\n",
    "    Xtr = prep.fit_transform(Xf)\n",
    "    Xev = prep.transform(Xe)\n",
    "\n",
    "    model = LGBMClassifier(**best_params, n_jobs=N_JOBS)\n",
    "\n",
    "    # Full-train: no eval_set here (no early stopping). Older LightGBM versions don't accept 'verbose' kw.\n",
    "    model.fit(Xtr, y_full)\n",
    "\n",
    "    # If no validation was used, best_iteration_ may be None. Fall back to default (all trees).\n",
    "    best_it = getattr(model, \"best_iteration_\", None)\n",
    "    if best_it is not None:\n",
    "        proba = model.predict_proba(Xev, num_iteration=best_it)\n",
    "    else:\n",
    "        proba = model.predict_proba(Xev)\n",
    "\n",
    "    print(f\"[Kaggle] Trained {name} on {Xf.shape[0]} rows; infer {Xev.shape[0]} rows.\")\n",
    "    return proba\n",
    "\n",
    "# Build train groups aligned to X\n",
    "X_full_male   = X[male_mask].reset_index(drop=True)\n",
    "y_full_male   = y_enc[male_mask]\n",
    "X_full_female = X[female_mask].reset_index(drop=True)\n",
    "y_full_female = y_enc[female_mask]\n",
    "\n",
    "X_k_male   = align_cols_like(X_full_male,   X_k[km_k].reset_index(drop=True))\n",
    "X_k_female = align_cols_like(X_full_female, X_k[kf_k].reset_index(drop=True))\n",
    "\n",
    "if X_full_male.shape[0] > 0 and X_k_male.shape[0] > 0:\n",
    "    kaggle_pred_proba[km_k.values] = train_full_and_predict_LGBM(X_full_male, y_full_male, X_k_male, \"MALE\")\n",
    "if X_full_female.shape[0] > 0 and X_k_female.shape[0] > 0:\n",
    "    kaggle_pred_proba[kf_k.values] = train_full_and_predict_LGBM(X_full_female, y_full_female, X_k_female, \"FEMALE\")\n",
    "\n",
    "kaggle_pred_idx = np.argmax(kaggle_pred_proba, axis=1)\n",
    "y_pred = le.inverse_transform(kaggle_pred_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e909c391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ LightGBM Overall Accuracy on Kaggle_test: 0.90488\n",
      "\n",
      "=== Confusion Matrix (counts) ===\n",
      "Predicted →\n",
      "True ↓\n",
      "Insufficient_Weight   :  610 |   40 |    3 |    0 |    0 |    0 |    0\n",
      "Normal_Weight         :   37 |  661 |   33 |    5 |    1 |    0 |    0\n",
      "Overweight_Level_I    :    4 |   62 |  435 |   71 |   11 |    0 |    0\n",
      "Overweight_Level_II   :    0 |   16 |   61 |  505 |   54 |    5 |    0\n",
      "Obesity_Type_I        :    1 |    1 |   10 |   41 |  632 |   16 |    2\n",
      "Obesity_Type_II       :    0 |    0 |    1 |    4 |   15 |  824 |    1\n",
      "Obesity_Type_III      :    0 |    1 |    0 |    0 |    1 |    0 | 1061\n",
      "\n",
      "=== Confusion Matrix (row-normalized) ===\n",
      "Insufficient_Weight   : 0.93 | 0.06 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00\n",
      "Normal_Weight         : 0.05 | 0.90 | 0.04 | 0.01 | 0.00 | 0.00 | 0.00\n",
      "Overweight_Level_I    : 0.01 | 0.11 | 0.75 | 0.12 | 0.02 | 0.00 | 0.00\n",
      "Overweight_Level_II   : 0.00 | 0.02 | 0.10 | 0.79 | 0.08 | 0.01 | 0.00\n",
      "Obesity_Type_I        : 0.00 | 0.00 | 0.01 | 0.06 | 0.90 | 0.02 | 0.00\n",
      "Obesity_Type_II       : 0.00 | 0.00 | 0.00 | 0.00 | 0.02 | 0.98 | 0.00\n",
      "Obesity_Type_III      : 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00\n",
      "\n",
      "=== Per-class metrics ===\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight     0.9356    0.9342    0.9349       653\n",
      "      Normal_Weight     0.8464    0.8969    0.8709       737\n",
      " Overweight_Level_I     0.8011    0.7461    0.7726       583\n",
      "Overweight_Level_II     0.8067    0.7878    0.7972       641\n",
      "     Obesity_Type_I     0.8852    0.8990    0.8920       703\n",
      "    Obesity_Type_II     0.9751    0.9751    0.9751       845\n",
      "   Obesity_Type_III     0.9972    0.9981    0.9976      1063\n",
      "\n",
      "           accuracy                         0.9049      5225\n",
      "          macro avg     0.8925    0.8910    0.8915      5225\n",
      "       weighted avg     0.9043    0.9049    0.9044      5225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------- Overall accuracy to 5 decimals --------\n",
    "overall_acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\n✅ LightGBM Overall Accuracy on Kaggle_test: {overall_acc:.5f}\")\n",
    "\n",
    "# -------- Detailed analysis --------\n",
    "order = [\n",
    "    'Insufficient_Weight',\n",
    "    'Normal_Weight',\n",
    "    'Overweight_Level_I',\n",
    "    'Overweight_Level_II',\n",
    "    'Obesity_Type_I',\n",
    "    'Obesity_Type_II',\n",
    "    'Obesity_Type_III'\n",
    "]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=order)\n",
    "cm_norm = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (counts) ===\")\n",
    "print(\"Predicted →\")\n",
    "print(\"True ↓\")\n",
    "for i, true_class in enumerate(order):\n",
    "    row = \" | \".join(f\"{cm[i, j]:4d}\" for j in range(len(order)))\n",
    "    print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (row-normalized) ===\")\n",
    "for i, true_class in enumerate(order):\n",
    "    row = \" | \".join(f\"{cm_norm[i, j]:.2f}\" for j in range(len(order)))\n",
    "    print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "print(\"\\n=== Per-class metrics ===\")\n",
    "try:\n",
    "    print(classification_report(y_true, y_pred, labels=order, target_names=order, digits=4, zero_division=0))\n",
    "except Exception as e:\n",
    "    print(f\"[Info] classification_report fallback: {e}\")\n",
    "    print(classification_report(y_true, y_pred, digits=4, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914698aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22a9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b41567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be618956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Classes: ['Insufficient_Weight', 'Normal_Weight', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III', 'Overweight_Level_I', 'Overweight_Level_II']\n",
      "[Info] Train male=7783 | female=7750\n",
      "\n",
      "[MALE] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE] best_iter=49 | Acc=0.8439 | MacroF1=0.7170\n",
      "\n",
      "[MALE] Fold 2/5\n",
      "[MALE] best_iter=277 | Acc=0.8831 | MacroF1=0.7487\n",
      "\n",
      "[MALE] Fold 3/5\n",
      "[MALE] best_iter=101 | Acc=0.8645 | MacroF1=0.7283\n",
      "\n",
      "[MALE] Fold 4/5\n",
      "[MALE] best_iter=942 | Acc=0.8817 | MacroF1=0.7443\n",
      "\n",
      "[MALE] Fold 5/5\n",
      "[MALE] best_iter=284 | Acc=0.8695 | MacroF1=0.8281\n",
      "\n",
      "[MALE] OOF Accuracy: 0.8686 | Macro F1: 0.7445\n",
      "[MALE] Best iterations per fold: [49, 277, 101, 942, 284] | Median: 277\n",
      "\n",
      "[FEMALE] Fold 1/5\n",
      "[FEMALE] best_iter=307 | Acc=0.9019 | MacroF1=0.7353\n",
      "\n",
      "[FEMALE] Fold 2/5\n",
      "[FEMALE] best_iter=122 | Acc=0.9045 | MacroF1=0.7373\n",
      "\n",
      "[FEMALE] Fold 3/5\n",
      "[FEMALE] best_iter=186 | Acc=0.8974 | MacroF1=0.7273\n",
      "\n",
      "[FEMALE] Fold 4/5\n",
      "[FEMALE] best_iter=123 | Acc=0.8968 | MacroF1=0.7266\n",
      "\n",
      "[FEMALE] Fold 5/5\n",
      "[FEMALE] best_iter=65 | Acc=0.8981 | MacroF1=0.7270\n",
      "\n",
      "[FEMALE] OOF Accuracy: 0.8997 | Macro F1: 0.7307\n",
      "[FEMALE] Best iterations per fold: [307, 122, 186, 123, 65] | Median: 123\n",
      "\n",
      "========== OVERALL OOF ==========\n",
      "OOF Accuracy: 0.8841 | Macro F1: 0.8733\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.90      0.93      0.91      1870\n",
      "      Normal_Weight       0.86      0.84      0.85      2345\n",
      "     Obesity_Type_I       0.88      0.87      0.87      2207\n",
      "    Obesity_Type_II       0.96      0.95      0.96      2403\n",
      "   Obesity_Type_III       0.99      1.00      0.99      2983\n",
      " Overweight_Level_I       0.76      0.75      0.75      1844\n",
      "Overweight_Level_II       0.76      0.79      0.77      1881\n",
      "\n",
      "           accuracy                           0.88     15533\n",
      "          macro avg       0.87      0.87      0.87     15533\n",
      "       weighted avg       0.88      0.88      0.88     15533\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You should provide test set for use best model. use_best_model parameter has been switched to false value.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Kaggle] Trained MALE (full): used_iters≈600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You should provide test set for use best model. use_best_model parameter has been switched to false value.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Kaggle] Trained FEMALE (full): used_iters≈600\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Gender-specific CatBoost + BMI + Age rounding\n",
    "# Train on train.csv → Tune per fold → Predict Kaggle_test.csv\n",
    "# Prints OOF metrics, Kaggle accuracy, confusion matrix, and report\n",
    "# ==============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# -------- Paths --------\n",
    "TRAIN_PATH = \"train.csv\"           # your training file\n",
    "KAGGLE_TEST_PATH = \"Kaggle_test.csv\"   # contains WeightCategory ground truth\n",
    "\n",
    "# -------- Config --------\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "USE_GPU = True        # set False if GPU not available\n",
    "ITERATIONS = 20000\n",
    "EARLY_STOP = 200\n",
    "LEARNING_RATE = 0.03\n",
    "DEPTH = 8\n",
    "L2_REG = 3.0\n",
    "\n",
    "# -------- Helpers --------\n",
    "def norm_col(s):\n",
    "    if s is None: return s\n",
    "    return str(s).replace(\"\\ufeff\", \"\").strip().lower()\n",
    "\n",
    "def detect_gender_column(df):\n",
    "    # prefer common names\n",
    "    for c in df.columns:\n",
    "        if norm_col(c) in {\"gender\", \"sex\"}:\n",
    "            return c\n",
    "    # fallback: values that look like M/F\n",
    "    for c in df.columns:\n",
    "        vals = pd.Series(df[c].dropna().astype(str).str.lower().str.strip()).unique()\n",
    "        if len(vals) in (2, 3):\n",
    "            if any(v.startswith(\"m\") for v in vals) and any(v.startswith(\"f\") for v in vals):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def split_by_gender(series):\n",
    "    s = series.astype(str).str.lower().str.strip()\n",
    "    male_mask = s.str.startswith((\"m\",\"1\",\"true\"))\n",
    "    female_mask = s.str.startswith((\"f\",\"0\",\"false\"))\n",
    "    if male_mask.sum()==0 and female_mask.sum()==0:\n",
    "        top = s.value_counts().index.tolist()\n",
    "        if len(top)>=2:\n",
    "            male_mask = s==top[0]\n",
    "            female_mask = s==top[1]\n",
    "    return male_mask, female_mask\n",
    "\n",
    "def add_bmi(df):\n",
    "    \"\"\"BMI = Weight / Height(m)^2 ; auto-handle cm heights.\"\"\"\n",
    "    if (\"Weight\" in df.columns) and (\"Height\" in df.columns):\n",
    "        h = pd.to_numeric(df[\"Height\"], errors=\"coerce\")\n",
    "        height_m = np.where(np.nanmedian(h) > 3.0, h/100.0, h)\n",
    "        w = pd.to_numeric(df[\"Weight\"], errors=\"coerce\")\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            bmi = w / (np.power(height_m, 2) + 1e-12)\n",
    "        df[\"BMI\"] = pd.Series(bmi).replace([np.inf, -np.inf], np.nan)\n",
    "    return df\n",
    "\n",
    "def round_age_inplace(df):\n",
    "    if \"Age\" in df.columns:\n",
    "        df[\"Age\"] = pd.to_numeric(df[\"Age\"], errors=\"coerce\").round().astype(\"Int64\")\n",
    "\n",
    "def cat_cols_of(df):\n",
    "    return df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "\n",
    "def pool_from_df(X, y=None, cat_cols=None):\n",
    "    cat_idx = [X.columns.get_loc(c) for c in (cat_cols or []) if c in X.columns]\n",
    "    return Pool(X, label=y, cat_features=cat_idx)\n",
    "\n",
    "# -------- Load & prepare --------\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "kdf   = pd.read_csv(KAGGLE_TEST_PATH)\n",
    "\n",
    "if \"WeightCategory\" not in kdf.columns:\n",
    "    raise KeyError(\"Kaggle_test.csv must contain 'WeightCategory' as ground truth.\")\n",
    "\n",
    "# Minimal cleaning to mirror past runs\n",
    "for c in [\"MTRANS\",\"SCC\"]:\n",
    "    if c in train.columns: train.drop(columns=[c], inplace=True)\n",
    "    if c in kdf.columns:   kdf.drop(columns=[c], inplace=True)\n",
    "\n",
    "# Add BMI + round Age\n",
    "train = add_bmi(train)\n",
    "kdf   = add_bmi(kdf)\n",
    "round_age_inplace(train)\n",
    "round_age_inplace(kdf)\n",
    "\n",
    "# --- Target / features\n",
    "target_col = None\n",
    "for cand in [\"WeightCategory\", \"NObeyesdad\", \"label\", \"target\", \"class\", \"y\"]:\n",
    "    if cand in train.columns:\n",
    "        target_col = cand\n",
    "        break\n",
    "if target_col is None:\n",
    "    raise ValueError(\"Could not detect target column in train.csv.\")\n",
    "\n",
    "y = train[target_col].copy()\n",
    "X = train.drop(columns=[target_col]).copy()\n",
    "\n",
    "# label encode target\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "classes = list(le.classes_)\n",
    "print(f\"[Info] Classes: {classes}\")\n",
    "\n",
    "# detect gender\n",
    "gender_col = detect_gender_column(X)\n",
    "if gender_col is None:\n",
    "    raise ValueError(\"Could not detect a gender column (e.g. Gender/SEX).\")\n",
    "\n",
    "male_mask, female_mask = split_by_gender(train[gender_col])\n",
    "print(f\"[Info] Train male={int(male_mask.sum())} | female={int(female_mask.sum())}\")\n",
    "\n",
    "# convenience splits\n",
    "X_male   = X.loc[male_mask].reset_index(drop=True)\n",
    "y_male   = y_enc[male_mask]\n",
    "X_female = X.loc[female_mask].reset_index(drop=True)\n",
    "y_female = y_enc[female_mask]\n",
    "\n",
    "# -------- per-gender CV training --------\n",
    "def train_cv_catboost(Xg, yg, group_name):\n",
    "    # drop gender column inside a group to avoid leakage\n",
    "    cols = [c for c in Xg.columns if c != gender_col]\n",
    "    Xg = Xg[cols].copy()\n",
    "    cat_cols = cat_cols_of(Xg)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    oof_proba = np.zeros((len(Xg), len(classes)), dtype=np.float32)\n",
    "    best_iters = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(Xg, yg), start=1):\n",
    "        print(f\"\\n[{group_name}] Fold {fold}/{N_FOLDS}\")\n",
    "\n",
    "        X_tr, X_va = Xg.iloc[tr_idx], Xg.iloc[va_idx]\n",
    "        y_tr, y_va = yg[tr_idx], yg[va_idx]\n",
    "\n",
    "        train_pool = pool_from_df(X_tr, y_tr, cat_cols)\n",
    "        valid_pool = pool_from_df(X_va, y_va, cat_cols)\n",
    "\n",
    "        model = CatBoostClassifier(\n",
    "            loss_function=\"MultiClass\",\n",
    "            eval_metric=\"MultiClass\",           # stable early stopping\n",
    "            auto_class_weights=\"Balanced\",      # handle imbalance\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            depth=DEPTH,\n",
    "            l2_leaf_reg=L2_REG,\n",
    "            iterations=ITERATIONS,\n",
    "            random_seed=RANDOM_STATE,\n",
    "            task_type=\"GPU\" if USE_GPU else \"CPU\",\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            train_pool,\n",
    "            eval_set=valid_pool,\n",
    "            use_best_model=True,\n",
    "            early_stopping_rounds=EARLY_STOP,\n",
    "            verbose=False\n",
    "        )\n",
    "        best_it = getattr(model, \"best_iteration_\", None)\n",
    "        if best_it is None:\n",
    "            best_it = ITERATIONS\n",
    "        best_iters.append(int(best_it))\n",
    "\n",
    "        # OOF proba\n",
    "        oof_proba[va_idx] = model.predict_proba(valid_pool)\n",
    "\n",
    "        # fold metrics\n",
    "        pred = np.argmax(oof_proba[va_idx], axis=1)\n",
    "        acc = accuracy_score(y_va, pred)\n",
    "        f1m = f1_score(y_va, pred, average=\"macro\")\n",
    "        print(f\"[{group_name}] best_iter={best_it} | Acc={acc:.4f} | MacroF1={f1m:.4f}\")\n",
    "\n",
    "    # group summary\n",
    "    labels = np.argmax(oof_proba, axis=1)\n",
    "    acc_g = accuracy_score(yg, labels)\n",
    "    f1_g  = f1_score(yg, labels, average=\"macro\")\n",
    "    print(f\"\\n[{group_name}] OOF Accuracy: {acc_g:.4f} | Macro F1: {f1_g:.4f}\")\n",
    "    print(f\"[{group_name}] Best iterations per fold: {best_iters} | Median: {int(np.median(best_iters))}\")\n",
    "\n",
    "    return oof_proba, cols, cat_cols, int(np.median(best_iters))\n",
    "\n",
    "male_oof, male_cols, male_cat_cols, male_best_it = train_cv_catboost(X_male, y_male, \"MALE\")\n",
    "female_oof, female_cols, female_cat_cols, female_best_it = train_cv_catboost(X_female, y_female, \"FEMALE\")\n",
    "\n",
    "# overall OOF\n",
    "oof_all = np.zeros((len(X), len(classes)), dtype=np.float32)\n",
    "oof_all[male_mask.values] = male_oof\n",
    "oof_all[female_mask.values] = female_oof\n",
    "oof_labels = np.argmax(oof_all, axis=1)\n",
    "print(\"\\n========== OVERALL OOF ==========\")\n",
    "print(\"OOF Accuracy:\", f\"{accuracy_score(y_enc, oof_labels):.4f}\",\n",
    "      \"| Macro F1:\", f\"{f1_score(y_enc, oof_labels, average='macro'):.4f}\")\n",
    "print(classification_report(y_enc, oof_labels, target_names=classes, zero_division=0))\n",
    "\n",
    "# -------- Train on FULL gender data and evaluate on Kaggle_test --------\n",
    "# Prepare Kaggle set the same way\n",
    "y_true = kdf[\"WeightCategory\"].copy()\n",
    "X_k = kdf.drop(columns=[\"WeightCategory\"], errors=\"ignore\").copy()\n",
    "\n",
    "for c in [\"MTRANS\",\"SCC\"]:\n",
    "    if c in X_k.columns: X_k.drop(columns=[c], inplace=True)\n",
    "X_k = add_bmi(X_k)\n",
    "round_age_inplace(X_k)\n",
    "\n",
    "# split kaggle by gender\n",
    "gcol_k = detect_gender_column(X_k)\n",
    "if gcol_k is None:\n",
    "    raise ValueError(\"Could not detect a gender column in Kaggle_test.csv\")\n",
    "km_k, kf_k = split_by_gender(X_k[gcol_k])\n",
    "\n",
    "def train_full_and_predict_catboost(X_full, y_full, X_eval, cols_keep, cat_cols, name, best_iters):\n",
    "    if len(X_eval) == 0 or len(X_full) == 0:\n",
    "        return np.zeros((len(X_eval), len(classes)), dtype=np.float32)\n",
    "\n",
    "    Xf = X_full[cols_keep].copy()\n",
    "    Xe = X_eval[cols_keep].copy()\n",
    "\n",
    "    train_pool = pool_from_df(Xf, y_full, cat_cols)\n",
    "    test_pool  = pool_from_df(Xe, None, cat_cols)\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "        loss_function=\"MultiClass\",\n",
    "        eval_metric=\"MultiClass\",\n",
    "        auto_class_weights=\"Balanced\",\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        depth=DEPTH,\n",
    "        l2_leaf_reg=L2_REG,\n",
    "        iterations=max(best_iters, EARLY_STOP*3),  # safe cap if best_iters small\n",
    "        random_seed=RANDOM_STATE,\n",
    "        task_type=\"GPU\" if USE_GPU else \"CPU\",\n",
    "        verbose=False\n",
    "    )\n",
    "    # small internal split for best_model; keeps behavior close to CV\n",
    "    model.fit(train_pool, use_best_model=True, verbose=False)\n",
    "\n",
    "    print(f\"[Kaggle] Trained {name}: used_iters≈{getattr(model,'best_iteration_', None) or model.get_params().get('iterations')}\")\n",
    "    return model.predict_proba(test_pool)\n",
    "\n",
    "# set up full gender frames (drop gender col within group)\n",
    "X_m_full = X.loc[male_mask, male_cols].reset_index(drop=True)\n",
    "y_m_full = y_enc[male_mask]\n",
    "X_f_full = X.loc[female_mask, female_cols].reset_index(drop=True)\n",
    "y_f_full = y_enc[female_mask]\n",
    "\n",
    "Xk_male   = X_k.loc[km_k, X_m_full.columns].reset_index(drop=True)\n",
    "Xk_female = X_k.loc[kf_k, X_f_full.columns].reset_index(drop=True)\n",
    "\n",
    "k_pred_proba = np.zeros((len(X_k), len(classes)), dtype=np.float32)\n",
    "if len(Xk_male) > 0:\n",
    "    k_pred_proba[km_k.values] = train_full_and_predict_catboost(\n",
    "        X_m_full, y_m_full, Xk_male, X_m_full.columns, cat_cols_of(X_m_full),\n",
    "        \"MALE (full)\", male_best_it\n",
    "    )\n",
    "if len(Xk_female) > 0:\n",
    "    k_pred_proba[kf_k.values] = train_full_and_predict_catboost(\n",
    "        X_f_full, y_f_full, Xk_female, X_f_full.columns, cat_cols_of(X_f_full),\n",
    "        \"FEMALE (full)\", female_best_it\n",
    "    )\n",
    "\n",
    "# predictions on Kaggle_test\n",
    "k_idx = np.argmax(k_pred_proba, axis=1)\n",
    "y_pred = le.inverse_transform(k_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29095e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Overall Accuracy on Kaggle_test: 0.90182\n",
      "\n",
      "=== Confusion Matrix (counts) ===\n",
      "Predicted →\n",
      "True ↓\n",
      "Insufficient_Weight   :  623 |   28 |    0 |    0 |    0 |    2 |    0\n",
      "Normal_Weight         :   47 |  643 |    1 |    0 |    0 |   38 |    8\n",
      "Obesity_Type_I        :    1 |    1 |  617 |   21 |    3 |   13 |   47\n",
      "Obesity_Type_II       :    0 |    0 |   27 |  810 |    0 |    2 |    6\n",
      "Obesity_Type_III      :    0 |    0 |    1 |    0 | 1061 |    1 |    0\n",
      "Overweight_Level_I    :    3 |   58 |    8 |    0 |    1 |  448 |   65\n",
      "Overweight_Level_II   :    0 |   13 |   46 |    4 |    0 |   68 |  510\n",
      "\n",
      "=== Confusion Matrix (row-normalized) ===\n",
      "Insufficient_Weight   : 0.95 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00\n",
      "Normal_Weight         : 0.06 | 0.87 | 0.00 | 0.00 | 0.00 | 0.05 | 0.01\n",
      "Obesity_Type_I        : 0.00 | 0.00 | 0.88 | 0.03 | 0.00 | 0.02 | 0.07\n",
      "Obesity_Type_II       : 0.00 | 0.00 | 0.03 | 0.96 | 0.00 | 0.00 | 0.01\n",
      "Obesity_Type_III      : 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 0.00 | 0.00\n",
      "Overweight_Level_I    : 0.01 | 0.10 | 0.01 | 0.00 | 0.00 | 0.77 | 0.11\n",
      "Overweight_Level_II   : 0.00 | 0.02 | 0.07 | 0.01 | 0.00 | 0.11 | 0.80\n",
      "\n",
      "=== Per-class metrics (Kaggle_test) ===\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight     0.9243    0.9541    0.9390       653\n",
      "      Normal_Weight     0.8654    0.8725    0.8689       737\n",
      "     Obesity_Type_I     0.8814    0.8777    0.8795       703\n",
      "    Obesity_Type_II     0.9701    0.9586    0.9643       845\n",
      "   Obesity_Type_III     0.9962    0.9981    0.9972      1063\n",
      " Overweight_Level_I     0.7832    0.7684    0.7758       583\n",
      "Overweight_Level_II     0.8019    0.7956    0.7987       641\n",
      "\n",
      "           accuracy                         0.9018      5225\n",
      "          macro avg     0.8889    0.8893    0.8891      5225\n",
      "       weighted avg     0.9015    0.9018    0.9016      5225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------- Accuracy & confusion matrix on Kaggle_test --------\n",
    "overall_acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\n✅ Overall Accuracy on Kaggle_test: {overall_acc:.5f}\")\n",
    "\n",
    "order = classes  # keep the learned class order\n",
    "cm = confusion_matrix(y_true, y_pred, labels=order)\n",
    "cm_norm = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (counts) ===\")\n",
    "print(\"Predicted →\")\n",
    "print(\"True ↓\")\n",
    "for i, true_class in enumerate(order):\n",
    "    row = \" | \".join(f\"{cm[i, j]:4d}\" for j in range(len(order)))\n",
    "    print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (row-normalized) ===\")\n",
    "for i, true_class in enumerate(order):\n",
    "    row = \" | \".join(f\"{cm_norm[i, j]:.2f}\" for j in range(len(order)))\n",
    "    print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "print(\"\\n=== Per-class metrics (Kaggle_test) ===\")\n",
    "print(classification_report(y_true, y_pred, labels=order, target_names=order, digits=4, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517cbef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
