{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4baac0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 17:16:42,501] A new study created in memory with name: no-name-832cad8e-c6a7-4acc-8395-19d88e6dee7b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Detected] Target in train: 'WeightCategory', Label in sample_sub: 'WeightCategory'\n",
      "[Detected] ID in train: 'id', ID in test: 'id'\n",
      "[Info] Train counts - M<24:4551  M>=24:3232  F<24:4755  F>=24:2995\n",
      "[Info] Test  counts - M<24:595  M>=24:471  F<24:643  F>=24:400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:17:28,492] Trial 0 finished with value: 0.7929147888052066 and parameters: {'eta': 0.01956360392823701, 'max_depth': 12, 'min_child_weight': 7.587945476302646, 'subsample': 0.8394633936788146, 'colsample_bytree': 0.6624074561769746, 'lambda': 0.004207053950287938, 'alpha': 0.0001707396743152812, 'gamma': 4.330880728874676}. Best is trial 0 with value: 0.7929147888052066.\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:17:50,469] Trial 1 finished with value: 0.7963879322196303 and parameters: {'eta': 0.0293601586511158, 'max_depth': 10, 'min_child_weight': 1.185260448662222, 'subsample': 0.9879639408647978, 'colsample_bytree': 0.9329770563201687, 'lambda': 0.0070689749506246055, 'alpha': 0.000533703276260396, 'gamma': 0.9170225492671691}. Best is trial 1 with value: 0.7963879322196303.\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:18:22,098] Trial 2 finished with value: 0.7999559190589283 and parameters: {'eta': 0.017248307328108965, 'max_depth': 8, 'min_child_weight': 4.887505167779041, 'subsample': 0.7164916560792167, 'colsample_bytree': 0.8447411578889518, 'lambda': 0.003613894271216527, 'alpha': 0.0014742753159914669, 'gamma': 1.8318092164684585}. Best is trial 2 with value: 0.7999559190589283.\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:18:37,393] Trial 3 finished with value: 0.7982352099755692 and parameters: {'eta': 0.02264078228235544, 'max_depth': 11, 'min_child_weight': 2.7970640394252375, 'subsample': 0.8056937753654446, 'colsample_bytree': 0.836965827544817, 'lambda': 0.0015339162591163618, 'alpha': 0.02692646910086179, 'gamma': 0.8526206184364576}. Best is trial 2 with value: 0.7999559190589283.\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:19:35,837] Trial 4 finished with value: 0.7949183428915736 and parameters: {'eta': 0.011236213390203352, 'max_depth': 12, 'min_child_weight': 9.690688297671034, 'subsample': 0.9233589392465844, 'colsample_bytree': 0.7218455076693483, 'lambda': 0.002458603276328005, 'alpha': 0.054567254856014755, 'gamma': 2.2007624686980067}. Best is trial 2 with value: 0.7999559190589283.\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:19:51,488] Trial 5 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:20:03,005] Trial 6 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:20:45,289] Trial 7 finished with value: 0.7975083082106023 and parameters: {'eta': 0.011718190423484345, 'max_depth': 5, 'min_child_weight': 1.4070456001948426, 'subsample': 0.7301321323053057, 'colsample_bytree': 0.7554709158757928, 'lambda': 0.01217295809836997, 'alpha': 0.20651425578959234, 'gamma': 1.7837666334679465}. Best is trial 2 with value: 0.7999559190589283.\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:20:49,789] Trial 8 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:21:15,372] Trial 9 finished with value: 0.7987517245438276 and parameters: {'eta': 0.010099434161262417, 'max_depth': 11, 'min_child_weight': 7.361716094628554, 'subsample': 0.8916028672163949, 'colsample_bytree': 0.9085081386743783, 'lambda': 0.0019777828512462727, 'alpha': 0.00271558195528294, 'gamma': 0.5793452976256486}. Best is trial 2 with value: 0.7999559190589283.\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:21:20,085] Trial 10 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:21:40,507] Trial 11 finished with value: 0.7976031281685063 and parameters: {'eta': 0.015505416909357242, 'max_depth': 9, 'min_child_weight': 6.8558302870942285, 'subsample': 0.7078349435778688, 'colsample_bytree': 0.886588474674238, 'lambda': 0.039428477355546665, 'alpha': 0.00343461663274141, 'gamma': 0.07833752085340617}. Best is trial 2 with value: 0.7999559190589283.\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:21:44,167] Trial 12 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:22:11,908] Trial 13 finished with value: 0.7993778805433445 and parameters: {'eta': 0.010316056685319406, 'max_depth': 10, 'min_child_weight': 7.362707978655825, 'subsample': 0.6526916860188785, 'colsample_bytree': 0.9914121887770276, 'lambda': 0.4399698345455773, 'alpha': 0.0012288952452522888, 'gamma': 0.017856078074685144}. Best is trial 2 with value: 0.7999559190589283.\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:22:18,580] Trial 14 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:22:24,188] Trial 15 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:22:33,972] Trial 16 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:22:36,363] Trial 17 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:22:44,751] Trial 18 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:22:54,001] Trial 19 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:23:15,610] Trial 20 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:23:45,502] Trial 21 finished with value: 0.7993513786470314 and parameters: {'eta': 0.010286161191358369, 'max_depth': 11, 'min_child_weight': 7.356729132728534, 'subsample': 0.853891397534113, 'colsample_bytree': 0.9147061239765318, 'lambda': 0.0041666192478791286, 'alpha': 0.002453121505064421, 'gamma': 0.48731739538165053}. Best is trial 2 with value: 0.7999559190589283.\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:23:49,464] Trial 22 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:23:51,064] Trial 23 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:23:54,340] Trial 24 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:23:59,753] Trial 25 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:24:02,815] Trial 26 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:24:09,034] Trial 27 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:24:31,492] Trial 28 finished with value: 0.7980715893705617 and parameters: {'eta': 0.014403106316139204, 'max_depth': 8, 'min_child_weight': 6.311324683469321, 'subsample': 0.7851722322158547, 'colsample_bytree': 0.8671492901511151, 'lambda': 0.053145601598583746, 'alpha': 0.002011669397542744, 'gamma': 0.6697881883192913}. Best is trial 2 with value: 0.7999559190589283.\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:24:43,349] Trial 29 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:24:59,003] Trial 30 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:25:04,933] Trial 31 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:25:16,448] Trial 32 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:25:27,210] Trial 33 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:25:36,139] Trial 34 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:25:53,112] Trial 35 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:26:13,046] Trial 36 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:26:15,620] Trial 37 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:26:22,948] Trial 38 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:26:29,924] Trial 39 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Optuna] Best macro-F1: 0.8000\n",
      "[Optuna] Best params: {'eta': 0.017248307328108965, 'max_depth': 8, 'min_child_weight': 4.887505167779041, 'subsample': 0.7164916560792167, 'colsample_bytree': 0.8447411578889518, 'lambda': 0.003613894271216527, 'alpha': 0.0014742753159914669, 'gamma': 1.8318092164684585}\n",
      "\n",
      "[MALE_<24] Fold 1/5\n",
      "[MALE_<24] Best iteration: 2500\n",
      "\n",
      "[MALE_<24] Fold 2/5\n",
      "[MALE_<24] Best iteration: 3439\n",
      "\n",
      "[MALE_<24] Fold 3/5\n",
      "[MALE_<24] Best iteration: 2134\n",
      "\n",
      "[MALE_<24] Fold 4/5\n",
      "[MALE_<24] Best iteration: 4422\n",
      "\n",
      "[MALE_<24] Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 17:27:01,958] A new study created in memory with name: no-name-072b55d3-37f0-414a-9e46-2a9751a1661f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE_<24] Best iteration: 1611\n",
      "\n",
      "[MALE_<24] OOF Accuracy: 0.8699 | Macro F1: 0.7504\n",
      "[MALE_<24] Best iterations: [2500, 3439, 2134, 4422, 1611] | Median: 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:27:28,558] Trial 0 finished with value: 0.6343277042005868 and parameters: {'eta': 0.01956360392823701, 'max_depth': 12, 'min_child_weight': 7.587945476302646, 'subsample': 0.8394633936788146, 'colsample_bytree': 0.6624074561769746, 'lambda': 0.004207053950287938, 'alpha': 0.0001707396743152812, 'gamma': 4.330880728874676}. Best is trial 0 with value: 0.6343277042005868.\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:27:39,652] Trial 1 finished with value: 0.6355770343994949 and parameters: {'eta': 0.0293601586511158, 'max_depth': 10, 'min_child_weight': 1.185260448662222, 'subsample': 0.9879639408647978, 'colsample_bytree': 0.9329770563201687, 'lambda': 0.0070689749506246055, 'alpha': 0.000533703276260396, 'gamma': 0.9170225492671691}. Best is trial 1 with value: 0.6355770343994949.\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:27:51,058] Trial 2 finished with value: 0.6484154221908486 and parameters: {'eta': 0.017248307328108965, 'max_depth': 8, 'min_child_weight': 4.887505167779041, 'subsample': 0.7164916560792167, 'colsample_bytree': 0.8447411578889518, 'lambda': 0.003613894271216527, 'alpha': 0.0014742753159914669, 'gamma': 1.8318092164684585}. Best is trial 2 with value: 0.6484154221908486.\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:27:59,772] Trial 3 finished with value: 0.6457989953189806 and parameters: {'eta': 0.02264078228235544, 'max_depth': 11, 'min_child_weight': 2.7970640394252375, 'subsample': 0.8056937753654446, 'colsample_bytree': 0.836965827544817, 'lambda': 0.0015339162591163618, 'alpha': 0.02692646910086179, 'gamma': 0.8526206184364576}. Best is trial 2 with value: 0.6484154221908486.\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:28:35,228] Trial 4 finished with value: 0.6394323811964345 and parameters: {'eta': 0.011236213390203352, 'max_depth': 12, 'min_child_weight': 9.690688297671034, 'subsample': 0.9233589392465844, 'colsample_bytree': 0.7218455076693483, 'lambda': 0.002458603276328005, 'alpha': 0.054567254856014755, 'gamma': 2.2007624686980067}. Best is trial 2 with value: 0.6484154221908486.\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:29:02,396] Trial 5 finished with value: 0.640828242118504 and parameters: {'eta': 0.012444120419299869, 'max_depth': 8, 'min_child_weight': 1.3094966900369656, 'subsample': 0.9637281608315128, 'colsample_bytree': 0.7035119926400067, 'lambda': 0.4467752817973907, 'alpha': 0.0017654048052495078, 'gamma': 2.600340105889054}. Best is trial 2 with value: 0.6484154221908486.\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[I 2025-10-23 17:29:07,749] Trial 6 pruned. \n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "[W 2025-10-23 17:29:14,198] Trial 7 failed with parameters: {'eta': 0.011718190423484345, 'max_depth': 5, 'min_child_weight': 1.4070456001948426, 'subsample': 0.7301321323053057, 'colsample_bytree': 0.7554709158757928, 'lambda': 0.01217295809836997, 'alpha': 0.20651425578959234, 'gamma': 1.7837666334679465} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\surya\\AppData\\Local\\Temp\\ipykernel_15172\\2240948060.py\", line 226, in objective\n",
      "    bst = xgb.train(\n",
      "  File \"C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 2050, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-10-23 17:29:14,203] Trial 7 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 465\u001b[0m\n\u001b[0;32m    462\u001b[0m y_grp \u001b[38;5;241m=\u001b[39m y_enc[tr_mask\u001b[38;5;241m.\u001b[39mvalues]\n\u001b[0;32m    463\u001b[0m test_grp \u001b[38;5;241m=\u001b[39m test_features[te_mask]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 465\u001b[0m grp_oof, grp_test_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_group_and_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_grp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_grp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_grp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrp_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBASE_XGB_PARAMS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUSE_OPTUNA_TUNING\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m oof_full[tr_mask\u001b[38;5;241m.\u001b[39mvalues] \u001b[38;5;241m=\u001b[39m grp_oof\n\u001b[0;32m    472\u001b[0m test_pred_proba[te_mask\u001b[38;5;241m.\u001b[39mvalues] \u001b[38;5;241m=\u001b[39m grp_test_pred\n",
      "Cell \u001b[1;32mIn[3], line 364\u001b[0m, in \u001b[0;36mtrain_group_and_predict\u001b[1;34m(X_grp, y_enc_grp, test_grp, group_name, base_params, tune)\u001b[0m\n\u001b[0;32m    362\u001b[0m tuned \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tune \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(Xg) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_enc_grp)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 364\u001b[0m     tuned \u001b[38;5;241m=\u001b[39m \u001b[43mtune_xgb_hyperparams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_enc_grp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOPTUNA_TRIALS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_FOLDS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRANDOM_STATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_JOBS\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# final params\u001b[39;00m\n\u001b[0;32m    373\u001b[0m xgb_params \u001b[38;5;241m=\u001b[39m base_params\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[1;32mIn[3], line 247\u001b[0m, in \u001b[0;36mtune_xgb_hyperparams\u001b[1;34m(X_df, y_enc, classes, n_trials, n_folds, random_state, n_jobs)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(f1s))\n\u001b[0;32m    246\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m, sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39mrandom_state))\n\u001b[1;32m--> 247\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Optuna] Best macro-F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Optuna] Best params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\optuna\\study\\study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\optuna\\study\\_optimize.py:258\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    254\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    257\u001b[0m ):\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[3], line 226\u001b[0m, in \u001b[0;36mtune_xgb_hyperparams.<locals>.objective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    223\u001b[0m dtrain \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(Xtr, label\u001b[38;5;241m=\u001b[39my_tr)\n\u001b[0;32m    224\u001b[0m dval   \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(Xva, label\u001b[38;5;241m=\u001b[39my_va)\n\u001b[1;32m--> 226\u001b[0m bst \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_BOOST_ROUND\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEARLY_STOP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    233\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m best_round \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(bst\u001b[38;5;241m.\u001b[39mbest_iteration \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    236\u001b[0m proba \u001b[38;5;241m=\u001b[39m bst\u001b[38;5;241m.\u001b[39mpredict(dval, iteration_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, best_round))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:2050\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2046\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2049\u001b[0m     _check_call(\n\u001b[1;32m-> 2050\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2051\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2053\u001b[0m     )\n\u001b[0;32m   2054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2055\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# End-to-end (4 subgroup models: Gender × AgeBin) + BMI feature + Optuna tuning\n",
    "# Load → Detect ID/Target/Gender → Drop MTRANS/SMOKE → +BMI →\n",
    "# Round Age → AgeBin (0:<24, 1:>=24) →\n",
    "# Split to 4 groups → Per-group 5-Fold XGB (ES) [+ optional Optuna] →\n",
    "# Predict → submission.csv\n",
    "# ==============================================\n",
    "\n",
    "# -------- Imports --------\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.base import clone\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna  # <- for hyperparameter tuning\n",
    "\n",
    "# -------- Paths --------\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "SAMPLE_SUB_PATH = \"sample_submission.csv\"\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "N_JOBS = -1\n",
    "NUM_CLASSES_EXPECTED = 7   # sanity warning only\n",
    "\n",
    "# -------- Tuning toggles --------\n",
    "USE_OPTUNA_TUNING = True     # set False to skip tuning and use base params\n",
    "OPTUNA_TRIALS = 40           # per subgroup; increase for more thorough search\n",
    "\n",
    "# -------- Helpers --------\n",
    "def norm_col(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    return str(s).replace(\"\\ufeff\", \"\").strip().lower()\n",
    "\n",
    "def build_norm_map(cols):\n",
    "    fwd = {c: norm_col(c) for c in cols}\n",
    "    rev = {}\n",
    "    for orig, n in fwd.items():\n",
    "        if n not in rev:\n",
    "            rev[n] = orig\n",
    "    return fwd, rev\n",
    "\n",
    "def find_id_and_label(sample_sub, train, test):\n",
    "    ss_fwd, _ = build_norm_map(sample_sub.columns)\n",
    "    tr_fwd, _ = build_norm_map(train.columns)\n",
    "    te_fwd, _ = build_norm_map(test.columns)\n",
    "\n",
    "    ss_norm_cols = [ss_fwd[c] for c in sample_sub.columns]\n",
    "    tr_norm_cols = [tr_fwd[c] for c in train.columns]\n",
    "    te_norm_cols = [te_fwd[c] for c in test.columns]\n",
    "\n",
    "    id_norm, label_norm = None, None\n",
    "    if len(ss_norm_cols) == 2:\n",
    "        c1, c2 = ss_norm_cols\n",
    "        if c1 in te_norm_cols and c2 not in te_norm_cols:\n",
    "            id_norm, label_norm = c1, c2\n",
    "        elif c2 in te_norm_cols and c1 not in te_norm_cols:\n",
    "            id_norm, label_norm = c2, c1\n",
    "        else:\n",
    "            if c1 in te_norm_cols and c1 in tr_norm_cols:\n",
    "                id_norm, label_norm = c1, c2\n",
    "            elif c2 in te_norm_cols and c2 in tr_norm_cols:\n",
    "                id_norm, label_norm = c2, c1\n",
    "\n",
    "    if id_norm is None:\n",
    "        for cand in [\"id\", \"row_id\", \"index\", \"sample_id\"]:\n",
    "            if cand in te_norm_cols and cand in tr_norm_cols:\n",
    "                id_norm = cand\n",
    "                break\n",
    "\n",
    "    if label_norm is None:\n",
    "        candidates = [c for c in ss_norm_cols if c != id_norm]\n",
    "        if len(candidates) == 1:\n",
    "            label_norm = candidates[0]\n",
    "\n",
    "    if label_norm is None:\n",
    "        for cand in [\"label\", \"target\", \"class\", \"y\", \"weightcategory\", \"nobeyesdad\"]:\n",
    "            if cand in tr_norm_cols and cand != id_norm:\n",
    "                label_norm = cand\n",
    "                break\n",
    "\n",
    "    if label_norm is None:\n",
    "        for c in reversed(tr_norm_cols):\n",
    "            if c != id_norm:\n",
    "                label_norm = c\n",
    "                break\n",
    "\n",
    "    return {\n",
    "        \"id_norm\": id_norm,\n",
    "        \"label_norm\": label_norm,\n",
    "        \"id_in_train\": build_norm_map(train.columns)[1].get(id_norm, None),\n",
    "        \"id_in_test\": build_norm_map(test.columns)[1].get(id_norm, None),\n",
    "        \"id_in_sample\": build_norm_map(sample_sub.columns)[1].get(id_norm, None),\n",
    "        \"label_in_train\": build_norm_map(train.columns)[1].get(label_norm, None),\n",
    "        \"label_in_sample\": build_norm_map(sample_sub.columns)[1].get(label_norm, None),\n",
    "    }\n",
    "\n",
    "def infer_feature_types(df):\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def detect_gender_column(df):\n",
    "    candidates = [c for c in df.columns if norm_col(c) in {\"gender\",\"sex\"}]\n",
    "    if candidates:\n",
    "        return candidates[0]\n",
    "    for c in df.columns:\n",
    "        vals = pd.Series(df[c].dropna().astype(str).str.lower().str.strip()).unique()\n",
    "        if len(vals) in (2, 3):\n",
    "            if any(v.startswith(\"m\") for v in vals) and any(v.startswith(\"f\") for v in vals):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def split_by_gender(series):\n",
    "    s = series.astype(str).str.lower().str.strip()\n",
    "    male_mask = s.str.startswith((\"m\",\"1\",\"true\"))\n",
    "    female_mask = s.str.startswith((\"f\",\"0\",\"false\"))\n",
    "    if male_mask.sum()==0 and female_mask.sum()==0:\n",
    "        top = s.value_counts().index.tolist()\n",
    "        if len(top)>=2:\n",
    "            male_mask = s==top[0]\n",
    "            female_mask = s==top[1]\n",
    "    return male_mask, female_mask\n",
    "\n",
    "def add_bmi(df):\n",
    "    \"\"\"Compute BMI exactly as Weight / (Height ** 2).\"\"\"\n",
    "    if (\"Weight\" in df.columns) and (\"Height\" in df.columns):\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            df[\"BMI\"] = pd.to_numeric(df[\"Weight\"], errors=\"coerce\") / (\n",
    "                pd.to_numeric(df[\"Height\"], errors=\"coerce\") ** 2\n",
    "            )\n",
    "        df[\"BMI\"] = df[\"BMI\"].replace([np.inf, -np.inf], np.nan)\n",
    "    return df\n",
    "\n",
    "def round_and_bin_age(df):\n",
    "    \"\"\"Round Age, then make AgeBin = 0 if <24, 1 if >=24.\"\"\"\n",
    "    if \"Age\" in df.columns:\n",
    "        age_num = pd.to_numeric(df[\"Age\"], errors=\"coerce\").round()\n",
    "        df[\"Age\"] = age_num\n",
    "        df[\"AgeBin\"] = (age_num >= 24).astype(\"Int8\")\n",
    "    return df\n",
    "\n",
    "# --- Optuna-powered XGB tuner (CV + early stopping, macro F1) ---\n",
    "def tune_xgb_hyperparams(\n",
    "    X_df: pd.DataFrame,\n",
    "    y_enc: np.ndarray,\n",
    "    classes: list,\n",
    "    n_trials: int = 40,\n",
    "    n_folds: int = 5,\n",
    "    random_state: int = 42,\n",
    "    n_jobs: int = -1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns best_params (dict) to merge into your xgb_params.\n",
    "    Works with sparse outputs from ColumnTransformer+OHE.\n",
    "    \"\"\"\n",
    "    num_cols, cat_cols = infer_feature_types(X_df)\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", ohe)\n",
    "    ])\n",
    "    preprocessor_tpl = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=1.0\n",
    "    )\n",
    "\n",
    "    base = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": len(classes),\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"nthread\": n_jobs,\n",
    "        \"seed\": random_state,\n",
    "    }\n",
    "    NUM_BOOST_ROUND = 20000\n",
    "    EARLY_STOP = 200\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    def objective(trial: optuna.Trial):\n",
    "        params = {\n",
    "            **base,\n",
    "            \"eta\": trial.suggest_float(\"eta\", 0.01, 0.06, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12),\n",
    "            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 10.0),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"lambda\": trial.suggest_float(\"lambda\", 1e-3, 10.0, log=True),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 1.0, log=True),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "        }\n",
    "\n",
    "        f1s = []\n",
    "        for tr_idx, va_idx in skf.split(X_df, y_enc):\n",
    "            X_tr, X_va = X_df.iloc[tr_idx], X_df.iloc[va_idx]\n",
    "            y_tr, y_va = y_enc[tr_idx], y_enc[va_idx]\n",
    "\n",
    "            prep = clone(preprocessor_tpl)\n",
    "            Xtr = prep.fit_transform(X_tr)\n",
    "            Xva = prep.transform(X_va)\n",
    "\n",
    "            dtrain = xgb.DMatrix(Xtr, label=y_tr)\n",
    "            dval   = xgb.DMatrix(Xva, label=y_va)\n",
    "\n",
    "            bst = xgb.train(\n",
    "                params=params,\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=NUM_BOOST_ROUND,\n",
    "                evals=[(dtrain, \"train\"), (dval, \"valid\")],\n",
    "                early_stopping_rounds=EARLY_STOP,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "            best_round = int(bst.best_iteration + 1)\n",
    "\n",
    "            proba = bst.predict(dval, iteration_range=(0, best_round))\n",
    "            pred = np.argmax(proba, axis=1)\n",
    "            f1s.append(f1_score(y_va, pred, average=\"macro\"))\n",
    "\n",
    "            trial.report(float(np.mean(f1s)), step=len(f1s))\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        return float(np.mean(f1s))\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=random_state))\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "\n",
    "    print(f\"[Optuna] Best macro-F1: {study.best_value:.4f}\")\n",
    "    print(f\"[Optuna] Best params: {study.best_params}\")\n",
    "    return study.best_params\n",
    "\n",
    "# -------- Load data --------\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "# Drop not-used columns\n",
    "for c in [\"MTRANS\",\"SMOKE\"]:\n",
    "    if c in train.columns: train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:  test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# >>> Add BMI + Round Age + AgeBin (train & test) <<<\n",
    "train = add_bmi(train)\n",
    "test  = add_bmi(test)\n",
    "train = round_and_bin_age(train)\n",
    "test  = round_and_bin_age(test)\n",
    "\n",
    "info = find_id_and_label(sample_sub, train, test)\n",
    "\n",
    "ID_COL_TRAIN   = info[\"id_in_train\"]\n",
    "ID_COL_TEST    = info[\"id_in_test\"]\n",
    "ID_COL_SAMPLE  = info[\"id_in_sample\"]\n",
    "TARGET_COL     = info[\"label_in_train\"]\n",
    "LABEL_COL_SAMP = info[\"label_in_sample\"]\n",
    "\n",
    "if TARGET_COL is None:\n",
    "    raise ValueError(\"Could not detect the target column. Please ensure sample_submission and train headers align.\")\n",
    "if LABEL_COL_SAMP is None:\n",
    "    ss_cols = list(sample_sub.columns)\n",
    "    others = [c for c in ss_cols if c != ID_COL_SAMPLE]\n",
    "    if len(others)==1:\n",
    "        LABEL_COL_SAMP = others[0]\n",
    "    else:\n",
    "        raise ValueError(\"Could not detect label header in sample_submission.csv\")\n",
    "\n",
    "print(f\"[Detected] Target in train: '{TARGET_COL}', Label in sample_sub: '{LABEL_COL_SAMP}'\")\n",
    "if ID_COL_TRAIN and ID_COL_TEST:\n",
    "    print(f\"[Detected] ID in train: '{ID_COL_TRAIN}', ID in test: '{ID_COL_TEST}'\")\n",
    "\n",
    "# -------- Target / Features --------\n",
    "y = train[TARGET_COL].copy()\n",
    "X = train.drop(columns=[TARGET_COL]).copy()\n",
    "if ID_COL_TRAIN in X.columns:\n",
    "    X.drop(columns=[ID_COL_TRAIN], inplace=True)\n",
    "\n",
    "test_features = test.copy()\n",
    "if ID_COL_TEST in test_features.columns:\n",
    "    test_ids = test_features[ID_COL_TEST].copy()\n",
    "    test_features.drop(columns=[ID_COL_TEST], inplace=True)\n",
    "else:\n",
    "    test_ids = pd.Series(np.arange(len(test_features)), name=\"id\")\n",
    "\n",
    "# -------- Label encode target --------\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "classes = list(le.classes_)\n",
    "if len(classes) != NUM_CLASSES_EXPECTED:\n",
    "    print(f\"[Warn] Expected {NUM_CLASSES_EXPECTED} classes but found {len(classes)}. Proceeding.\")\n",
    "\n",
    "# -------- Detect gender column and split (then 4-way split with AgeBin) --------\n",
    "gender_col = detect_gender_column(pd.concat([X, test_features], axis=0))\n",
    "if gender_col is None:\n",
    "    raise ValueError(\"Could not detect a gender column (e.g., 'Gender' or 'SEX'). Please confirm the column name.\")\n",
    "if \"AgeBin\" not in X.columns:\n",
    "    raise ValueError(\"AgeBin not found. Check round_and_bin_age step.\")\n",
    "\n",
    "male_mask, female_mask = split_by_gender(train[gender_col])\n",
    "test_male_mask, test_female_mask = split_by_gender(test_features[gender_col])\n",
    "\n",
    "young_mask = (train[\"AgeBin\"] == 0)\n",
    "old_mask   = (train[\"AgeBin\"] == 1)\n",
    "test_young_mask = (test_features[\"AgeBin\"] == 0)\n",
    "test_old_mask   = (test_features[\"AgeBin\"] == 1)\n",
    "\n",
    "print(f\"[Info] Train counts - M<24:{int((male_mask & young_mask).sum())}  M>=24:{int((male_mask & old_mask).sum())}  F<24:{int((female_mask & young_mask).sum())}  F>=24:{int((female_mask & old_mask).sum())}\")\n",
    "print(f\"[Info] Test  counts - M<24:{int((test_male_mask & test_young_mask).sum())}  M>=24:{int((test_male_mask & test_old_mask).sum())}  F<24:{int((test_female_mask & test_young_mask).sum())}  F>=24:{int((test_female_mask & test_old_mask).sum())}\")\n",
    "\n",
    "# We drop split columns (gender, Age, AgeBin) inside each subgroup (constants after split)\n",
    "DROP_IN_GROUP = set([gender_col, \"Age\", \"AgeBin\"])\n",
    "\n",
    "def make_preprocessor(Xdf):\n",
    "    num_cols, cat_cols = infer_feature_types(Xdf)\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", ohe)\n",
    "    ])\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=1.0\n",
    "    )\n",
    "\n",
    "def train_group_and_predict(X_grp, y_enc_grp, test_grp, group_name, base_params, tune=False):\n",
    "    # remove split columns from features if present\n",
    "    cols_to_use = [c for c in X_grp.columns if c not in DROP_IN_GROUP]\n",
    "    Xg = X_grp[cols_to_use].copy()\n",
    "    Xtestg = test_grp[cols_to_use].copy()\n",
    "\n",
    "    # optionally tune\n",
    "    tuned = {}\n",
    "    if tune and len(Xg) > 0 and len(np.unique(y_enc_grp)) > 1:\n",
    "        tuned = tune_xgb_hyperparams(\n",
    "            Xg, y_enc_grp, classes,\n",
    "            n_trials=OPTUNA_TRIALS,\n",
    "            n_folds=N_FOLDS,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=N_JOBS\n",
    "        )\n",
    "\n",
    "    # final params\n",
    "    xgb_params = base_params.copy()\n",
    "    xgb_params.update(tuned)\n",
    "\n",
    "    preprocessor = make_preprocessor(Xg)\n",
    "\n",
    "    NUM_BOOST_ROUND = 20000\n",
    "    EARLY_STOP = 200\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    oof_group = np.zeros((len(Xg), len(classes)), dtype=np.float32)\n",
    "    test_group_pred = np.zeros((len(Xtestg), len(classes)), dtype=np.float32)\n",
    "    fold_best = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(Xg, y_enc_grp), start=1):\n",
    "        print(f\"\\n[{group_name}] Fold {fold}/{N_FOLDS}\")\n",
    "        X_tr, X_va = Xg.iloc[tr_idx], Xg.iloc[va_idx]\n",
    "        y_tr, y_va = y_enc_grp[tr_idx], y_enc_grp[va_idx]\n",
    "\n",
    "        prep = clone(preprocessor)\n",
    "        Xtr = prep.fit_transform(X_tr)\n",
    "        Xva = prep.transform(X_va)\n",
    "\n",
    "        dtrain = xgb.DMatrix(Xtr, label=y_tr)\n",
    "        dval   = xgb.DMatrix(Xva, label=y_va)\n",
    "\n",
    "        bst = xgb.train(\n",
    "            params=xgb_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=NUM_BOOST_ROUND,\n",
    "            evals=[(dtrain, \"train\"), (dval, \"valid\")],\n",
    "            early_stopping_rounds=EARLY_STOP,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        best_round = int(bst.best_iteration + 1)\n",
    "        fold_best.append(best_round)\n",
    "        print(f\"[{group_name}] Best iteration: {best_round}\")\n",
    "\n",
    "        oof_proba = bst.predict(dval, iteration_range=(0, best_round))\n",
    "        oof_group[va_idx] = oof_proba\n",
    "\n",
    "        Xtest_tf = prep.transform(Xtestg)\n",
    "        dtest = xgb.DMatrix(Xtest_tf)\n",
    "        test_group_pred += bst.predict(dtest, iteration_range=(0, best_round)) / N_FOLDS\n",
    "\n",
    "    # OOF summary for the group\n",
    "    if len(Xg) > 0:\n",
    "        oof_argmax = np.argmax(oof_group, axis=1)\n",
    "        acc_g = accuracy_score(y_enc_grp, oof_argmax)\n",
    "        f1_g = f1_score(y_enc_grp, oof_argmax, average=\"macro\")\n",
    "        print(f\"\\n[{group_name}] OOF Accuracy: {acc_g:.4f} | Macro F1: {f1_g:.4f}\")\n",
    "        if fold_best:\n",
    "            print(f\"[{group_name}] Best iterations: {fold_best} | Median: {int(np.median(fold_best))}\")\n",
    "\n",
    "    return oof_group, test_group_pred\n",
    "\n",
    "# -------- Base XGBoost params (merged with tuned params if enabled) --------\n",
    "BASE_XGB_PARAMS = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": len(classes),\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 2.0,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"lambda\": 1.0,\n",
    "    \"alpha\": 0.0,\n",
    "    \"eta\": 0.03,\n",
    "    \"nthread\": N_JOBS,\n",
    "    \"seed\": RANDOM_STATE,\n",
    "}\n",
    "\n",
    "# -------- Build 4 groups and train --------\n",
    "groups = [\n",
    "    (\"MALE_<24\",   (male_mask & (train[\"AgeBin\"] == 0)),   (test_male_mask & (test_features[\"AgeBin\"] == 0))),\n",
    "    (\"MALE_>=24\",  (male_mask & (train[\"AgeBin\"] == 1)),   (test_male_mask & (test_features[\"AgeBin\"] == 1))),\n",
    "    (\"FEMALE_<24\", (female_mask & (train[\"AgeBin\"] == 0)), (test_female_mask & (test_features[\"AgeBin\"] == 0))),\n",
    "    (\"FEMALE_>=24\",(female_mask & (train[\"AgeBin\"] == 1)), (test_female_mask & (test_features[\"AgeBin\"] == 1))),\n",
    "]\n",
    "\n",
    "oof_full = np.zeros((len(X), len(classes)), dtype=np.float32)\n",
    "test_pred_proba = np.zeros((len(test_features), len(classes)), dtype=np.float32)\n",
    "\n",
    "for grp_name, tr_mask, te_mask in groups:\n",
    "    if tr_mask.sum() == 0:\n",
    "        print(f\"[Warn] No training rows for group {grp_name}; skipping.\")\n",
    "        continue\n",
    "\n",
    "    X_grp = X[tr_mask].reset_index(drop=True)\n",
    "    y_grp = y_enc[tr_mask.values]\n",
    "    test_grp = test_features[te_mask].reset_index(drop=True)\n",
    "\n",
    "    grp_oof, grp_test_pred = train_group_and_predict(\n",
    "        X_grp, y_grp, test_grp, grp_name,\n",
    "        base_params=BASE_XGB_PARAMS,\n",
    "        tune=USE_OPTUNA_TUNING\n",
    "    )\n",
    "\n",
    "    oof_full[tr_mask.values] = grp_oof\n",
    "    test_pred_proba[te_mask.values] = grp_test_pred\n",
    "\n",
    "# -------- Overall OOF report --------\n",
    "oof_labels = np.argmax(oof_full, axis=1)\n",
    "oof_acc = accuracy_score(y_enc, oof_labels)\n",
    "oof_f1 = f1_score(y_enc, oof_labels, average=\"macro\")\n",
    "print(\"\\n========== OVERALL OOF ==========\")\n",
    "print(f\"OOF Accuracy: {oof_acc:.4f} | OOF Macro F1: {oof_f1:.4f}\")\n",
    "try:\n",
    "    print(\"\\nOOF Classification Report:\\n\",\n",
    "          classification_report(y_enc, oof_labels, target_names=classes))\n",
    "except Exception as e:\n",
    "    print(f\"[Info] Could not print classification report: {e}\")\n",
    "\n",
    "# -------- Build submission --------\n",
    "test_pred_int = np.argmax(test_pred_proba, axis=1)\n",
    "test_pred_labels = le.inverse_transform(test_pred_int)\n",
    "\n",
    "ss_cols = list(sample_sub.columns)\n",
    "ID_HEADER = ID_COL_SAMPLE if ID_COL_SAMPLE in sample_sub.columns else None\n",
    "LABEL_HEADER = LABEL_COL_SAMP\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "if ID_HEADER is not None and ID_COL_TEST in test.columns:\n",
    "    sub[ID_HEADER] = test[ID_COL_TEST].values\n",
    "elif ID_HEADER is not None:\n",
    "    sub[ID_HEADER] = np.arange(len(test_features))\n",
    "sub[LABEL_HEADER] = test_pred_labels\n",
    "\n",
    "# Reorder/complete to match sample_sub exactly\n",
    "for c in ss_cols:\n",
    "    if c not in sub.columns:\n",
    "        sub[c] = sample_sub[c].iloc[0] if len(sample_sub[c]) else None\n",
    "sub = sub[ss_cols]\n",
    "\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSaved submission.csv\")\n",
    "print(sub.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d285084e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779de9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
