{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83efc6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Classes: ['Insufficient_Weight', 'Normal_Weight', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III', 'Overweight_Level_I', 'Overweight_Level_II']\n",
      "[Info] Train male=7783, female=7750\n",
      "[Info] Test  male=10336, female=10422\n",
      "\n",
      "[MALE] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE] Best iteration: 336\n",
      "\n",
      "[MALE] Fold 2/5\n",
      "[MALE] Best iteration: 328\n",
      "\n",
      "[MALE] Fold 3/5\n",
      "[MALE] Best iteration: 398\n",
      "\n",
      "[MALE] Fold 4/5\n",
      "[MALE] Best iteration: 354\n",
      "\n",
      "[MALE] Fold 5/5\n",
      "[MALE] Best iteration: 332\n",
      "\n",
      "[MALE] OOF Accuracy: 0.8876 | Macro F1: 0.7513\n",
      "[MALE] Best iterations: [336, 328, 398, 354, 332] | Median: 336\n",
      "\n",
      "[FEMALE] Fold 1/5\n",
      "[FEMALE] Best iteration: 360\n",
      "\n",
      "[FEMALE] Fold 2/5\n",
      "[FEMALE] Best iteration: 334\n",
      "\n",
      "[FEMALE] Fold 3/5\n",
      "[FEMALE] Best iteration: 242\n",
      "\n",
      "[FEMALE] Fold 4/5\n",
      "[FEMALE] Best iteration: 368\n",
      "\n",
      "[FEMALE] Fold 5/5\n",
      "[FEMALE] Best iteration: 363\n",
      "\n",
      "[FEMALE] OOF Accuracy: 0.9163 | Macro F1: 0.7487\n",
      "[FEMALE] Best iterations: [360, 334, 242, 368, 363] | Median: 360\n",
      "\n",
      "========== OVERALL OOF ==========\n",
      "OOF Accuracy: 0.9019 | OOF Macro F1: 0.8925\n",
      "\n",
      "OOF Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.93      0.94      0.93      1870\n",
      "      Normal_Weight       0.89      0.88      0.88      2345\n",
      "     Obesity_Type_I       0.89      0.87      0.88      2207\n",
      "    Obesity_Type_II       0.96      0.97      0.97      2403\n",
      "   Obesity_Type_III       1.00      1.00      1.00      2983\n",
      " Overweight_Level_I       0.79      0.78      0.79      1844\n",
      "Overweight_Level_II       0.79      0.83      0.81      1881\n",
      "\n",
      "           accuracy                           0.90     15533\n",
      "          macro avg       0.89      0.89      0.89     15533\n",
      "       weighted avg       0.90      0.90      0.90     15533\n",
      "\n",
      "\n",
      "Saved submission.csv\n",
      "   id       WeightCategory\n",
      "0   0  Overweight_Level_II\n",
      "1   1        Normal_Weight\n",
      "2   2  Insufficient_Weight\n",
      "3   3     Obesity_Type_III\n",
      "4   4  Overweight_Level_II\n",
      "\n",
      "[MALE (Kaggle)] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE (Kaggle)] Best iteration: 336\n",
      "\n",
      "[MALE (Kaggle)] Fold 2/5\n",
      "[MALE (Kaggle)] Best iteration: 328\n",
      "\n",
      "[MALE (Kaggle)] Fold 3/5\n",
      "[MALE (Kaggle)] Best iteration: 398\n",
      "\n",
      "[MALE (Kaggle)] Fold 4/5\n",
      "[MALE (Kaggle)] Best iteration: 354\n",
      "\n",
      "[MALE (Kaggle)] Fold 5/5\n",
      "[MALE (Kaggle)] Best iteration: 332\n",
      "\n",
      "[MALE (Kaggle)] OOF Accuracy: 0.8876 | Macro F1: 0.7513\n",
      "[MALE (Kaggle)] Best iterations: [336, 328, 398, 354, 332] | Median: 336\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 1/5\n",
      "[FEMALE (Kaggle)] Best iteration: 360\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 2/5\n",
      "[FEMALE (Kaggle)] Best iteration: 334\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 3/5\n",
      "[FEMALE (Kaggle)] Best iteration: 242\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 4/5\n",
      "[FEMALE (Kaggle)] Best iteration: 368\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 5/5\n",
      "[FEMALE (Kaggle)] Best iteration: 363\n",
      "\n",
      "[FEMALE (Kaggle)] OOF Accuracy: 0.9163 | Macro F1: 0.7487\n",
      "[FEMALE (Kaggle)] Best iterations: [360, 334, 242, 368, 363] | Median: 360\n",
      "\n",
      "âœ… Overall Accuracy on Kaggle_test: 0.90947\n",
      "\n",
      "=== Confusion Matrix (counts) ===\n",
      "Predicted â†’\n",
      "True â†“\n",
      "Insufficient_Weight   :  623 |   27 |    3 |    0 |    0 |    0 |    0\n",
      "Normal_Weight         :   42 |  648 |   38 |    8 |    1 |    0 |    0\n",
      "Overweight_Level_I    :    4 |   49 |  453 |   68 |    9 |    0 |    0\n",
      "Overweight_Level_II   :    0 |   16 |   51 |  525 |   45 |    4 |    0\n",
      "Obesity_Type_I        :    1 |    1 |   11 |   48 |  623 |   17 |    2\n",
      "Obesity_Type_II       :    0 |    0 |    2 |    5 |   19 |  819 |    0\n",
      "Obesity_Type_III      :    0 |    0 |    1 |    0 |    1 |    0 | 1061\n",
      "\n",
      "=== Confusion Matrix (row-normalized) ===\n",
      "Insufficient_Weight   : 0.95 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00\n",
      "Normal_Weight         : 0.06 | 0.88 | 0.05 | 0.01 | 0.00 | 0.00 | 0.00\n",
      "Overweight_Level_I    : 0.01 | 0.08 | 0.78 | 0.12 | 0.02 | 0.00 | 0.00\n",
      "Overweight_Level_II   : 0.00 | 0.02 | 0.08 | 0.82 | 0.07 | 0.01 | 0.00\n",
      "Obesity_Type_I        : 0.00 | 0.00 | 0.02 | 0.07 | 0.89 | 0.02 | 0.00\n",
      "Obesity_Type_II       : 0.00 | 0.00 | 0.00 | 0.01 | 0.02 | 0.97 | 0.00\n",
      "Obesity_Type_III      : 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00\n",
      "\n",
      "=== Per-class metrics ===\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight     0.9299    0.9541    0.9418       653\n",
      "      Normal_Weight     0.8745    0.8792    0.8769       737\n",
      " Overweight_Level_I     0.8104    0.7770    0.7933       583\n",
      "Overweight_Level_II     0.8028    0.8190    0.8108       641\n",
      "     Obesity_Type_I     0.8926    0.8862    0.8894       703\n",
      "    Obesity_Type_II     0.9750    0.9692    0.9721       845\n",
      "   Obesity_Type_III     0.9981    0.9981    0.9981      1063\n",
      "\n",
      "           accuracy                         0.9095      5225\n",
      "          macro avg     0.8976    0.8976    0.8975      5225\n",
      "       weighted avg     0.9093    0.9095    0.9093      5225\n",
      "\n",
      "\n",
      "=== Per-class accuracy (diagonal/row total) ===\n",
      "Insufficient_Weight    | Correct: 623 / 653 |  95.41%\n",
      "Normal_Weight          | Correct: 648 / 737 |  87.92%\n",
      "Overweight_Level_I     | Correct: 453 / 583 |  77.70%\n",
      "Overweight_Level_II    | Correct: 525 / 641 |  81.90%\n",
      "Obesity_Type_I         | Correct: 623 / 703 |  88.62%\n",
      "Obesity_Type_II        | Correct: 819 / 845 |  96.92%\n",
      "Obesity_Type_III       | Correct: 1061 / 1063 |  99.81%\n",
      "\n",
      "=== Most common confusions (true â†’ predicted) ===\n",
      "Overweight_Level_I        â†’ Overweight_Level_II       | Count:  68 | Row%:  11.7\n",
      "Overweight_Level_II       â†’ Overweight_Level_I        | Count:  51 | Row%:   8.0\n",
      "Overweight_Level_I        â†’ Normal_Weight             | Count:  49 | Row%:   8.4\n",
      "Obesity_Type_I            â†’ Overweight_Level_II       | Count:  48 | Row%:   6.8\n",
      "Overweight_Level_II       â†’ Obesity_Type_I            | Count:  45 | Row%:   7.0\n",
      "Normal_Weight             â†’ Insufficient_Weight       | Count:  42 | Row%:   5.7\n",
      "Normal_Weight             â†’ Overweight_Level_I        | Count:  38 | Row%:   5.2\n",
      "Insufficient_Weight       â†’ Normal_Weight             | Count:  27 | Row%:   4.1\n",
      "Obesity_Type_II           â†’ Obesity_Type_I            | Count:  19 | Row%:   2.2\n",
      "Obesity_Type_I            â†’ Obesity_Type_II           | Count:  17 | Row%:   2.4\n",
      "\n",
      "=== Sample of misclassified rows (first 10) ===\n",
      "Row    9: true=Overweight_Level_II    pred=Obesity_Type_I         conf=0.927 2nd=Overweight_Level_II   (0.041)\n",
      "Row   16: true=Overweight_Level_I     pred=Normal_Weight          conf=0.935 2nd=Overweight_Level_I    (0.056)\n",
      "Row   28: true=Normal_Weight          pred=Insufficient_Weight    conf=0.893 2nd=Normal_Weight         (0.100)\n",
      "Row   30: true=Overweight_Level_I     pred=Overweight_Level_II    conf=0.786 2nd=Overweight_Level_I    (0.162)\n",
      "Row   33: true=Overweight_Level_I     pred=Normal_Weight          conf=0.961 2nd=Overweight_Level_I    (0.027)\n",
      "Row   43: true=Obesity_Type_II        pred=Obesity_Type_I         conf=0.678 2nd=Obesity_Type_II       (0.306)\n",
      "Row   61: true=Obesity_Type_I         pred=Overweight_Level_II    conf=0.850 2nd=Obesity_Type_I        (0.126)\n",
      "Row   65: true=Overweight_Level_I     pred=Overweight_Level_II    conf=0.779 2nd=Obesity_Type_I        (0.151)\n",
      "Row   68: true=Overweight_Level_I     pred=Normal_Weight          conf=0.708 2nd=Overweight_Level_I    (0.283)\n",
      "Row   75: true=Overweight_Level_I     pred=Normal_Weight          conf=0.938 2nd=Overweight_Level_I    (0.058)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Gender-specific XGB + BMI + targeted class boost + Kaggle_test eval\n",
    "# ==============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.base import clone\n",
    "import xgboost as xgb\n",
    "\n",
    "# -------- Paths --------\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "SAMPLE_SUB_PATH = \"sample_submission.csv\"\n",
    "KAGGLE_TEST_PATH = \"Kaggle_test.csv\"  # has WeightCategory ground truth\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "N_JOBS = -1\n",
    "\n",
    "# -------- Helpers --------\n",
    "def norm_col(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    return str(s).replace(\"\\ufeff\", \"\").strip().lower()\n",
    "\n",
    "def infer_feature_types(df):\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def detect_gender_column(df):\n",
    "    # common names\n",
    "    for c in df.columns:\n",
    "        if norm_col(c) in {\"gender\", \"sex\"}:\n",
    "            return c\n",
    "    # fallback: column that looks like M/F\n",
    "    for c in df.columns:\n",
    "        vals = pd.Series(df[c].dropna().astype(str).str.lower().str.strip()).unique()\n",
    "        if len(vals) in (2, 3):\n",
    "            if any(v.startswith(\"m\") for v in vals) and any(v.startswith(\"f\") for v in vals):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def split_by_gender(series):\n",
    "    s = series.astype(str).str.lower().str.strip()\n",
    "    male_mask = s.str.startswith((\"m\",\"1\",\"true\"))\n",
    "    female_mask = s.str.startswith((\"f\",\"0\",\"false\"))\n",
    "    if male_mask.sum()==0 and female_mask.sum()==0:\n",
    "        top = s.value_counts().index.tolist()\n",
    "        if len(top)>=2:\n",
    "            male_mask = s==top[0]\n",
    "            female_mask = s==top[1]\n",
    "    return male_mask, female_mask\n",
    "\n",
    "def add_bmi(df):\n",
    "    \"\"\"Compute BMI = Weight / (Height_m^2).\n",
    "       If median height > 3 assume cm â†’ convert to meters.\"\"\"\n",
    "    if (\"Weight\" in df.columns) and (\"Height\" in df.columns):\n",
    "        h = df[\"Height\"].astype(float)\n",
    "        height_m = np.where(h.median() > 3.0, h / 100.0, h)\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            bmi = df[\"Weight\"].astype(float) / (np.power(height_m, 2) + 1e-12)\n",
    "        df[\"BMI\"] = pd.Series(bmi).replace([np.inf, -np.inf], np.nan)\n",
    "    return df\n",
    "\n",
    "# -------- Load data --------\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "# Drop columns we donâ€™t want in this run\n",
    "for c in [\"MTRANS\",\"SMOKE\"]:\n",
    "    if c in train.columns: train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:  test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "train = add_bmi(train)\n",
    "test  = add_bmi(test)\n",
    "\n",
    "# Detect ID/Target from files (simple logic)\n",
    "id_col = None\n",
    "for cand in [\"id\", \"row_id\", \"index\", \"sample_id\"]:\n",
    "    if cand in train.columns and cand in test.columns:\n",
    "        id_col = cand\n",
    "        break\n",
    "\n",
    "target_col = None\n",
    "for cand in [\"WeightCategory\", \"NObeyesdad\", \"label\", \"target\", \"class\", \"y\"]:\n",
    "    if cand in train.columns:\n",
    "        target_col = cand\n",
    "        break\n",
    "if target_col is None:\n",
    "    raise ValueError(\"Could not detect target column in train.csv\")\n",
    "\n",
    "# Build X/y\n",
    "y = train[target_col].copy()\n",
    "X = train.drop(columns=[target_col]).copy()\n",
    "if id_col and id_col in X.columns:\n",
    "    X.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "test_features = test.copy()\n",
    "if id_col and id_col in test_features.columns:\n",
    "    test_ids = test_features[id_col].copy()\n",
    "    test_features.drop(columns=[id_col], inplace=True)\n",
    "else:\n",
    "    test_ids = pd.Series(np.arange(len(test_features)), name=\"id\")\n",
    "\n",
    "# Label encode target\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "classes = list(le.classes_)\n",
    "print(f\"[Info] Classes: {classes}\")\n",
    "\n",
    "# Detect gender and split\n",
    "gender_col = detect_gender_column(pd.concat([X, test_features], axis=0))\n",
    "if gender_col is None:\n",
    "    raise ValueError(\"Could not detect a gender column (e.g., 'Gender'/'SEX').\")\n",
    "male_mask, female_mask = split_by_gender(train[gender_col])\n",
    "test_male_mask, test_female_mask = split_by_gender(test_features[gender_col])\n",
    "print(f\"[Info] Train male={int(male_mask.sum())}, female={int(female_mask.sum())}\")\n",
    "print(f\"[Info] Test  male={int(test_male_mask.sum())}, female={int(test_female_mask.sum())}\")\n",
    "\n",
    "# -------- Training function (gender-specific) with class boosting for two classes --------\n",
    "def train_group_and_predict(X_grp, y_enc_grp, test_grp, group_name,\n",
    "                            boost_targets=(\"Overweight_Level_I\",\"Overweight_Level_II\"),\n",
    "                            base_boost=1.50, jitter_amp=0.10):\n",
    "    # Drop gender column inside a group (constant after split)\n",
    "    cols_to_use = [c for c in X_grp.columns if c != gender_col]\n",
    "    Xg = X_grp[cols_to_use].copy()\n",
    "    Xtestg = test_grp[cols_to_use].copy()\n",
    "\n",
    "    num_cols, cat_cols = infer_feature_types(Xg)\n",
    "\n",
    "    # Preprocessor\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", ohe)\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=1.0\n",
    "    )\n",
    "\n",
    "    # XGB params\n",
    "    xgb_params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": len(classes),\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"max_depth\": 6,\n",
    "        \"min_child_weight\": 2,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"lambda\": 1.0,\n",
    "        \"alpha\": 0.0,\n",
    "        \"eta\": 0.03,\n",
    "        \"nthread\": N_JOBS,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "    }\n",
    "    NUM_BOOST_ROUND = 20000\n",
    "    EARLY_STOP = 200\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof_group = np.zeros((len(Xg), len(classes)), dtype=np.float32)\n",
    "    test_group_pred = np.zeros((len(Xtestg), len(classes)), dtype=np.float32)\n",
    "    fold_best = []\n",
    "\n",
    "    # map class name -> index\n",
    "    cls_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(Xg, y_enc_grp), start=1):\n",
    "        print(f\"\\n[{group_name}] Fold {fold}/{N_FOLDS}\")\n",
    "        X_tr, X_va = Xg.iloc[tr_idx], Xg.iloc[va_idx]\n",
    "        y_tr, y_va = y_enc_grp[tr_idx], y_enc_grp[va_idx]\n",
    "\n",
    "        prep = clone(preprocessor)\n",
    "        Xtr = prep.fit_transform(X_tr)\n",
    "        Xva = prep.transform(X_va)\n",
    "\n",
    "        # ---- RANDOM (non-count) WEIGHTS to gently boost two classes ----\n",
    "        w_tr = np.ones_like(y_tr, dtype=float)\n",
    "        rng = np.random.default_rng(RANDOM_STATE + fold)  # deterministic per fold\n",
    "        for t in boost_targets:\n",
    "            if t in cls_to_idx:\n",
    "                cls_id = cls_to_idx[t]\n",
    "                idx_t = np.where(y_tr == cls_id)[0]\n",
    "                if idx_t.size > 0:\n",
    "                    jitter = rng.uniform(-jitter_amp, jitter_amp, size=idx_t.size)\n",
    "                    w_tr[idx_t] = base_boost + jitter\n",
    "        w_va = np.ones_like(y_va, dtype=float)\n",
    "\n",
    "        dtrain = xgb.DMatrix(Xtr, label=y_tr, weight=w_tr)\n",
    "        dval   = xgb.DMatrix(Xva, label=y_va, weight=w_va)\n",
    "\n",
    "        bst = xgb.train(\n",
    "            params=xgb_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=NUM_BOOST_ROUND,\n",
    "            evals=[(dtrain, \"train\"), (dval, \"valid\")],\n",
    "            feval=None,\n",
    "            early_stopping_rounds=EARLY_STOP,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        best_round = int(bst.best_iteration + 1)\n",
    "        fold_best.append(best_round)\n",
    "        print(f\"[{group_name}] Best iteration: {best_round}\")\n",
    "\n",
    "        oof_proba = bst.predict(dval, iteration_range=(0, best_round))\n",
    "        oof_group[va_idx] = oof_proba\n",
    "\n",
    "        # test preds for this fold\n",
    "        Xtest_tf = prep.transform(Xtestg)\n",
    "        dtest = xgb.DMatrix(Xtest_tf)\n",
    "        test_group_pred += bst.predict(dtest, iteration_range=(0, best_round)) / N_FOLDS\n",
    "\n",
    "    # OOF summary for the group\n",
    "    oof_labels = np.argmax(oof_group, axis=1)\n",
    "    acc_g = accuracy_score(y_enc_grp, oof_labels)\n",
    "    f1_g = f1_score(y_enc_grp, oof_labels, average=\"macro\")\n",
    "    print(f\"\\n[{group_name}] OOF Accuracy: {acc_g:.4f} | Macro F1: {f1_g:.4f}\")\n",
    "    print(f\"[{group_name}] Best iterations: {fold_best} | Median: {int(np.median(fold_best))}\")\n",
    "\n",
    "    return oof_group, test_group_pred\n",
    "\n",
    "# -------- Train per-gender and predict full test --------\n",
    "X_male = X[male_mask].reset_index(drop=True)\n",
    "y_male_enc = y_enc[male_mask]\n",
    "test_male = test_features[test_male_mask].reset_index(drop=True)\n",
    "\n",
    "X_female = X[female_mask].reset_index(drop=True)\n",
    "y_female_enc = y_enc[female_mask]\n",
    "test_female = test_features[test_female_mask].reset_index(drop=True)\n",
    "\n",
    "male_oof, male_test_pred = train_group_and_predict(X_male, y_male_enc, test_male, \"MALE\")\n",
    "female_oof, female_test_pred = train_group_and_predict(X_female, y_female_enc, test_female, \"FEMALE\")\n",
    "\n",
    "# Combine OOF\n",
    "oof_full = np.zeros((len(X), len(classes)), dtype=np.float32)\n",
    "oof_full[male_mask.values] = male_oof\n",
    "oof_full[female_mask.values] = female_oof\n",
    "\n",
    "oof_labels = np.argmax(oof_full, axis=1)\n",
    "oof_acc = accuracy_score(y_enc, oof_labels)\n",
    "oof_f1 = f1_score(y_enc, oof_labels, average=\"macro\")\n",
    "print(\"\\n========== OVERALL OOF ==========\")\n",
    "print(f\"OOF Accuracy: {oof_acc:.4f} | OOF Macro F1: {oof_f1:.4f}\")\n",
    "try:\n",
    "    print(\"\\nOOF Classification Report:\\n\",\n",
    "          classification_report(y_enc, oof_labels, target_names=classes))\n",
    "except Exception as e:\n",
    "    print(f\"[Info] Could not print classification report: {e}\")\n",
    "\n",
    "# Build full test predictions (for Kaggle submission use-case)\n",
    "test_pred_proba = np.zeros((len(test_features), len(classes)), dtype=np.float32)\n",
    "test_pred_proba[test_male_mask.values] = male_test_pred\n",
    "test_pred_proba[test_female_mask.values] = female_test_pred\n",
    "\n",
    "test_pred_int = np.argmax(test_pred_proba, axis=1)\n",
    "test_pred_labels = le.inverse_transform(test_pred_int)\n",
    "\n",
    "# Submission\n",
    "ss_cols = list(sample_sub.columns)\n",
    "ID_HEADER = None\n",
    "LABEL_HEADER = None\n",
    "if len(ss_cols) == 2:\n",
    "    # detect which is ID by presence in test\n",
    "    c1, c2 = ss_cols\n",
    "    if c1 in test.columns and c2 not in test.columns:\n",
    "        ID_HEADER, LABEL_HEADER = c1, c2\n",
    "    elif c2 in test.columns and c1 not in test.columns:\n",
    "        ID_HEADER, LABEL_HEADER = c2, c1\n",
    "if ID_HEADER is None:\n",
    "    # fallback\n",
    "    ID_HEADER = ss_cols[0]\n",
    "    LABEL_HEADER = ss_cols[1]\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "if ID_HEADER in test.columns:\n",
    "    sub[ID_HEADER] = test[ID_HEADER].values\n",
    "else:\n",
    "    sub[ID_HEADER] = np.arange(len(test_features))\n",
    "sub[LABEL_HEADER] = test_pred_labels\n",
    "\n",
    "# Ensure column order\n",
    "for c in ss_cols:\n",
    "    if c not in sub.columns:\n",
    "        sub[c] = sample_sub[c].iloc[0] if len(sample_sub[c]) else None\n",
    "sub = sub[ss_cols]\n",
    "\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSaved submission.csv\")\n",
    "print(sub.head(5))\n",
    "\n",
    "# ==============================================\n",
    "# Evaluate on Kaggle_test.csv (with ground truth)\n",
    "# ==============================================\n",
    "kdf = pd.read_csv(KAGGLE_TEST_PATH)\n",
    "if \"WeightCategory\" not in kdf.columns:\n",
    "    raise KeyError(\"Kaggle_test.csv must contain 'WeightCategory'.\")\n",
    "\n",
    "y_true = kdf[\"WeightCategory\"].copy()\n",
    "X_k = kdf.drop(columns=[\"WeightCategory\"], errors=\"ignore\").copy()\n",
    "if id_col and id_col in X_k.columns:\n",
    "    X_k.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "# same drops + BMI\n",
    "for c in [\"MTRANS\",\"SMOKE\"]:\n",
    "    if c in X_k.columns:\n",
    "        X_k.drop(columns=[c], inplace=True)\n",
    "X_k = add_bmi(X_k)\n",
    "\n",
    "# detect gender and split for Kaggle set\n",
    "gender_col_k = detect_gender_column(X_k)\n",
    "if gender_col_k is None:\n",
    "    raise ValueError(\"Could not detect a gender column in Kaggle_test.csv\")\n",
    "km_k, kf_k = split_by_gender(X_k[gender_col_k])\n",
    "\n",
    "# Predict on Kaggle by reusing the same training procedure (per gender)\n",
    "kaggle_pred_proba = np.zeros((len(X_k), len(classes)), dtype=np.float32)\n",
    "\n",
    "if X_male.shape[0] > 0 and km_k.sum() > 0:\n",
    "    _, male_k_pred = train_group_and_predict(X_male, y_male_enc, X_k[km_k].reset_index(drop=True), \"MALE (Kaggle)\")\n",
    "    kaggle_pred_proba[km_k.values] = male_k_pred\n",
    "if X_female.shape[0] > 0 and kf_k.sum() > 0:\n",
    "    _, female_k_pred = train_group_and_predict(X_female, y_female_enc, X_k[kf_k].reset_index(drop=True), \"FEMALE (Kaggle)\")\n",
    "    kaggle_pred_proba[kf_k.values] = female_k_pred\n",
    "\n",
    "kaggle_pred_idx = np.argmax(kaggle_pred_proba, axis=1)\n",
    "y_pred = le.inverse_transform(kaggle_pred_idx)\n",
    "\n",
    "# -------- Overall accuracy to 5 decimals --------\n",
    "overall_acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nâœ… Overall Accuracy on Kaggle_test: {overall_acc:.5f}\")\n",
    "\n",
    "# -------- Text-only error analysis (custom order) --------\n",
    "order = [\n",
    "    'Insufficient_Weight',\n",
    "    'Normal_Weight',\n",
    "    'Overweight_Level_I',\n",
    "    'Overweight_Level_II',\n",
    "    'Obesity_Type_I',\n",
    "    'Obesity_Type_II',\n",
    "    'Obesity_Type_III'\n",
    "]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=order)\n",
    "cm_norm = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (counts) ===\")\n",
    "print(\"Predicted â†’\")\n",
    "print(\"True â†“\")\n",
    "for i, true_class in enumerate(order):\n",
    "    row = \" | \".join(f\"{cm[i, j]:4d}\" for j in range(len(order)))\n",
    "    print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (row-normalized) ===\")\n",
    "for i, true_class in enumerate(order):\n",
    "    row = \" | \".join(f\"{cm_norm[i, j]:.2f}\" for j in range(len(order)))\n",
    "    print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "print(\"\\n=== Per-class metrics ===\")\n",
    "try:\n",
    "    print(classification_report(y_true, y_pred, labels=order, target_names=order, digits=4, zero_division=0))\n",
    "except Exception as e:\n",
    "    print(f\"[Info] classification_report fallback: {e}\")\n",
    "    print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "print(\"\\n=== Per-class accuracy (diagonal/row total) ===\")\n",
    "for i, c in enumerate(order):\n",
    "    total = cm[i].sum()\n",
    "    correct = cm[i, i]\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"{c:<22} | Correct: {correct:3d} / {total:3d} | {acc*100:6.2f}%\")\n",
    "\n",
    "print(\"\\n=== Most common confusions (true â†’ predicted) ===\")\n",
    "pairs = []\n",
    "for i, t in enumerate(order):\n",
    "    for j, p in enumerate(order):\n",
    "        if i == j or cm[i, j] == 0:\n",
    "            continue\n",
    "        pairs.append((cm[i, j], t, p, cm_norm[i, j]))\n",
    "pairs = sorted(pairs, key=lambda x: (-x[0], -x[3]))\n",
    "for cnt, true_label, pred_label, norm_val in pairs[:10]:\n",
    "    print(f\"{true_label:25} â†’ {pred_label:25} | Count: {cnt:3d} | Row%: {norm_val*100:5.1f}\")\n",
    "\n",
    "print(\"\\n=== Sample of misclassified rows (first 10) ===\")\n",
    "mis_idx = np.where(np.asarray(y_true) != np.asarray(y_pred))[0]\n",
    "if len(mis_idx) == 0:\n",
    "    print(\"ðŸŽ‰ No misclassifications!\")\n",
    "else:\n",
    "    for idx in mis_idx[:10]:\n",
    "        true_lab = y_true.iloc[idx] if hasattr(y_true, \"iloc\") else y_true[idx]\n",
    "        pred_lab = y_pred[idx]\n",
    "        conf = float(np.max(kaggle_pred_proba[idx]))\n",
    "        rank = np.argsort(-kaggle_pred_proba[idx])\n",
    "        second_idx = rank[1] if rank.size > 1 else rank[0]\n",
    "        second_lab = le.inverse_transform([second_idx])[0]\n",
    "        second_conf = float(kaggle_pred_proba[idx][second_idx])\n",
    "        print(f\"Row {idx:4d}: true={true_lab:<22} pred={pred_lab:<22} conf={conf:.3f} 2nd={second_lab:<22}({second_conf:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9afb89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f4ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d533e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bbfd6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Overall Accuracy on Kaggle_test: 0.90947\n",
      "\n",
      "=== Confusion Matrix (counts) ===\n",
      "Predicted â†’\n",
      "True â†“\n",
      "Insufficient_Weight   :  623 |   27 |    3 |    0 |    0 |    0 |    0\n",
      "Normal_Weight         :   42 |  648 |   38 |    8 |    1 |    0 |    0\n",
      "Overweight_Level_I    :    4 |   49 |  453 |   68 |    9 |    0 |    0\n",
      "Overweight_Level_II   :    0 |   16 |   51 |  525 |   45 |    4 |    0\n",
      "Obesity_Type_I        :    1 |    1 |   11 |   48 |  623 |   17 |    2\n",
      "Obesity_Type_II       :    0 |    0 |    2 |    5 |   19 |  819 |    0\n",
      "Obesity_Type_III      :    0 |    0 |    1 |    0 |    1 |    0 | 1061\n",
      "\n",
      "=== Confusion Matrix (row-normalized) ===\n",
      "Insufficient_Weight   : 0.95 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00\n",
      "Normal_Weight         : 0.06 | 0.88 | 0.05 | 0.01 | 0.00 | 0.00 | 0.00\n",
      "Overweight_Level_I    : 0.01 | 0.08 | 0.78 | 0.12 | 0.02 | 0.00 | 0.00\n",
      "Overweight_Level_II   : 0.00 | 0.02 | 0.08 | 0.82 | 0.07 | 0.01 | 0.00\n",
      "Obesity_Type_I        : 0.00 | 0.00 | 0.02 | 0.07 | 0.89 | 0.02 | 0.00\n",
      "Obesity_Type_II       : 0.00 | 0.00 | 0.00 | 0.01 | 0.02 | 0.97 | 0.00\n",
      "Obesity_Type_III      : 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00\n",
      "\n",
      "=== Per-class metrics ===\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight     0.9299    0.9541    0.9418       653\n",
      "      Normal_Weight     0.8745    0.8792    0.8769       737\n",
      " Overweight_Level_I     0.8104    0.7770    0.7933       583\n",
      "Overweight_Level_II     0.8028    0.8190    0.8108       641\n",
      "     Obesity_Type_I     0.8926    0.8862    0.8894       703\n",
      "    Obesity_Type_II     0.9750    0.9692    0.9721       845\n",
      "   Obesity_Type_III     0.9981    0.9981    0.9981      1063\n",
      "\n",
      "           accuracy                         0.9095      5225\n",
      "          macro avg     0.8976    0.8976    0.8975      5225\n",
      "       weighted avg     0.9093    0.9095    0.9093      5225\n",
      "\n",
      "\n",
      "=== Per-class accuracy (diagonal/row total) ===\n",
      "Insufficient_Weight    | Correct: 623 / 653 |  95.41%\n",
      "Normal_Weight          | Correct: 648 / 737 |  87.92%\n",
      "Overweight_Level_I     | Correct: 453 / 583 |  77.70%\n",
      "Overweight_Level_II    | Correct: 525 / 641 |  81.90%\n",
      "Obesity_Type_I         | Correct: 623 / 703 |  88.62%\n",
      "Obesity_Type_II        | Correct: 819 / 845 |  96.92%\n",
      "Obesity_Type_III       | Correct: 1061 / 1063 |  99.81%\n",
      "\n",
      "=== Most common confusions (true â†’ predicted) ===\n",
      "Overweight_Level_I        â†’ Overweight_Level_II       | Count:  68 | Row%:  11.7\n",
      "Overweight_Level_II       â†’ Overweight_Level_I        | Count:  51 | Row%:   8.0\n",
      "Overweight_Level_I        â†’ Normal_Weight             | Count:  49 | Row%:   8.4\n",
      "Obesity_Type_I            â†’ Overweight_Level_II       | Count:  48 | Row%:   6.8\n",
      "Overweight_Level_II       â†’ Obesity_Type_I            | Count:  45 | Row%:   7.0\n",
      "Normal_Weight             â†’ Insufficient_Weight       | Count:  42 | Row%:   5.7\n",
      "Normal_Weight             â†’ Overweight_Level_I        | Count:  38 | Row%:   5.2\n",
      "Insufficient_Weight       â†’ Normal_Weight             | Count:  27 | Row%:   4.1\n",
      "Obesity_Type_II           â†’ Obesity_Type_I            | Count:  19 | Row%:   2.2\n",
      "Obesity_Type_I            â†’ Obesity_Type_II           | Count:  17 | Row%:   2.4\n",
      "\n",
      "=== Sample of misclassified rows (first 10) ===\n",
      "Row    9: true=Overweight_Level_II    pred=Obesity_Type_I         conf=0.927 2nd=Overweight_Level_II   (0.041)\n",
      "Row   16: true=Overweight_Level_I     pred=Normal_Weight          conf=0.935 2nd=Overweight_Level_I    (0.056)\n",
      "Row   28: true=Normal_Weight          pred=Insufficient_Weight    conf=0.893 2nd=Normal_Weight         (0.100)\n",
      "Row   30: true=Overweight_Level_I     pred=Overweight_Level_II    conf=0.786 2nd=Overweight_Level_I    (0.162)\n",
      "Row   33: true=Overweight_Level_I     pred=Normal_Weight          conf=0.961 2nd=Overweight_Level_I    (0.027)\n",
      "Row   43: true=Obesity_Type_II        pred=Obesity_Type_I         conf=0.678 2nd=Obesity_Type_II       (0.306)\n",
      "Row   61: true=Obesity_Type_I         pred=Overweight_Level_II    conf=0.850 2nd=Obesity_Type_I        (0.126)\n",
      "Row   65: true=Overweight_Level_I     pred=Overweight_Level_II    conf=0.779 2nd=Obesity_Type_I        (0.151)\n",
      "Row   68: true=Overweight_Level_I     pred=Normal_Weight          conf=0.708 2nd=Overweight_Level_I    (0.283)\n",
      "Row   75: true=Overweight_Level_I     pred=Normal_Weight          conf=0.938 2nd=Overweight_Level_I    (0.058)\n"
     ]
    }
   ],
   "source": [
    "# -------- Overall accuracy to 5 decimals --------\n",
    "overall_acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nâœ… Overall Accuracy on Kaggle_test: {overall_acc:.5f}\")\n",
    "\n",
    "# -------- Text-only error analysis (custom order) --------\n",
    "order = [\n",
    "    'Insufficient_Weight',\n",
    "    'Normal_Weight',\n",
    "    'Overweight_Level_I',\n",
    "    'Overweight_Level_II',\n",
    "    'Obesity_Type_I',\n",
    "    'Obesity_Type_II',\n",
    "    'Obesity_Type_III'\n",
    "]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=order)\n",
    "cm_norm = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (counts) ===\")\n",
    "print(\"Predicted â†’\")\n",
    "print(\"True â†“\")\n",
    "for i, true_class in enumerate(order):\n",
    "    row = \" | \".join(f\"{cm[i, j]:4d}\" for j in range(len(order)))\n",
    "    print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (row-normalized) ===\")\n",
    "for i, true_class in enumerate(order):\n",
    "    row = \" | \".join(f\"{cm_norm[i, j]:.2f}\" for j in range(len(order)))\n",
    "    print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "print(\"\\n=== Per-class metrics ===\")\n",
    "try:\n",
    "    print(classification_report(y_true, y_pred, labels=order, target_names=order, digits=4, zero_division=0))\n",
    "except Exception as e:\n",
    "    print(f\"[Info] classification_report fallback: {e}\")\n",
    "    print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "print(\"\\n=== Per-class accuracy (diagonal/row total) ===\")\n",
    "for i, c in enumerate(order):\n",
    "    total = cm[i].sum()\n",
    "    correct = cm[i, i]\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f\"{c:<22} | Correct: {correct:3d} / {total:3d} | {acc*100:6.2f}%\")\n",
    "\n",
    "print(\"\\n=== Most common confusions (true â†’ predicted) ===\")\n",
    "pairs = []\n",
    "for i, t in enumerate(order):\n",
    "    for j, p in enumerate(order):\n",
    "        if i == j or cm[i, j] == 0:\n",
    "            continue\n",
    "        pairs.append((cm[i, j], t, p, cm_norm[i, j]))\n",
    "pairs = sorted(pairs, key=lambda x: (-x[0], -x[3]))\n",
    "for cnt, true_label, pred_label, norm_val in pairs[:10]:\n",
    "    print(f\"{true_label:25} â†’ {pred_label:25} | Count: {cnt:3d} | Row%: {norm_val*100:5.1f}\")\n",
    "\n",
    "print(\"\\n=== Sample of misclassified rows (first 10) ===\")\n",
    "mis_idx = np.where(np.asarray(y_true) != np.asarray(y_pred))[0]\n",
    "if len(mis_idx) == 0:\n",
    "    print(\"ðŸŽ‰ No misclassifications!\")\n",
    "else:\n",
    "    for idx in mis_idx[:10]:\n",
    "        true_lab = y_true.iloc[idx] if hasattr(y_true, \"iloc\") else y_true[idx]\n",
    "        pred_lab = y_pred[idx]\n",
    "        conf = float(np.max(kaggle_pred_proba[idx]))\n",
    "        rank = np.argsort(-kaggle_pred_proba[idx])\n",
    "        second_idx = rank[1] if rank.size > 1 else rank[0]\n",
    "        second_lab = le.inverse_transform([second_idx])[0]\n",
    "        second_conf = float(kaggle_pred_proba[idx][second_idx])\n",
    "        print(f\"Row {idx:4d}: true={true_lab:<22} pred={pred_lab:<22} conf={conf:.3f} 2nd={second_lab:<22}({second_conf:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2262cb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c95615a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# Gender-specific XGB + BMI + targeted class boost + Kaggle_test eval\n",
    "# Cost-sensitive training + weighted-F1 early stopping for Overweight I/II\n",
    "# ==============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.base import clone\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "# -------- Paths --------\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "SAMPLE_SUB_PATH = \"sample_submission.csv\"\n",
    "KAGGLE_TEST_PATH = \"Kaggle_test.csv\"  # must contain WeightCategory for eval\n",
    "\n",
    "RANDOM_STATE = 27\n",
    "N_FOLDS = 5\n",
    "N_JOBS = -1\n",
    "\n",
    "# ---- Cost-sensitive knobs ----\n",
    "BOOST_CLASSES = (\"Overweight_Level_I\", \"Overweight_Level_II\")  # classes to emphasize\n",
    "TRAIN_WEIGHT_MULT = 2.25  # sample-weight multiplier for boosted classes (1.5â€“3.0 typical)\n",
    "F1_WEIGHT_FOR_BOOST = 2.0  # eval-time weight on boosted classes (1.5â€“3.0 typical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b41a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# Helpers\n",
    "# ==============================================\n",
    "def norm_col(s: str) -> str:\n",
    "    if s is None:\n",
    "        return s\n",
    "    return str(s).replace(\"\\ufeff\", \"\").strip().lower()\n",
    "\n",
    "def infer_feature_types(df):\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def detect_gender_column(df):\n",
    "    # common names\n",
    "    for c in df.columns:\n",
    "        if norm_col(c) in {\"gender\", \"sex\"}:\n",
    "            return c\n",
    "    # fallback: column that looks like M/F\n",
    "    for c in df.columns:\n",
    "        vals = pd.Series(df[c].dropna().astype(str).str.lower().str.strip()).unique()\n",
    "        if len(vals) in (2, 3):\n",
    "            if any(v.startswith(\"m\") for v in vals) and any(v.startswith(\"f\") for v in vals):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def split_by_gender(series):\n",
    "    s = series.astype(str).str.lower().str.strip()\n",
    "    male_mask = s.str.startswith((\"m\", \"1\", \"true\"))\n",
    "    female_mask = s.str.startswith((\"f\", \"0\", \"false\"))\n",
    "    if male_mask.sum() == 0 and female_mask.sum() == 0:\n",
    "        top = s.value_counts().index.tolist()\n",
    "        if len(top) >= 2:\n",
    "            male_mask = s == top[0]\n",
    "            female_mask = s == top[1]\n",
    "    return male_mask, female_mask\n",
    "\n",
    "def add_bmi(df):\n",
    "    \"\"\"Compute BMI = Weight / (Height_m^2).\n",
    "       If median height > 3 assume cm â†’ convert to meters.\"\"\"\n",
    "    if (\"Weight\" in df.columns) and (\"Height\" in df.columns):\n",
    "        h = pd.to_numeric(df[\"Height\"], errors=\"coerce\")\n",
    "        height_m = np.where(h.median(skipna=True) > 3.0, h / 100.0, h)\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            bmi = pd.to_numeric(df[\"Weight\"], errors=\"coerce\") / (np.power(height_m, 2) + 1e-12)\n",
    "        df[\"BMI\"] = pd.Series(bmi).replace([np.inf, -np.inf], np.nan)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6148954b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Classes: ['Insufficient_Weight', 'Normal_Weight', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III', 'Overweight_Level_I', 'Overweight_Level_II']\n",
      "[Info] Train male=7783, female=7750\n",
      "[Info] Test  male=10336, female=10422\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================\n",
    "# Load data\n",
    "# ==============================================\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "# Optional drops\n",
    "for c in [\"MTRANS\", \"SMOKE\"]:\n",
    "    if c in train.columns:\n",
    "        train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:\n",
    "        test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "train = add_bmi(train)\n",
    "test = add_bmi(test)\n",
    "\n",
    "# Detect ID/Target\n",
    "id_col = None\n",
    "for cand in [\"id\", \"row_id\", \"index\", \"sample_id\"]:\n",
    "    if cand in train.columns and cand in test.columns:\n",
    "        id_col = cand\n",
    "        break\n",
    "\n",
    "target_col = None\n",
    "for cand in [\"WeightCategory\", \"NObeyesdad\", \"label\", \"target\", \"class\", \"y\"]:\n",
    "    if cand in train.columns:\n",
    "        target_col = cand\n",
    "        break\n",
    "if target_col is None:\n",
    "    raise ValueError(\"Could not detect target column in train.csv\")\n",
    "\n",
    "# Build X/y\n",
    "y = train[target_col].copy()\n",
    "X = train.drop(columns=[target_col]).copy()\n",
    "if id_col and id_col in X.columns:\n",
    "    X.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "test_features = test.copy()\n",
    "if id_col and id_col in test_features.columns:\n",
    "    test_ids = test_features[id_col].copy()\n",
    "    test_features.drop(columns=[id_col], inplace=True)\n",
    "else:\n",
    "    test_ids = pd.Series(np.arange(len(test_features)), name=\"id\")\n",
    "\n",
    "# Label encode target\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "classes = list(le.classes_)\n",
    "print(f\"[Info] Classes: {classes}\")\n",
    "\n",
    "# Detect gender and split\n",
    "gender_col = detect_gender_column(pd.concat([X, test_features], axis=0))\n",
    "if gender_col is None:\n",
    "    raise ValueError(\"Could not detect a gender column (e.g., 'Gender'/'SEX').\")\n",
    "male_mask, female_mask = split_by_gender(train[gender_col])\n",
    "test_male_mask, test_female_mask = split_by_gender(test_features[gender_col])\n",
    "print(f\"[Info] Train male={int(male_mask.sum())}, female={int(female_mask.sum())}\")\n",
    "print(f\"[Info] Test  male={int(test_male_mask.sum())}, female={int(test_female_mask.sum())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78bcbfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# Training function (gender-specific)\n",
    "# Cost-sensitive training + weighted-F1 early stopping\n",
    "# ==============================================\n",
    "def train_group_and_predict(\n",
    "    X_grp,\n",
    "    y_enc_grp,\n",
    "    test_grp,\n",
    "    group_name,\n",
    "    boost_targets=BOOST_CLASSES,\n",
    "    base_boost=TRAIN_WEIGHT_MULT,\n",
    "):\n",
    "    # Drop gender column inside a group (constant after split)\n",
    "    cols_to_use = [c for c in X_grp.columns if c != gender_col]\n",
    "    Xg = X_grp[cols_to_use].copy()\n",
    "    Xtestg = test_grp[cols_to_use].copy()\n",
    "\n",
    "    num_cols, cat_cols = infer_feature_types(Xg)\n",
    "\n",
    "    # Preprocessor\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", ohe)\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=1.0\n",
    "    )\n",
    "\n",
    "    # XGB params\n",
    "    xgb_params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": len(classes),\n",
    "        \"eval_metric\": \"mlogloss\",  # keep for logging; early stopping uses feval\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"max_depth\": 6,\n",
    "        \"min_child_weight\": 2,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"lambda\": 1.0,\n",
    "        \"alpha\": 0.0,\n",
    "        \"eta\": 0.03,\n",
    "        \"nthread\": N_JOBS,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "    }\n",
    "    NUM_BOOST_ROUND = 20000\n",
    "    EARLY_STOP = 200\n",
    "\n",
    "    # ---- Build cost maps (name -> index) ----\n",
    "    cls_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    boost_idx = {cls_to_idx[c] for c in boost_targets if c in cls_to_idx}\n",
    "\n",
    "    # weighted macro-F1 eval that emphasizes the two tough classes\n",
    "    f1_weights = np.array([\n",
    "        (F1_WEIGHT_FOR_BOOST if i in boost_idx else 1.0) for i in range(len(classes))\n",
    "    ], dtype=float)\n",
    "\n",
    "    def weighted_macro_f1_eval(preds: np.ndarray, dmatrix: xgb.DMatrix):\n",
    "        y_true = dmatrix.get_label().astype(int)\n",
    "        proba = preds.reshape(-1, len(classes))\n",
    "        y_hat = np.argmax(proba, axis=1)\n",
    "        per_class_f1 = f1_score(\n",
    "            y_true, y_hat,\n",
    "            labels=np.arange(len(classes)),\n",
    "            average=None,\n",
    "            zero_division=0\n",
    "        )\n",
    "        score = float(np.average(per_class_f1, weights=f1_weights))\n",
    "        return (\"wF1_boost\", score)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof_group = np.zeros((len(Xg), len(classes)), dtype=np.float32)\n",
    "    test_group_pred = np.zeros((len(Xtestg), len(classes)), dtype=np.float32)\n",
    "    fold_best = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(Xg, y_enc_grp), start=1):\n",
    "        print(f\"\\n[{group_name}] Fold {fold}/{N_FOLDS}\")\n",
    "        X_tr, X_va = Xg.iloc[tr_idx], Xg.iloc[va_idx]\n",
    "        y_tr, y_va = y_enc_grp[tr_idx], y_enc_grp[va_idx]\n",
    "\n",
    "        prep = clone(preprocessor)\n",
    "        Xtr = prep.fit_transform(X_tr)\n",
    "        Xva = prep.transform(X_va)\n",
    "\n",
    "        # ---- Cost-sensitive sample weights (deterministic) ----\n",
    "        w_tr = np.ones_like(y_tr, dtype=float)\n",
    "        for idx in boost_idx:\n",
    "            w_tr[y_tr == idx] = base_boost\n",
    "        w_va = np.ones_like(y_va, dtype=float)  # keep validation unweighted\n",
    "\n",
    "        dtrain = xgb.DMatrix(Xtr, label=y_tr, weight=w_tr)\n",
    "        dval = xgb.DMatrix(Xva, label=y_va, weight=w_va)\n",
    "\n",
    "        bst = xgb.train(\n",
    "            params=xgb_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=NUM_BOOST_ROUND,\n",
    "            evals=[(dtrain, \"train\"), (dval, \"valid\")],\n",
    "            feval=weighted_macro_f1_eval,\n",
    "            maximize=True,                 # higher F1 is better\n",
    "            early_stopping_rounds=EARLY_STOP,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        best_round = int(bst.best_iteration + 1)\n",
    "        fold_best.append(best_round)\n",
    "        print(f\"[{group_name}] Best iteration (by wF1_boost): {best_round}\")\n",
    "\n",
    "        # OOF store\n",
    "        oof_proba = bst.predict(dval, iteration_range=(0, best_round))\n",
    "        oof_group[va_idx] = oof_proba\n",
    "\n",
    "        # Quick fold report for the emphasized classes\n",
    "        y_hat = np.argmax(oof_proba, axis=1)\n",
    "        for cname in boost_targets:\n",
    "            if cname in cls_to_idx:\n",
    "                cidx = cls_to_idx[cname]\n",
    "                f1_c = f1_score(y_va, y_hat, labels=[cidx], average=\"macro\", zero_division=0)\n",
    "                print(f\"[{group_name}] Fold {fold} F1({cname}): {f1_c:.4f}\")\n",
    "\n",
    "        # Fold predictions on this group's test slice\n",
    "        Xtest_tf = prep.transform(Xtestg)\n",
    "        dtest = xgb.DMatrix(Xtest_tf)\n",
    "        test_group_pred += bst.predict(dtest, iteration_range=(0, best_round)) / N_FOLDS\n",
    "\n",
    "    # OOF summary for the group\n",
    "    oof_labels = np.argmax(oof_group, axis=1)\n",
    "    acc_g = accuracy_score(y_enc_grp, oof_labels)\n",
    "    f1_g = f1_score(y_enc_grp, oof_labels, average=\"macro\")\n",
    "    print(f\"\\n[{group_name}] OOF Accuracy: {acc_g:.4f} | Macro F1: {f1_g:.4f}\")\n",
    "    print(f\"[{group_name}] Best iterations: {fold_best} | Median: {int(np.median(fold_best))}\")\n",
    "\n",
    "    return oof_group, test_group_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7e96af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MALE] Fold 1/5\n",
      "[MALE] Best iteration (by wF1_boost): 532\n",
      "[MALE] Fold 1 F1(Overweight_Level_I): 0.8019\n",
      "[MALE] Fold 1 F1(Overweight_Level_II): 0.8191\n",
      "\n",
      "[MALE] Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE] Best iteration (by wF1_boost): 280\n",
      "[MALE] Fold 2 F1(Overweight_Level_I): 0.8028\n",
      "[MALE] Fold 2 F1(Overweight_Level_II): 0.8492\n",
      "\n",
      "[MALE] Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE] Best iteration (by wF1_boost): 506\n",
      "[MALE] Fold 3 F1(Overweight_Level_I): 0.8266\n",
      "[MALE] Fold 3 F1(Overweight_Level_II): 0.8510\n",
      "\n",
      "[MALE] Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE] Best iteration (by wF1_boost): 422\n",
      "[MALE] Fold 4 F1(Overweight_Level_I): 0.7857\n",
      "[MALE] Fold 4 F1(Overweight_Level_II): 0.8220\n",
      "\n",
      "[MALE] Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE] Best iteration (by wF1_boost): 522\n",
      "[MALE] Fold 5 F1(Overweight_Level_I): 0.7981\n",
      "[MALE] Fold 5 F1(Overweight_Level_II): 0.8282\n",
      "\n",
      "[MALE] OOF Accuracy: 0.8903 | Macro F1: 0.7534\n",
      "[MALE] Best iterations: [532, 280, 506, 422, 522] | Median: 506\n",
      "\n",
      "[FEMALE] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE] Best iteration (by wF1_boost): 299\n",
      "[FEMALE] Fold 1 F1(Overweight_Level_I): 0.8012\n",
      "[FEMALE] Fold 1 F1(Overweight_Level_II): 0.7895\n",
      "\n",
      "[FEMALE] Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE] Best iteration (by wF1_boost): 230\n",
      "[FEMALE] Fold 2 F1(Overweight_Level_I): 0.7492\n",
      "[FEMALE] Fold 2 F1(Overweight_Level_II): 0.7401\n",
      "\n",
      "[FEMALE] Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE] Best iteration (by wF1_boost): 337\n",
      "[FEMALE] Fold 3 F1(Overweight_Level_I): 0.7803\n",
      "[FEMALE] Fold 3 F1(Overweight_Level_II): 0.7706\n",
      "\n",
      "[FEMALE] Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE] Best iteration (by wF1_boost): 788\n",
      "[FEMALE] Fold 4 F1(Overweight_Level_I): 0.8024\n",
      "[FEMALE] Fold 4 F1(Overweight_Level_II): 0.7249\n",
      "\n",
      "[FEMALE] Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE] Best iteration (by wF1_boost): 380\n",
      "[FEMALE] Fold 5 F1(Overweight_Level_I): 0.7771\n",
      "[FEMALE] Fold 5 F1(Overweight_Level_II): 0.7887\n",
      "\n",
      "[FEMALE] OOF Accuracy: 0.9169 | Macro F1: 0.7510\n",
      "[FEMALE] Best iterations: [299, 230, 337, 788, 380] | Median: 337\n",
      "\n",
      "========== OVERALL OOF ==========\n",
      "OOF Accuracy: 0.9036 | OOF Macro F1: 0.8945\n",
      "\n",
      "OOF Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.92      0.93      0.93      1870\n",
      "      Normal_Weight       0.89      0.87      0.88      2345\n",
      "     Obesity_Type_I       0.91      0.86      0.88      2207\n",
      "    Obesity_Type_II       0.97      0.97      0.97      2403\n",
      "   Obesity_Type_III       1.00      1.00      1.00      2983\n",
      " Overweight_Level_I       0.79      0.80      0.79      1844\n",
      "Overweight_Level_II       0.79      0.84      0.81      1881\n",
      "\n",
      "           accuracy                           0.90     15533\n",
      "          macro avg       0.89      0.89      0.89     15533\n",
      "       weighted avg       0.90      0.90      0.90     15533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Train per-gender and predict full test\n",
    "# ==============================================\n",
    "X_male = X[male_mask].reset_index(drop=True)\n",
    "y_male_enc = y_enc[male_mask]\n",
    "test_male = test_features[test_male_mask].reset_index(drop=True)\n",
    "\n",
    "X_female = X[female_mask].reset_index(drop=True)\n",
    "y_female_enc = y_enc[female_mask]\n",
    "test_female = test_features[test_female_mask].reset_index(drop=True)\n",
    "\n",
    "male_oof, male_test_pred = train_group_and_predict(X_male, y_male_enc, test_male, \"MALE\")\n",
    "female_oof, female_test_pred = train_group_and_predict(X_female, y_female_enc, test_female, \"FEMALE\")\n",
    "\n",
    "# Combine OOF\n",
    "oof_full = np.zeros((len(X), len(classes)), dtype=np.float32)\n",
    "oof_full[male_mask.values] = male_oof\n",
    "oof_full[female_mask.values] = female_oof\n",
    "\n",
    "oof_labels = np.argmax(oof_full, axis=1)\n",
    "oof_acc = accuracy_score(y_enc, oof_labels)\n",
    "oof_f1 = f1_score(y_enc, oof_labels, average=\"macro\")\n",
    "print(\"\\n========== OVERALL OOF ==========\")\n",
    "print(f\"OOF Accuracy: {oof_acc:.4f} | OOF Macro F1: {oof_f1:.4f}\")\n",
    "try:\n",
    "    print(\"\\nOOF Classification Report:\\n\",\n",
    "          classification_report(y_enc, oof_labels, target_names=classes, zero_division=0))\n",
    "except Exception as e:\n",
    "    print(f\"[Info] Could not print classification report: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59af6b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved submission.csv\n",
      "   id       WeightCategory\n",
      "0   0  Overweight_Level_II\n",
      "1   1        Normal_Weight\n",
      "2   2  Insufficient_Weight\n",
      "3   3     Obesity_Type_III\n",
      "4   4  Overweight_Level_II\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build full test predictions (for Kaggle submission use-case)\n",
    "test_pred_proba = np.zeros((len(test_features), len(classes)), dtype=np.float32)\n",
    "test_pred_proba[test_male_mask.values] = male_test_pred\n",
    "test_pred_proba[test_female_mask.values] = female_test_pred\n",
    "\n",
    "test_pred_int = np.argmax(test_pred_proba, axis=1)\n",
    "test_pred_labels = le.inverse_transform(test_pred_int)\n",
    "\n",
    "# Submission\n",
    "ss_cols = list(sample_sub.columns)\n",
    "ID_HEADER = None\n",
    "LABEL_HEADER = None\n",
    "if len(ss_cols) == 2:\n",
    "    c1, c2 = ss_cols\n",
    "    if c1 in test.columns and c2 not in test.columns:\n",
    "        ID_HEADER, LABEL_HEADER = c1, c2\n",
    "    elif c2 in test.columns and c1 not in test.columns:\n",
    "        ID_HEADER, LABEL_HEADER = c2, c1\n",
    "if ID_HEADER is None:\n",
    "    ID_HEADER = ss_cols[0]\n",
    "    LABEL_HEADER = ss_cols[1]\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "if ID_HEADER in test.columns:\n",
    "    sub[ID_HEADER] = test[ID_HEADER].values\n",
    "else:\n",
    "    sub[ID_HEADER] = np.arange(len(test_features))\n",
    "sub[LABEL_HEADER] = test_pred_labels\n",
    "\n",
    "# Ensure column order\n",
    "for c in ss_cols:\n",
    "    if c not in sub.columns:\n",
    "        sub[c] = sample_sub[c].iloc[0] if len(sample_sub[c]) else None\n",
    "sub = sub[ss_cols]\n",
    "\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSaved submission.csv\")\n",
    "print(sub.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a909ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MALE (Kaggle)] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE (Kaggle)] Best iteration (by wF1_boost): 532\n",
      "[MALE (Kaggle)] Fold 1 F1(Overweight_Level_I): 0.8019\n",
      "[MALE (Kaggle)] Fold 1 F1(Overweight_Level_II): 0.8191\n",
      "\n",
      "[MALE (Kaggle)] Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE (Kaggle)] Best iteration (by wF1_boost): 280\n",
      "[MALE (Kaggle)] Fold 2 F1(Overweight_Level_I): 0.8028\n",
      "[MALE (Kaggle)] Fold 2 F1(Overweight_Level_II): 0.8492\n",
      "\n",
      "[MALE (Kaggle)] Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE (Kaggle)] Best iteration (by wF1_boost): 506\n",
      "[MALE (Kaggle)] Fold 3 F1(Overweight_Level_I): 0.8266\n",
      "[MALE (Kaggle)] Fold 3 F1(Overweight_Level_II): 0.8510\n",
      "\n",
      "[MALE (Kaggle)] Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE (Kaggle)] Best iteration (by wF1_boost): 422\n",
      "[MALE (Kaggle)] Fold 4 F1(Overweight_Level_I): 0.7857\n",
      "[MALE (Kaggle)] Fold 4 F1(Overweight_Level_II): 0.8220\n",
      "\n",
      "[MALE (Kaggle)] Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE (Kaggle)] Best iteration (by wF1_boost): 522\n",
      "[MALE (Kaggle)] Fold 5 F1(Overweight_Level_I): 0.7981\n",
      "[MALE (Kaggle)] Fold 5 F1(Overweight_Level_II): 0.8282\n",
      "\n",
      "[MALE (Kaggle)] OOF Accuracy: 0.8903 | Macro F1: 0.7534\n",
      "[MALE (Kaggle)] Best iterations: [532, 280, 506, 422, 522] | Median: 506\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE (Kaggle)] Best iteration (by wF1_boost): 299\n",
      "[FEMALE (Kaggle)] Fold 1 F1(Overweight_Level_I): 0.8012\n",
      "[FEMALE (Kaggle)] Fold 1 F1(Overweight_Level_II): 0.7895\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE (Kaggle)] Best iteration (by wF1_boost): 230\n",
      "[FEMALE (Kaggle)] Fold 2 F1(Overweight_Level_I): 0.7492\n",
      "[FEMALE (Kaggle)] Fold 2 F1(Overweight_Level_II): 0.7401\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE (Kaggle)] Best iteration (by wF1_boost): 337\n",
      "[FEMALE (Kaggle)] Fold 3 F1(Overweight_Level_I): 0.7803\n",
      "[FEMALE (Kaggle)] Fold 3 F1(Overweight_Level_II): 0.7706\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE (Kaggle)] Best iteration (by wF1_boost): 788\n",
      "[FEMALE (Kaggle)] Fold 4 F1(Overweight_Level_I): 0.8024\n",
      "[FEMALE (Kaggle)] Fold 4 F1(Overweight_Level_II): 0.7249\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE (Kaggle)] Best iteration (by wF1_boost): 380\n",
      "[FEMALE (Kaggle)] Fold 5 F1(Overweight_Level_I): 0.7771\n",
      "[FEMALE (Kaggle)] Fold 5 F1(Overweight_Level_II): 0.7887\n",
      "\n",
      "[FEMALE (Kaggle)] OOF Accuracy: 0.9169 | Macro F1: 0.7510\n",
      "[FEMALE (Kaggle)] Best iterations: [299, 230, 337, 788, 380] | Median: 337\n",
      "\n",
      "âœ… Overall Accuracy on Kaggle_test: 0.90813\n",
      "\n",
      "=== Confusion Matrix (counts) ===\n",
      "Predicted â†’\n",
      "True â†“\n",
      "Insufficient_Weight   :  622 |   28 |    3 |    0 |    0 |    0 |    0\n",
      "Normal_Weight         :   41 |  643 |   44 |    8 |    1 |    0 |    0\n",
      "Overweight_Level_I    :    4 |   44 |  454 |   72 |    9 |    0 |    0\n",
      "Overweight_Level_II   :    0 |   15 |   54 |  526 |   42 |    4 |    0\n",
      "Obesity_Type_I        :    1 |    1 |   11 |   50 |  622 |   16 |    2\n",
      "Obesity_Type_II       :    0 |    0 |    1 |    6 |   21 |  817 |    0\n",
      "Obesity_Type_III      :    0 |    0 |    1 |    0 |    1 |    0 | 1061\n",
      "\n",
      "=== Confusion Matrix (row-normalized) ===\n",
      "Insufficient_Weight   : 0.95 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00\n",
      "Normal_Weight         : 0.06 | 0.87 | 0.06 | 0.01 | 0.00 | 0.00 | 0.00\n",
      "Overweight_Level_I    : 0.01 | 0.08 | 0.78 | 0.12 | 0.02 | 0.00 | 0.00\n",
      "Overweight_Level_II   : 0.00 | 0.02 | 0.08 | 0.82 | 0.07 | 0.01 | 0.00\n",
      "Obesity_Type_I        : 0.00 | 0.00 | 0.02 | 0.07 | 0.88 | 0.02 | 0.00\n",
      "Obesity_Type_II       : 0.00 | 0.00 | 0.00 | 0.01 | 0.02 | 0.97 | 0.00\n",
      "Obesity_Type_III      : 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00\n",
      "\n",
      "=== Per-class metrics ===\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight     0.9311    0.9525    0.9417       653\n",
      "      Normal_Weight     0.8796    0.8725    0.8760       737\n",
      " Overweight_Level_I     0.7993    0.7787    0.7889       583\n",
      "Overweight_Level_II     0.7946    0.8206    0.8074       641\n",
      "     Obesity_Type_I     0.8937    0.8848    0.8892       703\n",
      "    Obesity_Type_II     0.9761    0.9669    0.9715       845\n",
      "   Obesity_Type_III     0.9981    0.9981    0.9981      1063\n",
      "\n",
      "           accuracy                         0.9081      5225\n",
      "          macro avg     0.8961    0.8963    0.8961      5225\n",
      "       weighted avg     0.9083    0.9081    0.9081      5225\n",
      "\n",
      "\n",
      "=== Per-class accuracy (diagonal/row total) ===\n",
      "Insufficient_Weight    | Correct: 622 / 653 |  95.25%\n",
      "Normal_Weight          | Correct: 643 / 737 |  87.25%\n",
      "Overweight_Level_I     | Correct: 454 / 583 |  77.87%\n",
      "Overweight_Level_II    | Correct: 526 / 641 |  82.06%\n",
      "Obesity_Type_I         | Correct: 622 / 703 |  88.48%\n",
      "Obesity_Type_II        | Correct: 817 / 845 |  96.69%\n",
      "Obesity_Type_III       | Correct: 1061 / 1063 |  99.81%\n",
      "\n",
      "=== Most common confusions (true â†’ predicted) ===\n",
      "Overweight_Level_I        â†’ Overweight_Level_II       | Count:  72 | Row%:  12.3\n",
      "Overweight_Level_II       â†’ Overweight_Level_I        | Count:  54 | Row%:   8.4\n",
      "Obesity_Type_I            â†’ Overweight_Level_II       | Count:  50 | Row%:   7.1\n",
      "Overweight_Level_I        â†’ Normal_Weight             | Count:  44 | Row%:   7.5\n",
      "Normal_Weight             â†’ Overweight_Level_I        | Count:  44 | Row%:   6.0\n",
      "Overweight_Level_II       â†’ Obesity_Type_I            | Count:  42 | Row%:   6.6\n",
      "Normal_Weight             â†’ Insufficient_Weight       | Count:  41 | Row%:   5.6\n",
      "Insufficient_Weight       â†’ Normal_Weight             | Count:  28 | Row%:   4.3\n",
      "Obesity_Type_II           â†’ Obesity_Type_I            | Count:  21 | Row%:   2.5\n",
      "Obesity_Type_I            â†’ Obesity_Type_II           | Count:  16 | Row%:   2.3\n",
      "\n",
      "=== Sample of misclassified rows (first 10) ===\n",
      "Row    9: true=Overweight_Level_II    pred=Obesity_Type_I         conf=0.910 2nd=Overweight_Level_II   (0.053)\n",
      "Row   16: true=Overweight_Level_I     pred=Normal_Weight          conf=0.924 2nd=Overweight_Level_I    (0.067)\n",
      "Row   28: true=Normal_Weight          pred=Insufficient_Weight    conf=0.883 2nd=Normal_Weight         (0.111)\n",
      "Row   30: true=Overweight_Level_I     pred=Overweight_Level_II    conf=0.854 2nd=Overweight_Level_I    (0.118)\n",
      "Row   33: true=Overweight_Level_I     pred=Normal_Weight          conf=0.955 2nd=Overweight_Level_I    (0.035)\n",
      "Row   43: true=Obesity_Type_II        pred=Obesity_Type_I         conf=0.615 2nd=Obesity_Type_II       (0.375)\n",
      "Row   61: true=Obesity_Type_I         pred=Overweight_Level_II    conf=0.887 2nd=Obesity_Type_I        (0.092)\n",
      "Row   65: true=Overweight_Level_I     pred=Overweight_Level_II    conf=0.823 2nd=Obesity_Type_I        (0.112)\n",
      "Row   68: true=Overweight_Level_I     pred=Normal_Weight          conf=0.556 2nd=Overweight_Level_I    (0.437)\n",
      "Row   75: true=Overweight_Level_I     pred=Normal_Weight          conf=0.908 2nd=Overweight_Level_I    (0.089)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================\n",
    "# Evaluate on Kaggle_test.csv (with ground truth)\n",
    "# ==============================================\n",
    "if not os.path.exists(KAGGLE_TEST_PATH):\n",
    "    print(f\"\\n[Warn] {KAGGLE_TEST_PATH} not found. Skipping Kaggle_test evaluation.\")\n",
    "else:\n",
    "    kdf = pd.read_csv(KAGGLE_TEST_PATH)\n",
    "    if \"WeightCategory\" not in kdf.columns:\n",
    "        raise KeyError(\"Kaggle_test.csv must contain 'WeightCategory'.\")\n",
    "\n",
    "    y_true = kdf[\"WeightCategory\"].copy()\n",
    "    X_k = kdf.drop(columns=[\"WeightCategory\"], errors=\"ignore\").copy()\n",
    "    if id_col and id_col in X_k.columns:\n",
    "        X_k.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "    # same drops + BMI\n",
    "    for c in [\"MTRANS\", \"SMOKE\"]:\n",
    "        if c in X_k.columns:\n",
    "            X_k.drop(columns=[c], inplace=True)\n",
    "    X_k = add_bmi(X_k)\n",
    "\n",
    "    # detect gender and split for Kaggle set\n",
    "    gender_col_k = detect_gender_column(X_k)\n",
    "    if gender_col_k is None:\n",
    "        raise ValueError(\"Could not detect a gender column in Kaggle_test.csv\")\n",
    "    km_k, kf_k = split_by_gender(X_k[gender_col_k])\n",
    "\n",
    "    # Predict on Kaggle by reusing the same training procedure (per gender)\n",
    "    kaggle_pred_proba = np.zeros((len(X_k), len(classes)), dtype=np.float32)\n",
    "\n",
    "    if X_male.shape[0] > 0 and km_k.sum() > 0:\n",
    "        _, male_k_pred = train_group_and_predict(X_male, y_male_enc, X_k[km_k].reset_index(drop=True), \"MALE (Kaggle)\")\n",
    "        kaggle_pred_proba[km_k.values] = male_k_pred\n",
    "    if X_female.shape[0] > 0 and kf_k.sum() > 0:\n",
    "        _, female_k_pred = train_group_and_predict(X_female, y_female_enc, X_k[kf_k].reset_index(drop=True), \"FEMALE (Kaggle)\")\n",
    "        kaggle_pred_proba[kf_k.values] = female_k_pred\n",
    "\n",
    "    kaggle_pred_idx = np.argmax(kaggle_pred_proba, axis=1)\n",
    "    y_pred = le.inverse_transform(kaggle_pred_idx)\n",
    "\n",
    "    # -------- Overall accuracy to 5 decimals --------\n",
    "    overall_acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"\\nâœ… Overall Accuracy on Kaggle_test: {overall_acc:.5f}\")\n",
    "\n",
    "    # -------- Text-only error analysis (custom order) --------\n",
    "    order = [\n",
    "        'Insufficient_Weight',\n",
    "        'Normal_Weight',\n",
    "        'Overweight_Level_I',\n",
    "        'Overweight_Level_II',\n",
    "        'Obesity_Type_I',\n",
    "        'Obesity_Type_II',\n",
    "        'Obesity_Type_III'\n",
    "    ]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=order)\n",
    "    cm_norm = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    print(\"\\n=== Confusion Matrix (counts) ===\")\n",
    "    print(\"Predicted â†’\")\n",
    "    print(\"True â†“\")\n",
    "    for i, true_class in enumerate(order):\n",
    "        row = \" | \".join(f\"{cm[i, j]:4d}\" for j in range(len(order)))\n",
    "        print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "    print(\"\\n=== Confusion Matrix (row-normalized) ===\")\n",
    "    for i, true_class in enumerate(order):\n",
    "        row = \" | \".join(f\"{cm_norm[i, j]:.2f}\" for j in range(len(order)))\n",
    "        print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "    print(\"\\n=== Per-class metrics ===\")\n",
    "    try:\n",
    "        print(classification_report(y_true, y_pred, labels=order, target_names=order, digits=4, zero_division=0))\n",
    "    except Exception as e:\n",
    "        print(f\"[Info] classification_report fallback: {e}\")\n",
    "        print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "    print(\"\\n=== Per-class accuracy (diagonal/row total) ===\")\n",
    "    for i, c in enumerate(order):\n",
    "        total = cm[i].sum()\n",
    "        correct = cm[i, i]\n",
    "        acc = correct / total if total > 0 else 0.0\n",
    "        print(f\"{c:<22} | Correct: {correct:3d} / {total:3d} | {acc*100:6.2f}%\")\n",
    "\n",
    "    print(\"\\n=== Most common confusions (true â†’ predicted) ===\")\n",
    "    pairs = []\n",
    "    for i, t in enumerate(order):\n",
    "        for j, p in enumerate(order):\n",
    "            if i == j or cm[i, j] == 0:\n",
    "                continue\n",
    "            pairs.append((cm[i, j], t, p, cm_norm[i, j]))\n",
    "    pairs = sorted(pairs, key=lambda x: (-x[0], -x[3]))\n",
    "    for cnt, true_label, pred_label, norm_val in pairs[:10]:\n",
    "        print(f\"{true_label:25} â†’ {pred_label:25} | Count: {cnt:3d} | Row%: {norm_val*100:5.1f}\")\n",
    "    \n",
    "    print(\"\\n=== Sample of misclassified rows (first 10) ===\")\n",
    "    mis_idx = np.where(np.asarray(y_true) != np.asarray(y_pred))[0]\n",
    "    if len(mis_idx) == 0:\n",
    "        print(\"ðŸŽ‰ No misclassifications!\")\n",
    "    else:\n",
    "        for idx in mis_idx[:10]:\n",
    "            true_lab = y_true.iloc[idx] if hasattr(y_true, \"iloc\") else y_true[idx]\n",
    "            pred_lab = y_pred[idx]\n",
    "            conf = float(np.max(kaggle_pred_proba[idx]))\n",
    "            rank = np.argsort(-kaggle_pred_proba[idx])\n",
    "            second_idx = rank[1] if rank.size > 1 else rank[0]\n",
    "            second_lab = le.inverse_transform([second_idx])[0]\n",
    "            second_conf = float(kaggle_pred_proba[idx][second_idx])\n",
    "            print(f\"Row {idx:4d}: true={true_lab:<22} pred={pred_lab:<22} conf={conf:.3f} 2nd={second_lab:<22}({second_conf:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a99e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b30559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d46a14b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Classes: ['Insufficient_Weight', 'Normal_Weight', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III', 'Overweight_Level_I', 'Overweight_Level_II']\n",
      "[Info] Train male=7783, female=7750\n",
      "[Info] Test  male=10336, female=10422\n",
      "\n",
      "[MALE] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE] Best iteration (by wF1_boost): 487\n",
      "[MALE] Fold 1 F1(Overweight_Level_I): 0.8117\n",
      "[MALE] Fold 1 F1(Overweight_Level_II): 0.8324\n",
      "\n",
      "[MALE] Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE] Best iteration (by wF1_boost): 513\n",
      "[MALE] Fold 2 F1(Overweight_Level_I): 0.8224\n",
      "[MALE] Fold 2 F1(Overweight_Level_II): 0.8297\n",
      "\n",
      "[MALE] Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE] Best iteration (by wF1_boost): 232\n",
      "[MALE] Fold 3 F1(Overweight_Level_I): 0.7700\n",
      "[MALE] Fold 3 F1(Overweight_Level_II): 0.8244\n",
      "\n",
      "[MALE] Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE] Best iteration (by wF1_boost): 422\n",
      "[MALE] Fold 4 F1(Overweight_Level_I): 0.7982\n",
      "[MALE] Fold 4 F1(Overweight_Level_II): 0.8294\n",
      "\n",
      "[MALE] Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE] Best iteration (by wF1_boost): 234\n",
      "[MALE] Fold 5 F1(Overweight_Level_I): 0.7692\n",
      "[MALE] Fold 5 F1(Overweight_Level_II): 0.8195\n",
      "\n",
      "[MALE] OOF Accuracy: 0.8874 | Macro F1: 0.7514\n",
      "[MALE] Best iterations: [487, 513, 232, 422, 234] | Median: 422\n",
      "\n",
      "[FEMALE] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE] Best iteration (by wF1_boost): 67\n",
      "[FEMALE] Fold 1 F1(Overweight_Level_I): 0.7937\n",
      "[FEMALE] Fold 1 F1(Overweight_Level_II): 0.7542\n",
      "\n",
      "[FEMALE] Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE] Best iteration (by wF1_boost): 96\n",
      "[FEMALE] Fold 2 F1(Overweight_Level_I): 0.7674\n",
      "[FEMALE] Fold 2 F1(Overweight_Level_II): 0.7431\n",
      "\n",
      "[FEMALE] Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE] Best iteration (by wF1_boost): 643\n",
      "[FEMALE] Fold 3 F1(Overweight_Level_I): 0.7601\n",
      "[FEMALE] Fold 3 F1(Overweight_Level_II): 0.7196\n",
      "\n",
      "[FEMALE] Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE] Best iteration (by wF1_boost): 309\n",
      "[FEMALE] Fold 4 F1(Overweight_Level_I): 0.7673\n",
      "[FEMALE] Fold 4 F1(Overweight_Level_II): 0.7773\n",
      "\n",
      "[FEMALE] Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE] Best iteration (by wF1_boost): 217\n",
      "[FEMALE] Fold 5 F1(Overweight_Level_I): 0.8092\n",
      "[FEMALE] Fold 5 F1(Overweight_Level_II): 0.7966\n",
      "\n",
      "[FEMALE] OOF Accuracy: 0.9166 | Macro F1: 0.7504\n",
      "[FEMALE] Best iterations: [67, 96, 643, 309, 217] | Median: 217\n",
      "\n",
      "========== OVERALL OOF ==========\n",
      "OOF Accuracy: 0.9020 | OOF Macro F1: 0.8929\n",
      "\n",
      "OOF Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.93      0.93      0.93      1870\n",
      "      Normal_Weight       0.89      0.87      0.88      2345\n",
      "     Obesity_Type_I       0.90      0.86      0.88      2207\n",
      "    Obesity_Type_II       0.97      0.97      0.97      2403\n",
      "   Obesity_Type_III       1.00      1.00      1.00      2983\n",
      " Overweight_Level_I       0.79      0.79      0.79      1844\n",
      "Overweight_Level_II       0.78      0.84      0.81      1881\n",
      "\n",
      "           accuracy                           0.90     15533\n",
      "          macro avg       0.89      0.89      0.89     15533\n",
      "       weighted avg       0.90      0.90      0.90     15533\n",
      "\n",
      "\n",
      "Saved submission.csv\n",
      "   id       WeightCategory\n",
      "0   0  Overweight_Level_II\n",
      "1   1        Normal_Weight\n",
      "2   2  Insufficient_Weight\n",
      "3   3     Obesity_Type_III\n",
      "4   4  Overweight_Level_II\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Gender-specific XGB + BMI + targeted class boost\n",
    "# + Pairwise Expert (Overweight I vs II) re-ranking\n",
    "# + Kaggle_test evaluation & confusion analysis\n",
    "# ==============================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.base import clone\n",
    "import xgboost as xgb\n",
    "\n",
    "# -------- Paths --------\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "SAMPLE_SUB_PATH = \"sample_submission.csv\"\n",
    "KAGGLE_TEST_PATH = \"Kaggle_test.csv\"  # must contain WeightCategory\n",
    "\n",
    "# -------- Globals --------\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "N_JOBS = -1\n",
    "\n",
    "# ---- Cost-sensitive knobs ----\n",
    "BOOST_CLASSES = (\"Overweight_Level_I\", \"Overweight_Level_II\")\n",
    "TRAIN_WEIGHT_MULT = 2.0        # weight multiplier for emphasized classes\n",
    "F1_WEIGHT_FOR_BOOST = 1.75     # evaluation-time weight for emphasized classes\n",
    "\n",
    "# ---- Pairwise Overweight I vs II expert ----\n",
    "USE_PAIRWISE_EXPERT = True\n",
    "PAIR_CLASSES = (\"Overweight_Level_I\", \"Overweight_Level_II\")\n",
    "TAU_MARGIN = 0.08   # route to expert if |p1 - p2| < TAU (try 0.05â€“0.12)\n",
    "\n",
    "# ---- Multiclass XGB defaults ----\n",
    "NUM_BOOST_ROUND = 20000\n",
    "EARLY_STOP = 200\n",
    "\n",
    "# ==============================================\n",
    "# Helpers\n",
    "# ==============================================\n",
    "def norm_col(s: str) -> str:\n",
    "    if s is None:\n",
    "        return s\n",
    "    return str(s).replace(\"\\ufeff\", \"\").strip().lower()\n",
    "\n",
    "def infer_feature_types(df):\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def detect_gender_column(df):\n",
    "    for c in df.columns:\n",
    "        if norm_col(c) in {\"gender\", \"sex\"}:\n",
    "            return c\n",
    "    # fallback: detect M/F-ish column\n",
    "    for c in df.columns:\n",
    "        vals = pd.Series(df[c].dropna().astype(str).str.lower().str.strip()).unique()\n",
    "        if len(vals) in (2, 3):\n",
    "            if any(v.startswith(\"m\") for v in vals) and any(v.startswith(\"f\") for v in vals):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def split_by_gender(series):\n",
    "    s = series.astype(str).str.lower().str.strip()\n",
    "    male_mask = s.str.startswith((\"m\", \"1\", \"true\"))\n",
    "    female_mask = s.str.startswith((\"f\", \"0\", \"false\"))\n",
    "    if male_mask.sum() == 0 and female_mask.sum() == 0:\n",
    "        top = s.value_counts().index.tolist()\n",
    "        if len(top) >= 2:\n",
    "            male_mask = s == top[0]\n",
    "            female_mask = s == top[1]\n",
    "    return male_mask, female_mask\n",
    "\n",
    "def add_bmi(df):\n",
    "    \"\"\"BMI = Weight / (Height_m^2). If median height > 3 assume cm -> meters.\"\"\"\n",
    "    if (\"Weight\" in df.columns) and (\"Height\" in df.columns):\n",
    "        h = pd.to_numeric(df[\"Height\"], errors=\"coerce\")\n",
    "        height_m = np.where(np.nanmedian(h) > 3.0, h / 100.0, h)\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            bmi = pd.to_numeric(df[\"Weight\"], errors=\"coerce\") / (np.power(height_m, 2) + 1e-12)\n",
    "        df[\"BMI\"] = pd.Series(bmi).replace([np.inf, -np.inf], np.nan)\n",
    "    return df\n",
    "\n",
    "# ==============================================\n",
    "# Load data\n",
    "# ==============================================\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "# Optional drops\n",
    "for c in [\"MTRANS\", \"SMOKE\"]:\n",
    "    if c in train.columns:\n",
    "        train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:\n",
    "        test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "train = add_bmi(train)\n",
    "test = add_bmi(test)\n",
    "\n",
    "# Detect ID/Target\n",
    "id_col = None\n",
    "for cand in [\"id\", \"row_id\", \"index\", \"sample_id\"]:\n",
    "    if cand in train.columns and cand in test.columns:\n",
    "        id_col = cand\n",
    "        break\n",
    "\n",
    "target_col = None\n",
    "for cand in [\"WeightCategory\", \"NObeyesdad\", \"label\", \"target\", \"class\", \"y\"]:\n",
    "    if cand in train.columns:\n",
    "        target_col = cand\n",
    "        break\n",
    "if target_col is None:\n",
    "    raise ValueError(\"Could not detect target column in train.csv\")\n",
    "\n",
    "# Build X/y\n",
    "y = train[target_col].copy()\n",
    "X = train.drop(columns=[target_col]).copy()\n",
    "if id_col and id_col in X.columns:\n",
    "    X.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "test_features = test.copy()\n",
    "if id_col and id_col in test_features.columns:\n",
    "    test_ids = test_features[id_col].copy()\n",
    "    test_features.drop(columns=[id_col], inplace=True)\n",
    "else:\n",
    "    test_ids = pd.Series(np.arange(len(test_features)), name=\"id\")\n",
    "\n",
    "# Label encode target\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "classes = list(le.classes_)\n",
    "print(f\"[Info] Classes: {classes}\")\n",
    "\n",
    "# Detect gender and split\n",
    "gender_col = detect_gender_column(pd.concat([X, test_features], axis=0))\n",
    "if gender_col is None:\n",
    "    raise ValueError(\"Could not detect a gender column (e.g., 'Gender'/'SEX').\")\n",
    "male_mask, female_mask = split_by_gender(train[gender_col])\n",
    "test_male_mask, test_female_mask = split_by_gender(test_features[gender_col])\n",
    "print(f\"[Info] Train male={int(male_mask.sum())}, female={int(female_mask.sum())}\")\n",
    "print(f\"[Info] Test  male={int(test_male_mask.sum())}, female={int(test_female_mask.sum())}\")\n",
    "\n",
    "# ==============================================\n",
    "# Training function (gender-specific)\n",
    "# - Cost-sensitive training\n",
    "# - Weighted-F1 early stopping emphasizing BOOST_CLASSES\n",
    "# - Pairwise expert for Overweight I vs II (OOF + Test re-ranking)\n",
    "# ==============================================\n",
    "def train_group_and_predict(\n",
    "    X_grp,\n",
    "    y_enc_grp,\n",
    "    test_grp,\n",
    "    group_name,\n",
    "    boost_targets=BOOST_CLASSES,\n",
    "    base_boost=TRAIN_WEIGHT_MULT,\n",
    "):\n",
    "    # Drop gender column inside a group (constant after split)\n",
    "    cols_to_use = [c for c in X_grp.columns if c != gender_col]\n",
    "    Xg = X_grp[cols_to_use].copy()\n",
    "    Xtestg = test_grp[cols_to_use].copy()\n",
    "\n",
    "    num_cols, cat_cols = infer_feature_types(Xg)\n",
    "\n",
    "    # Preprocessor\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", ohe)\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=1.0\n",
    "    )\n",
    "\n",
    "    # XGB (multiclass) params\n",
    "    xgb_params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": len(classes),\n",
    "        \"eval_metric\": \"mlogloss\",  # logloss for logging; early stopping uses feval\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"max_depth\": 6,\n",
    "        \"min_child_weight\": 2,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"lambda\": 1.0,\n",
    "        \"alpha\": 0.0,\n",
    "        \"eta\": 0.03,\n",
    "        \"nthread\": N_JOBS,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "    }\n",
    "\n",
    "    # ---- indices\n",
    "    cls_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    boost_idx = {cls_to_idx[c] for c in boost_targets if c in cls_to_idx}\n",
    "    idx_I = cls_to_idx.get(PAIR_CLASSES[0], None)\n",
    "    idx_II = cls_to_idx.get(PAIR_CLASSES[1], None)\n",
    "\n",
    "    # weighted macro-F1 early stopping emphasizing boost classes\n",
    "    f1_weights = np.array([(F1_WEIGHT_FOR_BOOST if i in boost_idx else 1.0)\n",
    "                           for i in range(len(classes))], dtype=float)\n",
    "\n",
    "    def weighted_macro_f1_eval(preds, dmatrix):\n",
    "        y_true = dmatrix.get_label().astype(int)\n",
    "        proba = preds.reshape(-1, len(classes))\n",
    "        y_hat = np.argmax(proba, axis=1)\n",
    "        per_class_f1 = f1_score(y_true, y_hat,\n",
    "                                labels=np.arange(len(classes)),\n",
    "                                average=None, zero_division=0)\n",
    "        return (\"wF1_boost\", float(np.average(per_class_f1, weights=f1_weights)))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof_group = np.zeros((len(Xg), len(classes)), dtype=np.float32)\n",
    "    test_group_pred = np.zeros((len(Xtestg), len(classes)), dtype=np.float32)\n",
    "    fold_best = []\n",
    "\n",
    "    # -------- TRAIN + OOF + Test accumulation per fold\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(Xg, y_enc_grp), start=1):\n",
    "        print(f\"\\n[{group_name}] Fold {fold}/{N_FOLDS}\")\n",
    "        X_tr, X_va = Xg.iloc[tr_idx], Xg.iloc[va_idx]\n",
    "        y_tr, y_va = y_enc_grp[tr_idx], y_enc_grp[va_idx]\n",
    "\n",
    "        prep = clone(preprocessor)\n",
    "        Xtr = prep.fit_transform(X_tr)\n",
    "        Xva = prep.transform(X_va)\n",
    "        Xte = prep.transform(Xtestg)\n",
    "\n",
    "        # cost-sensitive sample weights\n",
    "        w_tr = np.ones_like(y_tr, dtype=float)\n",
    "        for idx in boost_idx:\n",
    "            w_tr[y_tr == idx] = base_boost\n",
    "        w_va = np.ones_like(y_va, dtype=float)\n",
    "\n",
    "        dtrain = xgb.DMatrix(Xtr, label=y_tr, weight=w_tr)\n",
    "        dval   = xgb.DMatrix(Xva, label=y_va, weight=w_va)\n",
    "        dtest  = xgb.DMatrix(Xte)\n",
    "\n",
    "        bst = xgb.train(\n",
    "            params=xgb_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=NUM_BOOST_ROUND,\n",
    "            evals=[(dtrain, \"train\"), (dval, \"valid\")],\n",
    "            feval=weighted_macro_f1_eval,\n",
    "            maximize=True,\n",
    "            early_stopping_rounds=EARLY_STOP,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        best_round = int(bst.best_iteration + 1)\n",
    "        fold_best.append(best_round)\n",
    "        print(f\"[{group_name}] Best iteration (by wF1_boost): {best_round}\")\n",
    "\n",
    "        # OOF proba from main model\n",
    "        oof_proba = bst.predict(dval, iteration_range=(0, best_round))\n",
    "\n",
    "        # ---- Train pairwise expert on this fold (y âˆˆ {I, II})\n",
    "        pair_bst = None\n",
    "        if USE_PAIRWISE_EXPERT and idx_I is not None and idx_II is not None:\n",
    "            mask_pair_tr = np.isin(y_tr, [idx_I, idx_II])\n",
    "            if mask_pair_tr.sum() >= 20:  # enough samples\n",
    "                y_tr_bin = (y_tr[mask_pair_tr] == idx_II).astype(int)  # 1=II, 0=I\n",
    "                Xtr_pair = Xtr[mask_pair_tr]\n",
    "                dtr_pair = xgb.DMatrix(Xtr_pair, label=y_tr_bin)\n",
    "                pair_bst = xgb.train(\n",
    "                    params={\n",
    "                        \"objective\": \"binary:logistic\",\n",
    "                        \"eval_metric\": \"logloss\",\n",
    "                        \"tree_method\": \"hist\",\n",
    "                        \"max_depth\": 5,\n",
    "                        \"min_child_weight\": 2,\n",
    "                        \"subsample\": 0.9,\n",
    "                        \"colsample_bytree\": 0.9,\n",
    "                        \"eta\": 0.03,\n",
    "                        \"nthread\": N_JOBS,\n",
    "                        \"seed\": RANDOM_STATE + 137 * fold,\n",
    "                    },\n",
    "                    dtrain=dtr_pair,\n",
    "                    num_boost_round=4000,\n",
    "                    verbose_eval=False\n",
    "                )\n",
    "\n",
    "        # ---- OOF re-ranking with expert when top-2 are I & II and margin is small\n",
    "        if USE_PAIRWISE_EXPERT and pair_bst is not None:\n",
    "            top2 = np.argsort(-oof_proba, axis=1)[:, :2]\n",
    "            margins = oof_proba[np.arange(len(oof_proba)), top2[:, 0]] - \\\n",
    "                      oof_proba[np.arange(len(oof_proba)), top2[:, 1]]\n",
    "            mask_candidates = np.logical_and(\n",
    "                np.isin(top2, [idx_I, idx_II]).sum(axis=1) == 2,\n",
    "                margins < TAU_MARGIN\n",
    "            )\n",
    "            if mask_candidates.any():\n",
    "                dval_pair = xgb.DMatrix(Xva[mask_candidates])\n",
    "                pII = pair_bst.predict(dval_pair)  # prob of class II\n",
    "                pI = 1.0 - pII\n",
    "                rows = np.where(mask_candidates)[0]\n",
    "                oof_proba[rows, idx_I] = pI\n",
    "                oof_proba[rows, idx_II] = pII\n",
    "                row_sum = oof_proba[rows].sum(axis=1, keepdims=True)\n",
    "                oof_proba[rows] /= (row_sum + 1e-12)\n",
    "\n",
    "        # store OOF\n",
    "        oof_group[va_idx] = oof_proba\n",
    "\n",
    "        # quick per-fold check on emphasized classes\n",
    "        y_hat = np.argmax(oof_proba, axis=1)\n",
    "        for cname in boost_targets:\n",
    "            if cname in cls_to_idx:\n",
    "                cidx = cls_to_idx[cname]\n",
    "                f1_c = f1_score(y_va, y_hat, labels=[cidx], average=\"macro\", zero_division=0)\n",
    "                print(f\"[{group_name}] Fold {fold} F1({cname}): {f1_c:.4f}\")\n",
    "\n",
    "        # ---- Test predictions for this fold + expert re-ranking\n",
    "        proba_fold = bst.predict(dtest, iteration_range=(0, best_round))\n",
    "        if USE_PAIRWISE_EXPERT and pair_bst is not None and idx_I is not None and idx_II is not None:\n",
    "            top2 = np.argsort(-proba_fold, axis=1)[:, :2]\n",
    "            margins = proba_fold[np.arange(len(proba_fold)), top2[:, 0]] - \\\n",
    "                      proba_fold[np.arange(len(proba_fold)), top2[:, 1]]\n",
    "            mask_candidates = np.logical_and(\n",
    "                np.isin(top2, [idx_I, idx_II]).sum(axis=1) == 2,\n",
    "                margins < TAU_MARGIN\n",
    "            )\n",
    "            if mask_candidates.any():\n",
    "                dtest_pair = xgb.DMatrix(Xte[mask_candidates])\n",
    "                pII = pair_bst.predict(dtest_pair)\n",
    "                pI = 1.0 - pII\n",
    "                rows = np.where(mask_candidates)[0]\n",
    "                proba_fold[rows, idx_I] = pI\n",
    "                proba_fold[rows, idx_II] = pII\n",
    "                row_sum = proba_fold[rows].sum(axis=1, keepdims=True)\n",
    "                proba_fold[rows] /= (row_sum + 1e-12)\n",
    "\n",
    "        test_group_pred += proba_fold / N_FOLDS\n",
    "\n",
    "    # ---- OOF summary\n",
    "    oof_labels = np.argmax(oof_group, axis=1)\n",
    "    acc_g = accuracy_score(y_enc_grp, oof_labels)\n",
    "    f1_g = f1_score(y_enc_grp, oof_labels, average=\"macro\")\n",
    "    print(f\"\\n[{group_name}] OOF Accuracy: {acc_g:.4f} | Macro F1: {f1_g:.4f}\")\n",
    "    print(f\"[{group_name}] Best iterations: {list(map(int, fold_best))} | Median: {int(np.median(fold_best))}\")\n",
    "\n",
    "    return oof_group, test_group_pred\n",
    "\n",
    "# ==============================================\n",
    "# Train per-gender and predict full test\n",
    "# ==============================================\n",
    "X_male = X[male_mask].reset_index(drop=True)\n",
    "y_male_enc = y_enc[male_mask]\n",
    "test_male = test_features[test_male_mask].reset_index(drop=True)\n",
    "\n",
    "X_female = X[female_mask].reset_index(drop=True)\n",
    "y_female_enc = y_enc[female_mask]\n",
    "test_female = test_features[test_female_mask].reset_index(drop=True)\n",
    "\n",
    "male_oof, male_test_pred = train_group_and_predict(X_male, y_male_enc, test_male, \"MALE\")\n",
    "female_oof, female_test_pred = train_group_and_predict(X_female, y_female_enc, test_female, \"FEMALE\")\n",
    "\n",
    "# Combine OOF\n",
    "oof_full = np.zeros((len(X), len(classes)), dtype=np.float32)\n",
    "oof_full[male_mask.values] = male_oof\n",
    "oof_full[female_mask.values] = female_oof\n",
    "\n",
    "oof_labels = np.argmax(oof_full, axis=1)\n",
    "oof_acc = accuracy_score(y_enc, oof_labels)\n",
    "oof_f1 = f1_score(y_enc, oof_labels, average=\"macro\")\n",
    "print(\"\\n========== OVERALL OOF ==========\")\n",
    "print(f\"OOF Accuracy: {oof_acc:.4f} | OOF Macro F1: {oof_f1:.4f}\")\n",
    "try:\n",
    "    print(\"\\nOOF Classification Report:\\n\",\n",
    "          classification_report(y_enc, oof_labels, target_names=classes, zero_division=0))\n",
    "except Exception as e:\n",
    "    print(f\"[Info] Could not print classification report: {e}\")\n",
    "\n",
    "# ==============================================\n",
    "# Build submission\n",
    "# ==============================================\n",
    "test_pred_proba = np.zeros((len(test_features), len(classes)), dtype=np.float32)\n",
    "test_pred_proba[test_male_mask.values] = male_test_pred\n",
    "test_pred_proba[test_female_mask.values] = female_test_pred\n",
    "\n",
    "test_pred_int = np.argmax(test_pred_proba, axis=1)\n",
    "test_pred_labels = le.inverse_transform(test_pred_int)\n",
    "\n",
    "ss_cols = list(sample_sub.columns)\n",
    "ID_HEADER = None\n",
    "LABEL_HEADER = None\n",
    "if len(ss_cols) == 2:\n",
    "    c1, c2 = ss_cols\n",
    "    if c1 in test.columns and c2 not in test.columns:\n",
    "        ID_HEADER, LABEL_HEADER = c1, c2\n",
    "    elif c2 in test.columns and c1 not in test.columns:\n",
    "        ID_HEADER, LABEL_HEADER = c2, c1\n",
    "if ID_HEADER is None:\n",
    "    ID_HEADER = ss_cols[0]\n",
    "    LABEL_HEADER = ss_cols[1]\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "if ID_HEADER in test.columns:\n",
    "    sub[ID_HEADER] = test[ID_HEADER].values\n",
    "else:\n",
    "    sub[ID_HEADER] = np.arange(len(test_features))\n",
    "sub[LABEL_HEADER] = test_pred_labels\n",
    "\n",
    "for c in ss_cols:\n",
    "    if c not in sub.columns:\n",
    "        sub[c] = sample_sub[c].iloc[0] if len(sample_sub[c]) else None\n",
    "sub = sub[ss_cols]\n",
    "\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSaved submission.csv\")\n",
    "print(sub.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7585fdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MALE (Kaggle)] Fold 1/5\n",
      "[MALE (Kaggle)] Best iteration (by wF1_boost): 487\n",
      "[MALE (Kaggle)] Fold 1 F1(Overweight_Level_I): 0.8117\n",
      "[MALE (Kaggle)] Fold 1 F1(Overweight_Level_II): 0.8324\n",
      "\n",
      "[MALE (Kaggle)] Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE (Kaggle)] Best iteration (by wF1_boost): 513\n",
      "[MALE (Kaggle)] Fold 2 F1(Overweight_Level_I): 0.8224\n",
      "[MALE (Kaggle)] Fold 2 F1(Overweight_Level_II): 0.8297\n",
      "\n",
      "[MALE (Kaggle)] Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE (Kaggle)] Best iteration (by wF1_boost): 232\n",
      "[MALE (Kaggle)] Fold 3 F1(Overweight_Level_I): 0.7700\n",
      "[MALE (Kaggle)] Fold 3 F1(Overweight_Level_II): 0.8244\n",
      "\n",
      "[MALE (Kaggle)] Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE (Kaggle)] Best iteration (by wF1_boost): 422\n",
      "[MALE (Kaggle)] Fold 4 F1(Overweight_Level_I): 0.7982\n",
      "[MALE (Kaggle)] Fold 4 F1(Overweight_Level_II): 0.8294\n",
      "\n",
      "[MALE (Kaggle)] Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE (Kaggle)] Best iteration (by wF1_boost): 234\n",
      "[MALE (Kaggle)] Fold 5 F1(Overweight_Level_I): 0.7692\n",
      "[MALE (Kaggle)] Fold 5 F1(Overweight_Level_II): 0.8195\n",
      "\n",
      "[MALE (Kaggle)] OOF Accuracy: 0.8874 | Macro F1: 0.7514\n",
      "[MALE (Kaggle)] Best iterations: [487, 513, 232, 422, 234] | Median: 422\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE (Kaggle)] Best iteration (by wF1_boost): 67\n",
      "[FEMALE (Kaggle)] Fold 1 F1(Overweight_Level_I): 0.7937\n",
      "[FEMALE (Kaggle)] Fold 1 F1(Overweight_Level_II): 0.7542\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE (Kaggle)] Best iteration (by wF1_boost): 96\n",
      "[FEMALE (Kaggle)] Fold 2 F1(Overweight_Level_I): 0.7674\n",
      "[FEMALE (Kaggle)] Fold 2 F1(Overweight_Level_II): 0.7431\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE (Kaggle)] Best iteration (by wF1_boost): 643\n",
      "[FEMALE (Kaggle)] Fold 3 F1(Overweight_Level_I): 0.7601\n",
      "[FEMALE (Kaggle)] Fold 3 F1(Overweight_Level_II): 0.7196\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE (Kaggle)] Best iteration (by wF1_boost): 309\n",
      "[FEMALE (Kaggle)] Fold 4 F1(Overweight_Level_I): 0.7673\n",
      "[FEMALE (Kaggle)] Fold 4 F1(Overweight_Level_II): 0.7773\n",
      "\n",
      "[FEMALE (Kaggle)] Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE (Kaggle)] Best iteration (by wF1_boost): 217\n",
      "[FEMALE (Kaggle)] Fold 5 F1(Overweight_Level_I): 0.8092\n",
      "[FEMALE (Kaggle)] Fold 5 F1(Overweight_Level_II): 0.7966\n",
      "\n",
      "[FEMALE (Kaggle)] OOF Accuracy: 0.9166 | Macro F1: 0.7504\n",
      "[FEMALE (Kaggle)] Best iterations: [67, 96, 643, 309, 217] | Median: 217\n",
      "\n",
      "âœ… Overall Accuracy on Kaggle_test: 0.90928\n",
      "\n",
      "=== Confusion Matrix (counts) ===\n",
      "Predicted â†’\n",
      "True â†“\n",
      "Insufficient_Weight   :  619 |   31 |    3 |    0 |    0 |    0 |    0\n",
      "Normal_Weight         :   39 |  646 |   46 |    5 |    1 |    0 |    0\n",
      "Overweight_Level_I    :    3 |   44 |  454 |   72 |   10 |    0 |    0\n",
      "Overweight_Level_II   :    0 |   16 |   51 |  529 |   41 |    4 |    0\n",
      "Obesity_Type_I        :    1 |    1 |   11 |   48 |  622 |   18 |    2\n",
      "Obesity_Type_II       :    0 |    0 |    2 |    5 |   18 |  820 |    0\n",
      "Obesity_Type_III      :    0 |    0 |    1 |    0 |    1 |    0 | 1061\n",
      "\n",
      "=== Confusion Matrix (row-normalized) ===\n",
      "Insufficient_Weight   : 0.95 | 0.05 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00\n",
      "Normal_Weight         : 0.05 | 0.88 | 0.06 | 0.01 | 0.00 | 0.00 | 0.00\n",
      "Overweight_Level_I    : 0.01 | 0.08 | 0.78 | 0.12 | 0.02 | 0.00 | 0.00\n",
      "Overweight_Level_II   : 0.00 | 0.02 | 0.08 | 0.83 | 0.06 | 0.01 | 0.00\n",
      "Obesity_Type_I        : 0.00 | 0.00 | 0.02 | 0.07 | 0.88 | 0.03 | 0.00\n",
      "Obesity_Type_II       : 0.00 | 0.00 | 0.00 | 0.01 | 0.02 | 0.97 | 0.00\n",
      "Obesity_Type_III      : 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00\n",
      "\n",
      "=== Per-class metrics ===\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight     0.9350    0.9479    0.9414       653\n",
      "      Normal_Weight     0.8753    0.8765    0.8759       737\n",
      " Overweight_Level_I     0.7993    0.7787    0.7889       583\n",
      "Overweight_Level_II     0.8027    0.8253    0.8138       641\n",
      "     Obesity_Type_I     0.8975    0.8848    0.8911       703\n",
      "    Obesity_Type_II     0.9739    0.9704    0.9721       845\n",
      "   Obesity_Type_III     0.9981    0.9981    0.9981      1063\n",
      "\n",
      "           accuracy                         0.9093      5225\n",
      "          macro avg     0.8974    0.8974    0.8974      5225\n",
      "       weighted avg     0.9093    0.9093    0.9093      5225\n",
      "\n",
      "\n",
      "=== Per-class accuracy (diagonal/row total) ===\n",
      "Insufficient_Weight    | Correct: 619 / 653 |  94.79%\n",
      "Normal_Weight          | Correct: 646 / 737 |  87.65%\n",
      "Overweight_Level_I     | Correct: 454 / 583 |  77.87%\n",
      "Overweight_Level_II    | Correct: 529 / 641 |  82.53%\n",
      "Obesity_Type_I         | Correct: 622 / 703 |  88.48%\n",
      "Obesity_Type_II        | Correct: 820 / 845 |  97.04%\n",
      "Obesity_Type_III       | Correct: 1061 / 1063 |  99.81%\n",
      "\n",
      "=== Most common confusions (true â†’ predicted) ===\n",
      "Overweight_Level_I        â†’ Overweight_Level_II       | Count:  72 | Row%:  12.3\n",
      "Overweight_Level_II       â†’ Overweight_Level_I        | Count:  51 | Row%:   8.0\n",
      "Obesity_Type_I            â†’ Overweight_Level_II       | Count:  48 | Row%:   6.8\n",
      "Normal_Weight             â†’ Overweight_Level_I        | Count:  46 | Row%:   6.2\n",
      "Overweight_Level_I        â†’ Normal_Weight             | Count:  44 | Row%:   7.5\n",
      "Overweight_Level_II       â†’ Obesity_Type_I            | Count:  41 | Row%:   6.4\n",
      "Normal_Weight             â†’ Insufficient_Weight       | Count:  39 | Row%:   5.3\n",
      "Insufficient_Weight       â†’ Normal_Weight             | Count:  31 | Row%:   4.7\n",
      "Obesity_Type_I            â†’ Obesity_Type_II           | Count:  18 | Row%:   2.6\n",
      "Obesity_Type_II           â†’ Obesity_Type_I            | Count:  18 | Row%:   2.1\n",
      "\n",
      "=== Sample of misclassified rows (first 10) ===\n",
      "Row    9: true=Overweight_Level_II    pred=Obesity_Type_I         conf=0.831 2nd=Overweight_Level_II   (0.089)\n",
      "Row   16: true=Overweight_Level_I     pred=Normal_Weight          conf=0.876 2nd=Overweight_Level_I    (0.057)\n",
      "Row   28: true=Normal_Weight          pred=Insufficient_Weight    conf=0.853 2nd=Normal_Weight         (0.081)\n",
      "Row   30: true=Overweight_Level_I     pred=Overweight_Level_II    conf=0.814 2nd=Overweight_Level_I    (0.150)\n",
      "Row   33: true=Overweight_Level_I     pred=Normal_Weight          conf=0.876 2nd=Overweight_Level_I    (0.050)\n",
      "Row   43: true=Obesity_Type_II        pred=Obesity_Type_I         conf=0.660 2nd=Obesity_Type_II       (0.320)\n",
      "Row   61: true=Obesity_Type_I         pred=Overweight_Level_II    conf=0.783 2nd=Obesity_Type_I        (0.129)\n",
      "Row   65: true=Overweight_Level_I     pred=Overweight_Level_II    conf=0.776 2nd=Obesity_Type_I        (0.093)\n",
      "Row   68: true=Overweight_Level_I     pred=Normal_Weight          conf=0.587 2nd=Overweight_Level_I    (0.400)\n",
      "Row   75: true=Overweight_Level_I     pred=Normal_Weight          conf=0.914 2nd=Overweight_Level_I    (0.079)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Evaluate on Kaggle_test.csv (with ground truth)\n",
    "# ==============================================\n",
    "if not os.path.exists(KAGGLE_TEST_PATH):\n",
    "    print(f\"\\n[Warn] {KAGGLE_TEST_PATH} not found. Skipping Kaggle_test evaluation.\")\n",
    "else:\n",
    "    kdf = pd.read_csv(KAGGLE_TEST_PATH)\n",
    "    if \"WeightCategory\" not in kdf.columns:\n",
    "        raise KeyError(\"Kaggle_test.csv must contain 'WeightCategory'.\")\n",
    "\n",
    "    y_true = kdf[\"WeightCategory\"].copy()\n",
    "    X_k = kdf.drop(columns=[\"WeightCategory\"], errors=\"ignore\").copy()\n",
    "    if id_col and id_col in X_k.columns:\n",
    "        X_k.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "    for c in [\"MTRANS\", \"SMOKE\"]:\n",
    "        if c in X_k.columns:\n",
    "            X_k.drop(columns=[c], inplace=True)\n",
    "    X_k = add_bmi(X_k)\n",
    "\n",
    "    gender_col_k = detect_gender_column(X_k)\n",
    "    if gender_col_k is None:\n",
    "        raise ValueError(\"Could not detect a gender column in Kaggle_test.csv\")\n",
    "    km_k, kf_k = split_by_gender(X_k[gender_col_k])\n",
    "\n",
    "    # Predict on Kaggle by reusing the same training procedure (per gender)\n",
    "    kaggle_pred_proba = np.zeros((len(X_k), len(classes)), dtype=np.float32)\n",
    "\n",
    "    if X_male.shape[0] > 0 and km_k.sum() > 0:\n",
    "        _, male_k_pred = train_group_and_predict(X_male, y_male_enc, X_k[km_k].reset_index(drop=True), \"MALE (Kaggle)\")\n",
    "        kaggle_pred_proba[km_k.values] = male_k_pred\n",
    "    if X_female.shape[0] > 0 and kf_k.sum() > 0:\n",
    "        _, female_k_pred = train_group_and_predict(X_female, y_female_enc, X_k[kf_k].reset_index(drop=True), \"FEMALE (Kaggle)\")\n",
    "        kaggle_pred_proba[kf_k.values] = female_k_pred\n",
    "\n",
    "    kaggle_pred_idx = np.argmax(kaggle_pred_proba, axis=1)\n",
    "    y_pred = le.inverse_transform(kaggle_pred_idx)\n",
    "\n",
    "    # -------- Overall accuracy to 5 decimals --------\n",
    "    overall_acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"\\nâœ… Overall Accuracy on Kaggle_test: {overall_acc:.5f}\")\n",
    "\n",
    "    # -------- Text-only error analysis (custom order) --------\n",
    "    order = [\n",
    "        'Insufficient_Weight',\n",
    "        'Normal_Weight',\n",
    "        'Overweight_Level_I',\n",
    "        'Overweight_Level_II',\n",
    "        'Obesity_Type_I',\n",
    "        'Obesity_Type_II',\n",
    "        'Obesity_Type_III'\n",
    "    ]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=order)\n",
    "    cm_norm = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    print(\"\\n=== Confusion Matrix (counts) ===\")\n",
    "    print(\"Predicted â†’\")\n",
    "    print(\"True â†“\")\n",
    "    for i, true_class in enumerate(order):\n",
    "        row = \" | \".join(f\"{cm[i, j]:4d}\" for j in range(len(order)))\n",
    "        print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "    print(\"\\n=== Confusion Matrix (row-normalized) ===\")\n",
    "    for i, true_class in enumerate(order):\n",
    "        row = \" | \".join(f\"{cm_norm[i, j]:.2f}\" for j in range(len(order)))\n",
    "        print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "    print(\"\\n=== Per-class metrics ===\")\n",
    "    try:\n",
    "        print(classification_report(y_true, y_pred, labels=order, target_names=order, digits=4, zero_division=0))\n",
    "    except Exception as e:\n",
    "        print(f\"[Info] classification_report fallback: {e}\")\n",
    "        print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "    print(\"\\n=== Per-class accuracy (diagonal/row total) ===\")\n",
    "    for i, c in enumerate(order):\n",
    "        total = cm[i].sum()\n",
    "        correct = cm[i, i]\n",
    "        acc = correct / total if total > 0 else 0.0\n",
    "        print(f\"{c:<22} | Correct: {correct:3d} / {total:3d} | {acc*100:6.2f}%\")\n",
    "\n",
    "    print(\"\\n=== Most common confusions (true â†’ predicted) ===\")\n",
    "    pairs = []\n",
    "    for i, t in enumerate(order):\n",
    "        for j, p in enumerate(order):\n",
    "            if i == j or cm[i, j] == 0:\n",
    "                continue\n",
    "            pairs.append((cm[i, j], t, p, cm_norm[i, j]))\n",
    "    pairs = sorted(pairs, key=lambda x: (-x[0], -x[3]))\n",
    "    for cnt, true_label, pred_label, norm_val in pairs[:10]:\n",
    "        print(f\"{true_label:25} â†’ {pred_label:25} | Count: {cnt:3d} | Row%: {norm_val*100:5.1f}\")\n",
    "\n",
    "    print(\"\\n=== Sample of misclassified rows (first 10) ===\")\n",
    "    mis_idx = np.where(np.asarray(y_true) != np.asarray(y_pred))[0]\n",
    "    if len(mis_idx) == 0:\n",
    "        print(\"ðŸŽ‰ No misclassifications!\")\n",
    "    else:\n",
    "        for idx in mis_idx[:10]:\n",
    "            true_lab = y_true.iloc[idx] if hasattr(y_true, \"iloc\") else y_true[idx]\n",
    "            pred_lab = y_pred[idx]\n",
    "            conf = float(np.max(kaggle_pred_proba[idx]))\n",
    "            rank = np.argsort(-kaggle_pred_proba[idx])\n",
    "            second_idx = rank[1] if rank.size > 1 else rank[0]\n",
    "            second_lab = le.inverse_transform([second_idx])[0]\n",
    "            second_conf = float(kaggle_pred_proba[idx][second_idx])\n",
    "            print(f\"Row {idx:4d}: true={true_lab:<22} pred={pred_lab:<22} conf={conf:.3f} 2nd={second_lab:<22}({second_conf:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff36c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e92c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f491a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992823c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "911445de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Classes: ['Insufficient_Weight', 'Normal_Weight', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III', 'Overweight_Level_I', 'Overweight_Level_II']\n",
      "[Info] Train M<24=4649, M>=24=3134, F<24=4786, F>=24=2964\n",
      "[Info] Test  M<24=6130, M>=24=4206, F<24=6420, F>=24=4002\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# XGB (Stratified CV) with BMI + Gender & Age-group routing (4 models)\n",
    "# Groups: Male<24, Male>=24, Female<24, Female>=24\n",
    "# Light class-boost for Overweight I/II + Kaggle_test evaluation\n",
    "# ==============================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.base import clone\n",
    "import xgboost as xgb\n",
    "\n",
    "# -------- Paths --------\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "SAMPLE_SUB_PATH = \"sample_submission.csv\"\n",
    "KAGGLE_TEST_PATH = \"Kaggle_test.csv\"  # must contain WeightCategory\n",
    "\n",
    "# -------- Globals --------\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "N_JOBS = -1\n",
    "\n",
    "# ---- Emphasize the tricky classes a bit ----\n",
    "BOOST_CLASSES = (\"Overweight_Level_I\", \"Overweight_Level_II\")\n",
    "TRAIN_WEIGHT_MULT = 1.75      # 1.5â€“2.5 is a sensible range\n",
    "NUM_BOOST_ROUND = 20000\n",
    "EARLY_STOP = 200\n",
    "\n",
    "# ==============================================\n",
    "# Helpers\n",
    "# ==============================================\n",
    "def norm_col(s: str) -> str:\n",
    "    if s is None:\n",
    "        return s\n",
    "    return str(s).replace(\"\\ufeff\", \"\").strip().lower()\n",
    "\n",
    "def infer_feature_types(df):\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def detect_gender_column(df):\n",
    "    # direct names\n",
    "    for c in df.columns:\n",
    "        if norm_col(c) in {\"gender\", \"sex\"}:\n",
    "            return c\n",
    "    # fallback: looks like M/F-ish\n",
    "    for c in df.columns:\n",
    "        vals = pd.Series(df[c].dropna().astype(str).str.lower().str.strip()).unique()\n",
    "        if len(vals) in (2, 3):\n",
    "            if any(v.startswith(\"m\") for v in vals) and any(v.startswith(\"f\") for v in vals):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def detect_age_column(df):\n",
    "    for c in df.columns:\n",
    "        if norm_col(c) in {\"age\", \"years\", \"age_years\"}:\n",
    "            return c\n",
    "    # heuristic: numeric col with reasonable range\n",
    "    candidates = []\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            q1, q99 = np.nanpercentile(s, [1, 99]) if s.notna().any() else (np.nan, np.nan)\n",
    "            if 5 <= q1 <= 60 or 5 <= q99 <= 100:  # very loose\n",
    "                candidates.append(c)\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "def split_by_gender(series):\n",
    "    s = series.astype(str).str.lower().str.strip()\n",
    "    male_mask = s.str.startswith((\"m\", \"1\", \"true\"))\n",
    "    female_mask = s.str.startswith((\"f\", \"0\", \"false\"))\n",
    "    if male_mask.sum() == 0 and female_mask.sum() == 0:\n",
    "        top = s.value_counts().index.tolist()\n",
    "        if len(top) >= 2:\n",
    "            male_mask = s == top[0]\n",
    "            female_mask = s == top[1]\n",
    "    return male_mask, female_mask\n",
    "\n",
    "def add_bmi(df):\n",
    "    \"\"\"BMI = Weight / (Height_m^2). If median height > 3 assume cm -> meters.\"\"\"\n",
    "    if (\"Weight\" in df.columns) and (\"Height\" in df.columns):\n",
    "        h = pd.to_numeric(df[\"Height\"], errors=\"coerce\")\n",
    "        height_m = np.where(np.nanmedian(h) > 3.0, h / 100.0, h)\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            bmi = pd.to_numeric(df[\"Weight\"], errors=\"coerce\") / (np.power(height_m, 2) + 1e-12)\n",
    "        df[\"BMI\"] = pd.Series(bmi).replace([np.inf, -np.inf], np.nan)\n",
    "    return df\n",
    "\n",
    "def add_age_features(df, age_col):\n",
    "    \"\"\"Create rounded age and binary AgeGroup (<24 vs >=24).\"\"\"\n",
    "    if age_col is None or age_col not in df.columns:\n",
    "        raise ValueError(\"Age column not found; cannot create AgeGroup split.\")\n",
    "    age = pd.to_numeric(df[age_col], errors=\"coerce\")\n",
    "    df[\"AgeRounded\"] = np.rint(age).astype(\"float32\")  # nearest integer (kept numeric for model)\n",
    "    df[\"AgeGroup\"] = np.where(age < 24, \"<24\", \">=24\")\n",
    "    return df\n",
    "\n",
    "# ==============================================\n",
    "# Load data\n",
    "# ==============================================\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "# Optional drops\n",
    "for c in [\"MTRANS\", \"SMOKE\"]:\n",
    "    if c in train.columns:\n",
    "        train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:\n",
    "        test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "train = add_bmi(train)\n",
    "test = add_bmi(test)\n",
    "\n",
    "# Detect ID/Target\n",
    "id_col = None\n",
    "for cand in [\"id\", \"row_id\", \"index\", \"sample_id\"]:\n",
    "    if cand in train.columns and cand in test.columns:\n",
    "        id_col = cand\n",
    "        break\n",
    "\n",
    "target_col = None\n",
    "for cand in [\"WeightCategory\", \"NObeyesdad\", \"label\", \"target\", \"class\", \"y\"]:\n",
    "    if cand in train.columns:\n",
    "        target_col = cand\n",
    "        break\n",
    "if target_col is None:\n",
    "    raise ValueError(\"Could not detect target column in train.csv\")\n",
    "\n",
    "# Build X/y\n",
    "y = train[target_col].copy()\n",
    "X = train.drop(columns=[target_col]).copy()\n",
    "if id_col and id_col in X.columns:\n",
    "    X.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "test_features = test.copy()\n",
    "if id_col and id_col in test_features.columns:\n",
    "    test_ids = test_features[id_col].copy()\n",
    "    test_features.drop(columns=[id_col], inplace=True)\n",
    "else:\n",
    "    test_ids = pd.Series(np.arange(len(test_features)), name=\"id\")\n",
    "\n",
    "# Label encode target\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "classes = list(le.classes_)\n",
    "print(f\"[Info] Classes: {classes}\")\n",
    "\n",
    "# Detect gender + age and create age groups\n",
    "gender_col = detect_gender_column(pd.concat([X, test_features], axis=0))\n",
    "if gender_col is None:\n",
    "    raise ValueError(\"Could not detect a gender column (e.g., 'Gender'/'SEX').\")\n",
    "\n",
    "age_col = detect_age_column(pd.concat([X, test_features], axis=0))\n",
    "if age_col is None:\n",
    "    raise ValueError(\"Could not detect an Age column.\")\n",
    "\n",
    "# Add AgeRounded & AgeGroup in BOTH train/test (and DO NOT drop originals)\n",
    "train = add_age_features(train, age_col)\n",
    "test_features = add_age_features(test_features, age_col)\n",
    "\n",
    "# Rebuild X (because train got new columns)\n",
    "X = train.drop(columns=[target_col]).copy()\n",
    "if id_col and id_col in X.columns:\n",
    "    X.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "# Split gender masks\n",
    "train_male_mask, train_female_mask = split_by_gender(train[gender_col])\n",
    "test_male_mask, test_female_mask = split_by_gender(test_features[gender_col])\n",
    "\n",
    "# Split age-group masks\n",
    "train_young_mask = train[\"AgeGroup\"] == \"<24\"\n",
    "train_old_mask = train[\"AgeGroup\"] != \"<24\"\n",
    "test_young_mask = test_features[\"AgeGroup\"] == \"<24\"\n",
    "test_old_mask = test_features[\"AgeGroup\"] != \"<24\"\n",
    "\n",
    "print(f\"[Info] Train M<24={int((train_male_mask & train_young_mask).sum())}, \"\n",
    "      f\"M>=24={int((train_male_mask & train_old_mask).sum())}, \"\n",
    "      f\"F<24={int((train_female_mask & train_young_mask).sum())}, \"\n",
    "      f\"F>=24={int((train_female_mask & train_old_mask).sum())}\")\n",
    "\n",
    "print(f\"[Info] Test  M<24={int((test_male_mask & test_young_mask).sum())}, \"\n",
    "      f\"M>=24={int((test_male_mask & test_old_mask).sum())}, \"\n",
    "      f\"F<24={int((test_female_mask & test_young_mask).sum())}, \"\n",
    "      f\"F>=24={int((test_female_mask & test_old_mask).sum())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4f6a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# Training function (group-specific)\n",
    "# Stratified 5-fold + light class-boost on Overweight I/II\n",
    "# ==============================================\n",
    "def train_group_and_predict(X_grp, y_enc_grp, test_grp, group_name,\n",
    "                            boost_targets=BOOST_CLASSES, base_boost=TRAIN_WEIGHT_MULT):\n",
    "    # Remove columns that are constant within this group (gender & age group stay but may be constant)\n",
    "    # Weâ€™ll explicitly drop gender because itâ€™s constant inside gender-split; AgeGroup can remain as it differs across groups globally\n",
    "    cols_to_use = [c for c in X_grp.columns if c != gender_col]\n",
    "    Xg = X_grp[cols_to_use].copy()\n",
    "    Xtestg = test_grp[cols_to_use].copy()\n",
    "\n",
    "    num_cols, cat_cols = infer_feature_types(Xg)\n",
    "\n",
    "    # Preprocessor\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", ohe)\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=1.0\n",
    "    )\n",
    "\n",
    "    # XGB params\n",
    "    xgb_params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": len(classes),\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"max_depth\": 6,\n",
    "        \"min_child_weight\": 2,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"lambda\": 1.0,\n",
    "        \"alpha\": 0.0,\n",
    "        \"eta\": 0.03,\n",
    "        \"nthread\": N_JOBS,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof_group = np.zeros((len(Xg), len(classes)), dtype=np.float32)\n",
    "    test_group_pred = np.zeros((len(Xtestg), len(classes)), dtype=np.float32)\n",
    "    fold_best = []\n",
    "\n",
    "    # map class name -> index\n",
    "    cls_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    boost_idx = {cls_to_idx[c] for c in boost_targets if c in cls_to_idx}\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(Xg, y_enc_grp), start=1):\n",
    "        print(f\"\\n[{group_name}] Fold {fold}/{N_FOLDS}\")\n",
    "        X_tr, X_va = Xg.iloc[tr_idx], Xg.iloc[va_idx]\n",
    "        y_tr, y_va = y_enc_grp[tr_idx], y_enc_grp[va_idx]\n",
    "\n",
    "        prep = clone(preprocessor)\n",
    "        Xtr = prep.fit_transform(X_tr)\n",
    "        Xva = prep.transform(X_va)\n",
    "\n",
    "        # deterministic class-boost weights (no jitter)\n",
    "        w_tr = np.ones_like(y_tr, dtype=float)\n",
    "        for idx in boost_idx:\n",
    "            w_tr[y_tr == idx] = base_boost\n",
    "        w_va = np.ones_like(y_va, dtype=float)\n",
    "\n",
    "        dtrain = xgb.DMatrix(Xtr, label=y_tr, weight=w_tr)\n",
    "        dval   = xgb.DMatrix(Xva, label=y_va, weight=w_va)\n",
    "\n",
    "        bst = xgb.train(\n",
    "            params=xgb_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=NUM_BOOST_ROUND,\n",
    "            evals=[(dtrain, \"train\"), (dval, \"valid\")],\n",
    "            early_stopping_rounds=EARLY_STOP,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        best_round = int(bst.best_iteration + 1)\n",
    "        fold_best.append(best_round)\n",
    "        print(f\"[{group_name}] Best iteration: {best_round}\")\n",
    "\n",
    "        # OOF store\n",
    "        oof_proba = bst.predict(dval, iteration_range=(0, best_round))\n",
    "        oof_group[va_idx] = oof_proba\n",
    "\n",
    "        # Test preds for this fold\n",
    "        Xtest_tf = prep.transform(Xtestg)\n",
    "        dtest = xgb.DMatrix(Xtest_tf)\n",
    "        test_group_pred += bst.predict(dtest, iteration_range=(0, best_round)) / N_FOLDS\n",
    "\n",
    "    # OOF summary\n",
    "    oof_labels = np.argmax(oof_group, axis=1)\n",
    "    acc_g = accuracy_score(y_enc_grp, oof_labels)\n",
    "    f1_g = f1_score(y_enc_grp, oof_labels, average=\"macro\")\n",
    "    print(f\"\\n[{group_name}] OOF Accuracy: {acc_g:.4f} | Macro F1: {f1_g:.4f}\")\n",
    "    print(f\"[{group_name}] Best iterations: {fold_best} | Median: {int(np.median(fold_best))}\")\n",
    "\n",
    "    return oof_group, test_group_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "409a659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MALE_<24] Fold 1/5\n",
      "[MALE_<24] Best iteration: 277\n",
      "\n",
      "[MALE_<24] Fold 2/5\n",
      "[MALE_<24] Best iteration: 268\n",
      "\n",
      "[MALE_<24] Fold 3/5\n",
      "[MALE_<24] Best iteration: 313\n",
      "\n",
      "[MALE_<24] Fold 4/5\n",
      "[MALE_<24] Best iteration: 329\n",
      "\n",
      "[MALE_<24] Fold 5/5\n",
      "[MALE_<24] Best iteration: 282\n",
      "\n",
      "[MALE_<24] OOF Accuracy: 0.8692 | Macro F1: 0.7504\n",
      "[MALE_<24] Best iterations: [277, 268, 313, 329, 282] | Median: 282\n",
      "\n",
      "[MALE_>=24] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE_>=24] Best iteration: 261\n",
      "\n",
      "[MALE_>=24] Fold 2/5\n",
      "[MALE_>=24] Best iteration: 198\n",
      "\n",
      "[MALE_>=24] Fold 3/5\n",
      "[MALE_>=24] Best iteration: 224\n",
      "\n",
      "[MALE_>=24] Fold 4/5\n",
      "[MALE_>=24] Best iteration: 238\n",
      "\n",
      "[MALE_>=24] Fold 5/5\n",
      "[MALE_>=24] Best iteration: 242\n",
      "\n",
      "[MALE_>=24] OOF Accuracy: 0.9017 | Macro F1: 0.6018\n",
      "[MALE_>=24] Best iterations: [261, 198, 224, 238, 242] | Median: 238\n",
      "\n",
      "[FEMALE_<24] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE_<24] Best iteration: 265\n",
      "\n",
      "[FEMALE_<24] Fold 2/5\n",
      "[FEMALE_<24] Best iteration: 391\n",
      "\n",
      "[FEMALE_<24] Fold 3/5\n",
      "[FEMALE_<24] Best iteration: 306\n",
      "\n",
      "[FEMALE_<24] Fold 4/5\n",
      "[FEMALE_<24] Best iteration: 310\n",
      "\n",
      "[FEMALE_<24] Fold 5/5\n",
      "[FEMALE_<24] Best iteration: 299\n",
      "\n",
      "[FEMALE_<24] OOF Accuracy: 0.9070 | Macro F1: 0.7537\n",
      "[FEMALE_<24] Best iterations: [265, 391, 306, 310, 299] | Median: 306\n",
      "\n",
      "[FEMALE_>=24] Fold 1/5\n",
      "[FEMALE_>=24] Best iteration: 270\n",
      "\n",
      "[FEMALE_>=24] Fold 2/5\n",
      "[FEMALE_>=24] Best iteration: 224\n",
      "\n",
      "[FEMALE_>=24] Fold 3/5\n",
      "[FEMALE_>=24] Best iteration: 216\n",
      "\n",
      "[FEMALE_>=24] Fold 4/5\n",
      "[FEMALE_>=24] Best iteration: 232\n",
      "\n",
      "[FEMALE_>=24] Fold 5/5\n",
      "[FEMALE_>=24] Best iteration: 201\n",
      "\n",
      "[FEMALE_>=24] OOF Accuracy: 0.9288 | Macro F1: 0.6814\n",
      "[FEMALE_>=24] Best iterations: [270, 224, 216, 232, 201] | Median: 224\n",
      "\n",
      "========== OVERALL OOF ==========\n",
      "OOF Accuracy: 0.8988 | OOF Macro F1: 0.8892\n",
      "\n",
      "OOF Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.92      0.93      0.93      1870\n",
      "      Normal_Weight       0.89      0.87      0.88      2345\n",
      "     Obesity_Type_I       0.90      0.86      0.88      2207\n",
      "    Obesity_Type_II       0.96      0.97      0.97      2403\n",
      "   Obesity_Type_III       0.99      1.00      1.00      2983\n",
      " Overweight_Level_I       0.78      0.78      0.78      1844\n",
      "Overweight_Level_II       0.78      0.82      0.80      1881\n",
      "\n",
      "           accuracy                           0.90     15533\n",
      "          macro avg       0.89      0.89      0.89     15533\n",
      "       weighted avg       0.90      0.90      0.90     15533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================\n",
    "# Build the four groups and train\n",
    "# ==============================================\n",
    "def pick_cols(df):\n",
    "    # ensure we train on the same column set as X (already dropped id)\n",
    "    return df[X.columns.tolist()].copy()\n",
    "\n",
    "groups = {\n",
    "    \"MALE_<24\":   (train_male_mask & train_young_mask,  test_male_mask & test_young_mask),\n",
    "    \"MALE_>=24\":  (train_male_mask & train_old_mask,    test_male_mask & test_old_mask),\n",
    "    \"FEMALE_<24\": (train_female_mask & train_young_mask,test_female_mask & test_young_mask),\n",
    "    \"FEMALE_>=24\":(train_female_mask & train_old_mask,  test_female_mask & test_old_mask),\n",
    "}\n",
    "\n",
    "oof_full = np.zeros((len(X), len(classes)), dtype=np.float32)\n",
    "test_pred_proba = np.zeros((len(test_features), len(classes)), dtype=np.float32)\n",
    "\n",
    "for gname, (tr_mask, te_mask) in groups.items():\n",
    "    if tr_mask.sum() == 0:\n",
    "        print(f\"[Warn] No training rows for group {gname}; skipping.\")\n",
    "        continue\n",
    "    Xg = pick_cols(train.loc[tr_mask])\n",
    "    yg = y_enc[tr_mask]\n",
    "    Xtg = pick_cols(test_features.loc[te_mask])\n",
    "\n",
    "    oof_g, test_g = train_group_and_predict(Xg, yg, Xtg, gname)\n",
    "    oof_full[np.where(tr_mask)[0]] = oof_g\n",
    "    test_pred_proba[np.where(te_mask)[0]] = test_g\n",
    "\n",
    "# ==============================================\n",
    "# OOF summary (all groups combined)\n",
    "# ==============================================\n",
    "oof_labels = np.argmax(oof_full, axis=1)\n",
    "oof_acc = accuracy_score(y_enc, oof_labels)\n",
    "oof_f1 = f1_score(y_enc, oof_labels, average=\"macro\")\n",
    "print(\"\\n========== OVERALL OOF ==========\")\n",
    "print(f\"OOF Accuracy: {oof_acc:.4f} | OOF Macro F1: {oof_f1:.4f}\")\n",
    "try:\n",
    "    print(\"\\nOOF Classification Report:\\n\",\n",
    "          classification_report(y_enc, oof_labels, target_names=classes, zero_division=0))\n",
    "except Exception as e:\n",
    "    print(f\"[Info] Could not print classification report: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fdde466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved submission.csv\n",
      "   id       WeightCategory\n",
      "0   0  Overweight_Level_II\n",
      "1   1        Normal_Weight\n",
      "2   2  Insufficient_Weight\n",
      "3   3     Obesity_Type_III\n",
      "4   4  Overweight_Level_II\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Build submission from test_pred_proba\n",
    "# ==============================================\n",
    "test_pred_int = np.argmax(test_pred_proba, axis=1)\n",
    "test_pred_labels = le.inverse_transform(test_pred_int)\n",
    "\n",
    "ss_cols = list(sample_sub.columns)\n",
    "ID_HEADER = None\n",
    "LABEL_HEADER = None\n",
    "if len(ss_cols) == 2:\n",
    "    c1, c2 = ss_cols\n",
    "    if c1 in test.columns and c2 not in test.columns:\n",
    "        ID_HEADER, LABEL_HEADER = c1, c2\n",
    "    elif c2 in test.columns and c1 not in test.columns:\n",
    "        ID_HEADER, LABEL_HEADER = c2, c1\n",
    "if ID_HEADER is None:\n",
    "    ID_HEADER = ss_cols[0]\n",
    "    LABEL_HEADER = ss_cols[1]\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "if ID_HEADER in test.columns:\n",
    "    sub[ID_HEADER] = test[ID_HEADER].values\n",
    "else:\n",
    "    sub[ID_HEADER] = np.arange(len(test_features))\n",
    "sub[LABEL_HEADER] = test_pred_labels\n",
    "\n",
    "for c in ss_cols:\n",
    "    if c not in sub.columns:\n",
    "        sub[c] = sample_sub[c].iloc[0] if len(sample_sub[c]) else None\n",
    "sub = sub[ss_cols]\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSaved submission.csv\")\n",
    "print(sub.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b88db550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MALE_<24 (Kaggle)] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE_<24 (Kaggle)] Best iteration: 277\n",
      "\n",
      "[MALE_<24 (Kaggle)] Fold 2/5\n",
      "[MALE_<24 (Kaggle)] Best iteration: 268\n",
      "\n",
      "[MALE_<24 (Kaggle)] Fold 3/5\n",
      "[MALE_<24 (Kaggle)] Best iteration: 313\n",
      "\n",
      "[MALE_<24 (Kaggle)] Fold 4/5\n",
      "[MALE_<24 (Kaggle)] Best iteration: 329\n",
      "\n",
      "[MALE_<24 (Kaggle)] Fold 5/5\n",
      "[MALE_<24 (Kaggle)] Best iteration: 282\n",
      "\n",
      "[MALE_<24 (Kaggle)] OOF Accuracy: 0.8692 | Macro F1: 0.7504\n",
      "[MALE_<24 (Kaggle)] Best iterations: [277, 268, 313, 329, 282] | Median: 282\n",
      "\n",
      "[MALE_>=24 (Kaggle)] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE_>=24 (Kaggle)] Best iteration: 261\n",
      "\n",
      "[MALE_>=24 (Kaggle)] Fold 2/5\n",
      "[MALE_>=24 (Kaggle)] Best iteration: 198\n",
      "\n",
      "[MALE_>=24 (Kaggle)] Fold 3/5\n",
      "[MALE_>=24 (Kaggle)] Best iteration: 224\n",
      "\n",
      "[MALE_>=24 (Kaggle)] Fold 4/5\n",
      "[MALE_>=24 (Kaggle)] Best iteration: 238\n",
      "\n",
      "[MALE_>=24 (Kaggle)] Fold 5/5\n",
      "[MALE_>=24 (Kaggle)] Best iteration: 242\n",
      "\n",
      "[MALE_>=24 (Kaggle)] OOF Accuracy: 0.9017 | Macro F1: 0.6018\n",
      "[MALE_>=24 (Kaggle)] Best iterations: [261, 198, 224, 238, 242] | Median: 238\n",
      "\n",
      "[FEMALE_<24 (Kaggle)] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE_<24 (Kaggle)] Best iteration: 265\n",
      "\n",
      "[FEMALE_<24 (Kaggle)] Fold 2/5\n",
      "[FEMALE_<24 (Kaggle)] Best iteration: 391\n",
      "\n",
      "[FEMALE_<24 (Kaggle)] Fold 3/5\n",
      "[FEMALE_<24 (Kaggle)] Best iteration: 306\n",
      "\n",
      "[FEMALE_<24 (Kaggle)] Fold 4/5\n",
      "[FEMALE_<24 (Kaggle)] Best iteration: 310\n",
      "\n",
      "[FEMALE_<24 (Kaggle)] Fold 5/5\n",
      "[FEMALE_<24 (Kaggle)] Best iteration: 299\n",
      "\n",
      "[FEMALE_<24 (Kaggle)] OOF Accuracy: 0.9070 | Macro F1: 0.7537\n",
      "[FEMALE_<24 (Kaggle)] Best iterations: [265, 391, 306, 310, 299] | Median: 306\n",
      "\n",
      "[FEMALE_>=24 (Kaggle)] Fold 1/5\n",
      "[FEMALE_>=24 (Kaggle)] Best iteration: 270\n",
      "\n",
      "[FEMALE_>=24 (Kaggle)] Fold 2/5\n",
      "[FEMALE_>=24 (Kaggle)] Best iteration: 224\n",
      "\n",
      "[FEMALE_>=24 (Kaggle)] Fold 3/5\n",
      "[FEMALE_>=24 (Kaggle)] Best iteration: 216\n",
      "\n",
      "[FEMALE_>=24 (Kaggle)] Fold 4/5\n",
      "[FEMALE_>=24 (Kaggle)] Best iteration: 232\n",
      "\n",
      "[FEMALE_>=24 (Kaggle)] Fold 5/5\n",
      "[FEMALE_>=24 (Kaggle)] Best iteration: 201\n",
      "\n",
      "[FEMALE_>=24 (Kaggle)] OOF Accuracy: 0.9288 | Macro F1: 0.6814\n",
      "[FEMALE_>=24 (Kaggle)] Best iterations: [270, 224, 216, 232, 201] | Median: 224\n",
      "\n",
      "âœ… Overall Accuracy on Kaggle_test: 0.90947\n",
      "\n",
      "=== Confusion Matrix (counts) ===\n",
      "Predicted â†’\n",
      "True â†“\n",
      "Insufficient_Weight   :  622 |   28 |    3 |    0 |    0 |    0 |    0\n",
      "Normal_Weight         :   42 |  643 |   43 |    8 |    1 |    0 |    0\n",
      "Overweight_Level_I    :    3 |   46 |  452 |   72 |   10 |    0 |    0\n",
      "Overweight_Level_II   :    0 |   16 |   50 |  530 |   41 |    4 |    0\n",
      "Obesity_Type_I        :    1 |    1 |   13 |   46 |  624 |   16 |    2\n",
      "Obesity_Type_II       :    0 |    0 |    1 |    6 |   17 |  821 |    0\n",
      "Obesity_Type_III      :    0 |    0 |    1 |    0 |    2 |    0 | 1060\n",
      "\n",
      "=== Confusion Matrix (row-normalized) ===\n",
      "Insufficient_Weight   : 0.95 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00\n",
      "Normal_Weight         : 0.06 | 0.87 | 0.06 | 0.01 | 0.00 | 0.00 | 0.00\n",
      "Overweight_Level_I    : 0.01 | 0.08 | 0.78 | 0.12 | 0.02 | 0.00 | 0.00\n",
      "Overweight_Level_II   : 0.00 | 0.02 | 0.08 | 0.83 | 0.06 | 0.01 | 0.00\n",
      "Obesity_Type_I        : 0.00 | 0.00 | 0.02 | 0.07 | 0.89 | 0.02 | 0.00\n",
      "Obesity_Type_II       : 0.00 | 0.00 | 0.00 | 0.01 | 0.02 | 0.97 | 0.00\n",
      "Obesity_Type_III      : 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00\n",
      "\n",
      "=== Per-class metrics ===\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight     0.9311    0.9525    0.9417       653\n",
      "      Normal_Weight     0.8760    0.8725    0.8742       737\n",
      " Overweight_Level_I     0.8028    0.7753    0.7888       583\n",
      "Overweight_Level_II     0.8006    0.8268    0.8135       641\n",
      "     Obesity_Type_I     0.8978    0.8876    0.8927       703\n",
      "    Obesity_Type_II     0.9762    0.9716    0.9739       845\n",
      "   Obesity_Type_III     0.9981    0.9972    0.9976      1063\n",
      "\n",
      "           accuracy                         0.9095      5225\n",
      "          macro avg     0.8975    0.8976    0.8975      5225\n",
      "       weighted avg     0.9095    0.9095    0.9094      5225\n",
      "\n",
      "\n",
      "=== Per-class accuracy (diagonal/row total) ===\n",
      "Insufficient_Weight    | Correct: 622 / 653 |  95.25%\n",
      "Normal_Weight          | Correct: 643 / 737 |  87.25%\n",
      "Overweight_Level_I     | Correct: 452 / 583 |  77.53%\n",
      "Overweight_Level_II    | Correct: 530 / 641 |  82.68%\n",
      "Obesity_Type_I         | Correct: 624 / 703 |  88.76%\n",
      "Obesity_Type_II        | Correct: 821 / 845 |  97.16%\n",
      "Obesity_Type_III       | Correct: 1060 / 1063 |  99.72%\n",
      "\n",
      "=== Most common confusions (true â†’ predicted) ===\n",
      "Overweight_Level_I        â†’ Overweight_Level_II       | Count:  72 | Row%:  12.3\n",
      "Overweight_Level_II       â†’ Overweight_Level_I        | Count:  50 | Row%:   7.8\n",
      "Overweight_Level_I        â†’ Normal_Weight             | Count:  46 | Row%:   7.9\n",
      "Obesity_Type_I            â†’ Overweight_Level_II       | Count:  46 | Row%:   6.5\n",
      "Normal_Weight             â†’ Overweight_Level_I        | Count:  43 | Row%:   5.8\n",
      "Normal_Weight             â†’ Insufficient_Weight       | Count:  42 | Row%:   5.7\n",
      "Overweight_Level_II       â†’ Obesity_Type_I            | Count:  41 | Row%:   6.4\n",
      "Insufficient_Weight       â†’ Normal_Weight             | Count:  28 | Row%:   4.3\n",
      "Obesity_Type_II           â†’ Obesity_Type_I            | Count:  17 | Row%:   2.0\n",
      "Overweight_Level_II       â†’ Normal_Weight             | Count:  16 | Row%:   2.5\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Evaluate on Kaggle_test.csv (with ground truth)\n",
    "# ==============================================\n",
    "if not os.path.exists(KAGGLE_TEST_PATH):\n",
    "    print(f\"\\n[Warn] {KAGGLE_TEST_PATH} not found. Skipping Kaggle_test evaluation.\")\n",
    "else:\n",
    "    kdf = pd.read_csv(KAGGLE_TEST_PATH)\n",
    "    if \"WeightCategory\" not in kdf.columns:\n",
    "        raise KeyError(\"Kaggle_test.csv must contain 'WeightCategory'.\")\n",
    "\n",
    "    y_true = kdf[\"WeightCategory\"].copy()\n",
    "    X_k = kdf.drop(columns=[\"WeightCategory\"], errors=\"ignore\").copy()\n",
    "    if id_col and id_col in X_k.columns:\n",
    "        X_k.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "    # same drops + BMI + Age features\n",
    "    for c in [\"MTRANS\", \"SMOKE\"]:\n",
    "        if c in X_k.columns:\n",
    "            X_k.drop(columns=[c], inplace=True)\n",
    "    X_k = add_bmi(X_k)\n",
    "\n",
    "    # detect gender/age and add age features in Kaggle set\n",
    "    gender_col_k = detect_gender_column(X_k)\n",
    "    if gender_col_k is None:\n",
    "        raise ValueError(\"Could not detect a gender column in Kaggle_test.csv\")\n",
    "    age_col_k = detect_age_column(X_k)\n",
    "    if age_col_k is None:\n",
    "        raise ValueError(\"Could not detect an Age column in Kaggle_test.csv\")\n",
    "    X_k = add_age_features(X_k, age_col_k)\n",
    "\n",
    "    km_k, kf_k = split_by_gender(X_k[gender_col_k])\n",
    "    ky_k = X_k[\"AgeGroup\"] == \"<24\"\n",
    "    ko_k = ~ky_k\n",
    "\n",
    "    # Reconstruct by routing through the same 4 groups\n",
    "    kaggle_pred_proba = np.zeros((len(X_k), len(classes)), dtype=np.float32)\n",
    "\n",
    "    def pick_cols_k(df):\n",
    "        # ensure the same training columns order\n",
    "        use_cols = [c for c in X.columns.tolist() if c in df.columns]\n",
    "        # fill any missing columns (all-NaN) to match transformer columns\n",
    "        missing = [c for c in X.columns.tolist() if c not in df.columns]\n",
    "        tmp = df[use_cols].copy()\n",
    "        for m in missing:\n",
    "            tmp[m] = np.nan\n",
    "        # reorder to X.columns\n",
    "        return tmp[X.columns.tolist()]\n",
    "\n",
    "    # helper to train and infer per group (reusing the same routine)\n",
    "    def infer_group(train_mask, kaggle_mask, name):\n",
    "        if train_mask.sum() == 0 or kaggle_mask.sum() == 0:\n",
    "            print(f\"[Kaggle] Skip group {name}: train={int(train_mask.sum())}, eval={int(kaggle_mask.sum())}\")\n",
    "            return\n",
    "        Xg = X.loc[train_mask, :]\n",
    "        yg = y_enc[train_mask]\n",
    "        Xkg = pick_cols_k(X_k.loc[kaggle_mask, :])\n",
    "        oof_g, pred_g = train_group_and_predict(Xg, yg, Xkg, f\"{name} (Kaggle)\")\n",
    "        kaggle_pred_proba[np.where(kaggle_mask)[0]] = pred_g\n",
    "\n",
    "    infer_group(train_male_mask & train_young_mask, km_k & ky_k, \"MALE_<24\")\n",
    "    infer_group(train_male_mask & train_old_mask,   km_k & ko_k, \"MALE_>=24\")\n",
    "    infer_group(train_female_mask & train_young_mask, kf_k & ky_k, \"FEMALE_<24\")\n",
    "    infer_group(train_female_mask & train_old_mask,   kf_k & ko_k, \"FEMALE_>=24\")\n",
    "\n",
    "    kaggle_pred_idx = np.argmax(kaggle_pred_proba, axis=1)\n",
    "    y_pred = le.inverse_transform(kaggle_pred_idx)\n",
    "\n",
    "        # -------- Overall accuracy to 5 decimals --------\n",
    "    overall_acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"\\nâœ… Overall Accuracy on Kaggle_test: {overall_acc:.5f}\")\n",
    "\n",
    "    # -------- Detailed analysis --------\n",
    "    order = [\n",
    "        'Insufficient_Weight',\n",
    "        'Normal_Weight',\n",
    "        'Overweight_Level_I',\n",
    "        'Overweight_Level_II',\n",
    "        'Obesity_Type_I',\n",
    "        'Obesity_Type_II',\n",
    "        'Obesity_Type_III'\n",
    "    ]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=order)\n",
    "    cm_norm = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    print(\"\\n=== Confusion Matrix (counts) ===\")\n",
    "    print(\"Predicted â†’\")\n",
    "    print(\"True â†“\")\n",
    "    for i, true_class in enumerate(order):\n",
    "        row = \" | \".join(f\"{cm[i, j]:4d}\" for j in range(len(order)))\n",
    "        print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "    print(\"\\n=== Confusion Matrix (row-normalized) ===\")\n",
    "    for i, true_class in enumerate(order):\n",
    "        row = \" | \".join(f\"{cm_norm[i, j]:.2f}\" for j in range(len(order)))\n",
    "        print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "    print(\"\\n=== Per-class metrics ===\")\n",
    "    try:\n",
    "        print(classification_report(y_true, y_pred, labels=order, target_names=order, digits=4, zero_division=0))\n",
    "    except Exception as e:\n",
    "        print(f\"[Info] classification_report fallback: {e}\")\n",
    "        print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "    print(\"\\n=== Per-class accuracy (diagonal/row total) ===\")\n",
    "    for i, c in enumerate(order):\n",
    "        total = cm[i].sum()\n",
    "        correct = cm[i, i]\n",
    "        acc = correct / total if total > 0 else 0.0\n",
    "        print(f\"{c:<22} | Correct: {correct:3d} / {total:3d} | {acc*100:6.2f}%\")\n",
    "\n",
    "    print(\"\\n=== Most common confusions (true â†’ predicted) ===\")\n",
    "    pairs = []\n",
    "    for i, t in enumerate(order):\n",
    "        for j, p in enumerate(order):\n",
    "            if i == j or cm[i, j] == 0:\n",
    "                continue\n",
    "            pairs.append((cm[i, j], t, p, cm_norm[i, j]))\n",
    "    pairs = sorted(pairs, key=lambda x: (-x[0], -x[3]))\n",
    "    for cnt, true_label, pred_label, norm_val in pairs[:10]:\n",
    "        print(f\"{true_label:25} â†’ {pred_label:25} | Count: {cnt:3d} | Row%: {norm_val*100:5.1f}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c3f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02f37340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# XGB (Stratified CV) with BMI + Gender & Age-group routing (4 models)\n",
    "# SMOTE (train-fold only) for Overweight I/II + Kaggle_test evaluation\n",
    "# ==============================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.base import clone\n",
    "import xgboost as xgb\n",
    "\n",
    "# Try to import SMOTE; fallback gracefully if not available\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    IMBLEARN_OK = True\n",
    "except Exception as e:\n",
    "    print(\"[Warn] imblearn not available; SMOTE will be disabled:\", e)\n",
    "    IMBLEARN_OK = False\n",
    "\n",
    "# -------- Paths --------\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "SAMPLE_SUB_PATH = \"sample_submission.csv\"\n",
    "KAGGLE_TEST_PATH = \"Kaggle_test.csv\"  # must contain WeightCategory\n",
    "\n",
    "# -------- Globals --------\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "N_JOBS = -1\n",
    "\n",
    "# ---- Emphasize the tricky classes a bit ----\n",
    "BOOST_CLASSES = (\"Overweight_Level_I\", \"Overweight_Level_II\")\n",
    "TRAIN_WEIGHT_MULT = 1.75      # 1.5â€“2.5 is a sensible range\n",
    "USE_SMOTE = True              # turn on/off globally\n",
    "NUM_BOOST_ROUND = 20000\n",
    "EARLY_STOP = 200\n",
    "\n",
    "# ==============================================\n",
    "# Helpers\n",
    "# ==============================================\n",
    "def norm_col(s: str) -> str:\n",
    "    if s is None:\n",
    "        return s\n",
    "    return str(s).replace(\"\\ufeff\", \"\").strip().lower()\n",
    "\n",
    "def infer_feature_types(df):\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def detect_gender_column(df):\n",
    "    for c in df.columns:\n",
    "        if norm_col(c) in {\"gender\", \"sex\"}:\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        vals = pd.Series(df[c].dropna().astype(str).str.lower().str.strip()).unique()\n",
    "        if len(vals) in (2, 3):\n",
    "            if any(v.startswith(\"m\") for v in vals) and any(v.startswith(\"f\") for v in vals):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def detect_age_column(df):\n",
    "    for c in df.columns:\n",
    "        if norm_col(c) in {\"age\", \"years\", \"age_years\"}:\n",
    "            return c\n",
    "    # heuristic: numeric col with plausible range\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            if s.notna().sum() > 0:\n",
    "                q1, q99 = np.nanpercentile(s, [1, 99])\n",
    "                if 5 <= q1 <= 60 or 5 <= q99 <= 100:\n",
    "                    return c\n",
    "    return None\n",
    "\n",
    "def split_by_gender(series):\n",
    "    s = series.astype(str).str.lower().str.strip()\n",
    "    male_mask = s.str.startswith((\"m\", \"1\", \"true\"))\n",
    "    female_mask = s.str.startswith((\"f\", \"0\", \"false\"))\n",
    "    if male_mask.sum() == 0 and female_mask.sum() == 0:\n",
    "        top = s.value_counts().index.tolist()\n",
    "        if len(top) >= 2:\n",
    "            male_mask = s == top[0]\n",
    "            female_mask = s == top[1]\n",
    "    return male_mask, female_mask\n",
    "\n",
    "def add_bmi(df):\n",
    "    \"\"\"BMI = Weight / (Height_m^2). If median height > 3 assume cm -> meters.\"\"\"\n",
    "    if (\"Weight\" in df.columns) and (\"Height\" in df.columns):\n",
    "        h = pd.to_numeric(df[\"Height\"], errors=\"coerce\")\n",
    "        height_m = np.where(np.nanmedian(h) > 3.0, h / 100.0, h)\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            bmi = pd.to_numeric(df[\"Weight\"], errors=\"coerce\") / (np.power(height_m, 2) + 1e-12)\n",
    "        df[\"BMI\"] = pd.Series(bmi).replace([np.inf, -np.inf], np.nan)\n",
    "    return df\n",
    "\n",
    "def add_age_features(df, age_col):\n",
    "    \"\"\"Create rounded age and binary AgeGroup (<24 vs >=24).\"\"\"\n",
    "    if age_col is None or age_col not in df.columns:\n",
    "        raise ValueError(\"Age column not found; cannot create AgeGroup split.\")\n",
    "    age = pd.to_numeric(df[age_col], errors=\"coerce\")\n",
    "    df[\"AgeRounded\"] = np.rint(age).astype(\"float32\")\n",
    "    df[\"AgeGroup\"] = np.where(age < 24, \"<24\", \">=24\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7883a6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Classes: ['Insufficient_Weight', 'Normal_Weight', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III', 'Overweight_Level_I', 'Overweight_Level_II']\n",
      "[Info] Train M<24=4649, M>=24=3134, F<24=4786, F>=24=2964\n",
      "[Info] Test  M<24=6130, M>=24=4206, F<24=6420, F>=24=4002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================\n",
    "# Load data\n",
    "# ==============================================\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "# Optional drops\n",
    "for c in [\"MTRANS\", \"SMOKE\"]:\n",
    "    if c in train.columns:\n",
    "        train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:\n",
    "        test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "train = add_bmi(train)\n",
    "test = add_bmi(test)\n",
    "\n",
    "# Detect ID/Target\n",
    "id_col = None\n",
    "for cand in [\"id\", \"row_id\", \"index\", \"sample_id\"]:\n",
    "    if cand in train.columns and cand in test.columns:\n",
    "        id_col = cand\n",
    "        break\n",
    "\n",
    "target_col = None\n",
    "for cand in [\"WeightCategory\", \"NObeyesdad\", \"label\", \"target\", \"class\", \"y\"]:\n",
    "    if cand in train.columns:\n",
    "        target_col = cand\n",
    "        break\n",
    "if target_col is None:\n",
    "    raise ValueError(\"Could not detect target column in train.csv\")\n",
    "\n",
    "# Build X/y\n",
    "y = train[target_col].copy()\n",
    "X = train.drop(columns=[target_col]).copy()\n",
    "if id_col and id_col in X.columns:\n",
    "    X.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "test_features = test.copy()\n",
    "if id_col and id_col in test_features.columns:\n",
    "    test_ids = test_features[id_col].copy()\n",
    "    test_features.drop(columns=[id_col], inplace=True)\n",
    "else:\n",
    "    test_ids = pd.Series(np.arange(len(test_features)), name=\"id\")\n",
    "\n",
    "# Label encode target\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "classes = list(le.classes_)\n",
    "print(f\"[Info] Classes: {classes}\")\n",
    "\n",
    "# Detect gender + age and create age groups\n",
    "gender_col = detect_gender_column(pd.concat([X, test_features], axis=0))\n",
    "if gender_col is None:\n",
    "    raise ValueError(\"Could not detect a gender column (e.g., 'Gender'/'SEX').\")\n",
    "\n",
    "age_col = detect_age_column(pd.concat([X, test_features], axis=0))\n",
    "if age_col is None:\n",
    "    raise ValueError(\"Could not detect an Age column.\")\n",
    "\n",
    "# Add AgeRounded & AgeGroup in BOTH train/test\n",
    "train = add_age_features(train, age_col)\n",
    "test_features = add_age_features(test_features, age_col)\n",
    "\n",
    "# Rebuild X (because train got new columns)\n",
    "X = train.drop(columns=[target_col]).copy()\n",
    "if id_col and id_col in X.columns:\n",
    "    X.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "# Split gender masks\n",
    "train_male_mask, train_female_mask = split_by_gender(train[gender_col])\n",
    "test_male_mask, test_female_mask = split_by_gender(test_features[gender_col])\n",
    "\n",
    "# Split age-group masks\n",
    "train_young_mask = train[\"AgeGroup\"] == \"<24\"\n",
    "train_old_mask = ~train_young_mask\n",
    "test_young_mask = test_features[\"AgeGroup\"] == \"<24\"\n",
    "test_old_mask = ~test_young_mask\n",
    "\n",
    "print(f\"[Info] Train M<24={int((train_male_mask & train_young_mask).sum())}, \"\n",
    "      f\"M>=24={int((train_male_mask & train_old_mask).sum())}, \"\n",
    "      f\"F<24={int((train_female_mask & train_young_mask).sum())}, \"\n",
    "      f\"F>=24={int((train_female_mask & train_old_mask).sum())}\")\n",
    "\n",
    "print(f\"[Info] Test  M<24={int((test_male_mask & test_young_mask).sum())}, \"\n",
    "      f\"M>=24={int((test_male_mask & test_old_mask).sum())}, \"\n",
    "      f\"F<24={int((test_female_mask & test_young_mask).sum())}, \"\n",
    "      f\"F>=24={int((test_female_mask & test_old_mask).sum())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad2bdda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================\n",
    "# Training function (group-specific)\n",
    "# Stratified 5-fold + optional SMOTE for Overweight I/II\n",
    "# ==============================================\n",
    "def train_group_and_predict(X_grp, y_enc_grp, test_grp, group_name,\n",
    "                            boost_targets=BOOST_CLASSES, base_boost=TRAIN_WEIGHT_MULT):\n",
    "    # Drop gender (constant in a gender-split); keep AgeGroup/rounded as features\n",
    "    cols_to_use = [c for c in X_grp.columns if c != gender_col]\n",
    "    Xg = X_grp[cols_to_use].copy()\n",
    "    Xtestg = test_grp[cols_to_use].copy()\n",
    "\n",
    "    num_cols, cat_cols = infer_feature_types(Xg)\n",
    "\n",
    "    # Preprocessor -> DENSE (SMOTE needs dense)\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=True))  # dense, so with_mean=True is fine\n",
    "    ])\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)  # force dense\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", ohe)\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.0   # <- ensure dense output\n",
    "    )\n",
    "\n",
    "    # XGB params\n",
    "    xgb_params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": len(classes),\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"max_depth\": 6,\n",
    "        \"min_child_weight\": 2,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"lambda\": 1.0,\n",
    "        \"alpha\": 0.0,\n",
    "        \"eta\": 0.03,\n",
    "        \"nthread\": N_JOBS,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof_group = np.zeros((len(Xg), len(classes)), dtype=np.float32)\n",
    "    test_group_pred = np.zeros((len(Xtestg), len(classes)), dtype=np.float32)\n",
    "    fold_best = []\n",
    "\n",
    "    # indices\n",
    "    cls_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    idx_I = cls_to_idx.get(\"Overweight_Level_I\", None)\n",
    "    idx_II = cls_to_idx.get(\"Overweight_Level_II\", None)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(Xg, y_enc_grp), start=1):\n",
    "        print(f\"\\n[{group_name}] Fold {fold}/{N_FOLDS}\")\n",
    "        X_tr, X_va = Xg.iloc[tr_idx], Xg.iloc[va_idx]\n",
    "        y_tr, y_va = y_enc_grp[tr_idx], y_enc_grp[va_idx]\n",
    "\n",
    "        prep = clone(preprocessor)\n",
    "        Xtr = prep.fit_transform(X_tr)  # dense\n",
    "        Xva = prep.transform(X_va)      # dense\n",
    "        Xte = prep.transform(Xtestg)    # dense\n",
    "\n",
    "        # --------- SMOTE (train split only) focusing on Overweight I/II ---------\n",
    "        used_smote = False\n",
    "        if USE_SMOTE and IMBLEARN_OK and (idx_I is not None) and (idx_II is not None):\n",
    "            # class counts\n",
    "            counts = np.bincount(y_tr, minlength=len(classes))\n",
    "            # need at least 2 samples per class for SMOTE kNN\n",
    "            if counts[idx_I] >= 2 and counts[idx_II] >= 2:\n",
    "                max_target = int(np.max(counts))  # upsample the two to max class size\n",
    "                sampling_strategy = {idx_I: max_target, idx_II: max_target}\n",
    "\n",
    "                # pick k_neighbors safely (must be < minority count)\n",
    "                k_safe = max(1, min(5, counts[idx_I] - 1, counts[idx_II] - 1))\n",
    "                if k_safe >= 1 and max_target > counts[idx_I] and max_target > counts[idx_II]:\n",
    "                    smote = SMOTE(\n",
    "                        random_state=RANDOM_STATE + fold,\n",
    "                        sampling_strategy=sampling_strategy,\n",
    "                        k_neighbors=k_safe\n",
    "                    )\n",
    "                    Xtr, y_tr = smote.fit_resample(Xtr, y_tr)\n",
    "                    used_smote = True\n",
    "                    print(f\"[{group_name}] SMOTE applied (k={k_safe}) to Overweight I/II -> size {Xtr.shape}\")\n",
    "\n",
    "        # --------- Sample weights (skip extra boosts if SMOTE already balanced) ---------\n",
    "        if used_smote:\n",
    "            w_tr = np.ones_like(y_tr, dtype=float)\n",
    "        else:\n",
    "            # light deterministic boost to I/II (kept small to avoid double counting)\n",
    "            w_tr = np.ones_like(y_tr, dtype=float)\n",
    "            if idx_I is not None:\n",
    "                w_tr[y_tr == idx_I] = TRAIN_WEIGHT_MULT\n",
    "            if idx_II is not None:\n",
    "                w_tr[y_tr == idx_II] = TRAIN_WEIGHT_MULT\n",
    "\n",
    "        w_va = np.ones_like(y_va, dtype=float)\n",
    "\n",
    "        dtrain = xgb.DMatrix(Xtr, label=y_tr, weight=w_tr)\n",
    "        dval   = xgb.DMatrix(Xva, label=y_va, weight=w_va)\n",
    "        dtest  = xgb.DMatrix(Xte)\n",
    "\n",
    "        bst = xgb.train(\n",
    "            params=xgb_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=NUM_BOOST_ROUND,\n",
    "            evals=[(dtrain, \"train\"), (dval, \"valid\")],\n",
    "            early_stopping_rounds=EARLY_STOP,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        best_round = int(bst.best_iteration + 1)\n",
    "        fold_best.append(best_round)\n",
    "        print(f\"[{group_name}] Best iteration: {best_round}\")\n",
    "\n",
    "        # OOF\n",
    "        oof_proba = bst.predict(dval, iteration_range=(0, best_round))\n",
    "        oof_group[va_idx] = oof_proba\n",
    "\n",
    "        # Test preds (fold-avg)\n",
    "        test_group_pred += bst.predict(dtest, iteration_range=(0, best_round)) / N_FOLDS\n",
    "\n",
    "    # OOF summary\n",
    "    oof_labels = np.argmax(oof_group, axis=1)\n",
    "    acc_g = accuracy_score(y_enc_grp, oof_labels)\n",
    "    f1_g = f1_score(y_enc_grp, oof_labels, average=\"macro\")\n",
    "    print(f\"\\n[{group_name}] OOF Accuracy: {acc_g:.4f} | Macro F1: {f1_g:.4f}\")\n",
    "    print(f\"[{group_name}] Best iterations: {fold_best} | Median: {int(np.median(fold_best))}\")\n",
    "\n",
    "    return oof_group, test_group_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "999deced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MALE_<24] Fold 1/5\n",
      "[MALE_<24] SMOTE applied (k=5) to Overweight I/II -> size (3987, 24)\n",
      "[MALE_<24] Best iteration: 287\n",
      "\n",
      "[MALE_<24] Fold 2/5\n",
      "[MALE_<24] SMOTE applied (k=5) to Overweight I/II -> size (3990, 24)\n",
      "[MALE_<24] Best iteration: 268\n",
      "\n",
      "[MALE_<24] Fold 3/5\n",
      "[MALE_<24] SMOTE applied (k=5) to Overweight I/II -> size (3989, 24)\n",
      "[MALE_<24] Best iteration: 301\n",
      "\n",
      "[MALE_<24] Fold 4/5\n",
      "[MALE_<24] SMOTE applied (k=5) to Overweight I/II -> size (3989, 24)\n",
      "[MALE_<24] Best iteration: 321\n",
      "\n",
      "[MALE_<24] Fold 5/5\n",
      "[MALE_<24] SMOTE applied (k=5) to Overweight I/II -> size (3989, 24)\n",
      "[MALE_<24] Best iteration: 254\n",
      "\n",
      "[MALE_<24] OOF Accuracy: 0.8684 | Macro F1: 0.7494\n",
      "[MALE_<24] Best iterations: [287, 268, 301, 321, 254] | Median: 287\n",
      "\n",
      "[MALE_>=24] Fold 1/5\n",
      "[MALE_>=24] SMOTE applied (k=5) to Overweight I/II -> size (5051, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE_>=24] Best iteration: 290\n",
      "\n",
      "[MALE_>=24] Fold 2/5\n",
      "[MALE_>=24] SMOTE applied (k=5) to Overweight I/II -> size (5051, 24)\n",
      "[MALE_>=24] Best iteration: 224\n",
      "\n",
      "[MALE_>=24] Fold 3/5\n",
      "[MALE_>=24] SMOTE applied (k=5) to Overweight I/II -> size (5049, 24)\n",
      "[MALE_>=24] Best iteration: 228\n",
      "\n",
      "[MALE_>=24] Fold 4/5\n",
      "[MALE_>=24] SMOTE applied (k=5) to Overweight I/II -> size (5050, 24)\n",
      "[MALE_>=24] Best iteration: 216\n",
      "\n",
      "[MALE_>=24] Fold 5/5\n",
      "[MALE_>=24] SMOTE applied (k=5) to Overweight I/II -> size (5051, 24)\n",
      "[MALE_>=24] Best iteration: 245\n",
      "\n",
      "[MALE_>=24] OOF Accuracy: 0.8928 | Macro F1: 0.5900\n",
      "[MALE_>=24] Best iterations: [290, 224, 228, 216, 245] | Median: 228\n",
      "\n",
      "[FEMALE_<24] Fold 1/5\n",
      "[FEMALE_<24] SMOTE applied (k=5) to Overweight I/II -> size (5007, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE_<24] Best iteration: 308\n",
      "\n",
      "[FEMALE_<24] Fold 2/5\n",
      "[FEMALE_<24] SMOTE applied (k=5) to Overweight I/II -> size (5008, 24)\n",
      "[FEMALE_<24] Best iteration: 366\n",
      "\n",
      "[FEMALE_<24] Fold 3/5\n",
      "[FEMALE_<24] SMOTE applied (k=5) to Overweight I/II -> size (5007, 24)\n",
      "[FEMALE_<24] Best iteration: 300\n",
      "\n",
      "[FEMALE_<24] Fold 4/5\n",
      "[FEMALE_<24] SMOTE applied (k=5) to Overweight I/II -> size (5009, 24)\n",
      "[FEMALE_<24] Best iteration: 315\n",
      "\n",
      "[FEMALE_<24] Fold 5/5\n",
      "[FEMALE_<24] SMOTE applied (k=5) to Overweight I/II -> size (5009, 24)\n",
      "[FEMALE_<24] Best iteration: 297\n",
      "\n",
      "[FEMALE_<24] OOF Accuracy: 0.9056 | Macro F1: 0.7536\n",
      "[FEMALE_<24] Best iterations: [308, 366, 300, 315, 297] | Median: 308\n",
      "\n",
      "[FEMALE_>=24] Fold 1/5\n",
      "[FEMALE_>=24] SMOTE applied (k=5) to Overweight I/II -> size (5059, 24)\n",
      "[FEMALE_>=24] Best iteration: 344\n",
      "\n",
      "[FEMALE_>=24] Fold 2/5\n",
      "[FEMALE_>=24] SMOTE applied (k=5) to Overweight I/II -> size (5058, 24)\n",
      "[FEMALE_>=24] Best iteration: 223\n",
      "\n",
      "[FEMALE_>=24] Fold 3/5\n",
      "[FEMALE_>=24] SMOTE applied (k=5) to Overweight I/II -> size (5058, 24)\n",
      "[FEMALE_>=24] Best iteration: 219\n",
      "\n",
      "[FEMALE_>=24] Fold 4/5\n",
      "[FEMALE_>=24] SMOTE applied (k=5) to Overweight I/II -> size (5058, 24)\n",
      "[FEMALE_>=24] Best iteration: 220\n",
      "\n",
      "[FEMALE_>=24] Fold 5/5\n",
      "[FEMALE_>=24] SMOTE applied (k=5) to Overweight I/II -> size (5059, 24)\n",
      "[FEMALE_>=24] Best iteration: 213\n",
      "\n",
      "[FEMALE_>=24] OOF Accuracy: 0.9204 | Macro F1: 0.6595\n",
      "[FEMALE_>=24] Best iterations: [344, 223, 219, 220, 213] | Median: 220\n",
      "\n",
      "========== OVERALL OOF ==========\n",
      "OOF Accuracy: 0.8947 | OOF Macro F1: 0.8847\n",
      "\n",
      "OOF Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.92      0.93      0.93      1870\n",
      "      Normal_Weight       0.88      0.87      0.87      2345\n",
      "     Obesity_Type_I       0.90      0.85      0.87      2207\n",
      "    Obesity_Type_II       0.96      0.97      0.97      2403\n",
      "   Obesity_Type_III       1.00      1.00      1.00      2983\n",
      " Overweight_Level_I       0.77      0.77      0.77      1844\n",
      "Overweight_Level_II       0.77      0.82      0.79      1881\n",
      "\n",
      "           accuracy                           0.89     15533\n",
      "          macro avg       0.88      0.88      0.88     15533\n",
      "       weighted avg       0.90      0.89      0.89     15533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================\n",
    "# Build the four groups and train\n",
    "# ==============================================\n",
    "def pick_cols(df):\n",
    "    return df[X.columns.tolist()].copy()\n",
    "\n",
    "groups = {\n",
    "    \"MALE_<24\":   (train_male_mask & train_young_mask,  test_male_mask & test_young_mask),\n",
    "    \"MALE_>=24\":  (train_male_mask & train_old_mask,    test_male_mask & test_old_mask),\n",
    "    \"FEMALE_<24\": (train_female_mask & train_young_mask,test_female_mask & test_young_mask),\n",
    "    \"FEMALE_>=24\":(train_female_mask & train_old_mask,  test_female_mask & test_old_mask),\n",
    "}\n",
    "\n",
    "oof_full = np.zeros((len(X), len(classes)), dtype=np.float32)\n",
    "test_pred_proba = np.zeros((len(test_features), len(classes)), dtype=np.float32)\n",
    "\n",
    "for gname, (tr_mask, te_mask) in groups.items():\n",
    "    if tr_mask.sum() == 0:\n",
    "        print(f\"[Warn] No training rows for group {gname}; skipping.\")\n",
    "        continue\n",
    "    Xg = pick_cols(train.loc[tr_mask])\n",
    "    yg = y_enc[tr_mask]\n",
    "    Xtg = pick_cols(test_features.loc[te_mask])\n",
    "\n",
    "    oof_g, test_g = train_group_and_predict(Xg, yg, Xtg, gname)\n",
    "    oof_full[np.where(tr_mask)[0]] = oof_g\n",
    "    test_pred_proba[np.where(te_mask)[0]] = test_g\n",
    "\n",
    "# ==============================================\n",
    "# OOF summary (all groups combined)\n",
    "# ==============================================\n",
    "oof_labels = np.argmax(oof_full, axis=1)\n",
    "oof_acc = accuracy_score(y_enc, oof_labels)\n",
    "oof_f1 = f1_score(y_enc, oof_labels, average=\"macro\")\n",
    "print(\"\\n========== OVERALL OOF ==========\")\n",
    "print(f\"OOF Accuracy: {oof_acc:.4f} | OOF Macro F1: {oof_f1:.4f}\")\n",
    "try:\n",
    "    print(\"\\nOOF Classification Report:\\n\",\n",
    "          classification_report(y_enc, oof_labels, target_names=classes, zero_division=0))\n",
    "except Exception as e:\n",
    "    print(f\"[Info] Could not print classification report: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ac8779e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved submission.csv\n",
      "   id       WeightCategory\n",
      "0   0  Overweight_Level_II\n",
      "1   1        Normal_Weight\n",
      "2   2  Insufficient_Weight\n",
      "3   3     Obesity_Type_III\n",
      "4   4  Overweight_Level_II\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================\n",
    "# Build submission\n",
    "# ==============================================\n",
    "test_pred_int = np.argmax(test_pred_proba, axis=1)\n",
    "test_pred_labels = le.inverse_transform(test_pred_int)\n",
    "\n",
    "ss_cols = list(sample_sub.columns)\n",
    "ID_HEADER = None\n",
    "LABEL_HEADER = None\n",
    "if len(ss_cols) == 2:\n",
    "    c1, c2 = ss_cols\n",
    "    if c1 in test.columns and c2 not in test.columns:\n",
    "        ID_HEADER, LABEL_HEADER = c1, c2\n",
    "    elif c2 in test.columns and c1 not in test.columns:\n",
    "        ID_HEADER, LABEL_HEADER = c2, c1\n",
    "if ID_HEADER is None:\n",
    "    ID_HEADER = ss_cols[0]\n",
    "    LABEL_HEADER = ss_cols[1]\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "if ID_HEADER in test.columns:\n",
    "    sub[ID_HEADER] = test[ID_HEADER].values\n",
    "else:\n",
    "    sub[ID_HEADER] = np.arange(len(test_features))\n",
    "sub[LABEL_HEADER] = test_pred_labels\n",
    "\n",
    "for c in ss_cols:\n",
    "    if c not in sub.columns:\n",
    "        sub[c] = sample_sub[c].iloc[0] if len(sample_sub[c]) else None\n",
    "sub = sub[ss_cols]\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSaved submission.csv\")\n",
    "print(sub.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03699350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MALE_<24 (Kaggle)] Fold 1/5\n",
      "[MALE_<24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (3987, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE_<24 (Kaggle)] Best iteration: 287\n",
      "\n",
      "[MALE_<24 (Kaggle)] Fold 2/5\n",
      "[MALE_<24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (3990, 24)\n",
      "[MALE_<24 (Kaggle)] Best iteration: 268\n",
      "\n",
      "[MALE_<24 (Kaggle)] Fold 3/5\n",
      "[MALE_<24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (3989, 24)\n",
      "[MALE_<24 (Kaggle)] Best iteration: 301\n",
      "\n",
      "[MALE_<24 (Kaggle)] Fold 4/5\n",
      "[MALE_<24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (3989, 24)\n",
      "[MALE_<24 (Kaggle)] Best iteration: 321\n",
      "\n",
      "[MALE_<24 (Kaggle)] Fold 5/5\n",
      "[MALE_<24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (3989, 24)\n",
      "[MALE_<24 (Kaggle)] Best iteration: 254\n",
      "\n",
      "[MALE_<24 (Kaggle)] OOF Accuracy: 0.8684 | Macro F1: 0.7494\n",
      "[MALE_<24 (Kaggle)] Best iterations: [287, 268, 301, 321, 254] | Median: 287\n",
      "\n",
      "[MALE_>=24 (Kaggle)] Fold 1/5\n",
      "[MALE_>=24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (5051, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MALE_>=24 (Kaggle)] Best iteration: 290\n",
      "\n",
      "[MALE_>=24 (Kaggle)] Fold 2/5\n",
      "[MALE_>=24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (5051, 24)\n",
      "[MALE_>=24 (Kaggle)] Best iteration: 224\n",
      "\n",
      "[MALE_>=24 (Kaggle)] Fold 3/5\n",
      "[MALE_>=24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (5049, 24)\n",
      "[MALE_>=24 (Kaggle)] Best iteration: 228\n",
      "\n",
      "[MALE_>=24 (Kaggle)] Fold 4/5\n",
      "[MALE_>=24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (5050, 24)\n",
      "[MALE_>=24 (Kaggle)] Best iteration: 216\n",
      "\n",
      "[MALE_>=24 (Kaggle)] Fold 5/5\n",
      "[MALE_>=24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (5051, 24)\n",
      "[MALE_>=24 (Kaggle)] Best iteration: 245\n",
      "\n",
      "[MALE_>=24 (Kaggle)] OOF Accuracy: 0.8928 | Macro F1: 0.5900\n",
      "[MALE_>=24 (Kaggle)] Best iterations: [290, 224, 228, 216, 245] | Median: 228\n",
      "\n",
      "[FEMALE_<24 (Kaggle)] Fold 1/5\n",
      "[FEMALE_<24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (5007, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surya\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEMALE_<24 (Kaggle)] Best iteration: 308\n",
      "\n",
      "[FEMALE_<24 (Kaggle)] Fold 2/5\n",
      "[FEMALE_<24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (5008, 24)\n",
      "[FEMALE_<24 (Kaggle)] Best iteration: 366\n",
      "\n",
      "[FEMALE_<24 (Kaggle)] Fold 3/5\n",
      "[FEMALE_<24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (5007, 24)\n",
      "[FEMALE_<24 (Kaggle)] Best iteration: 300\n",
      "\n",
      "[FEMALE_<24 (Kaggle)] Fold 4/5\n",
      "[FEMALE_<24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (5009, 24)\n",
      "[FEMALE_<24 (Kaggle)] Best iteration: 315\n",
      "\n",
      "[FEMALE_<24 (Kaggle)] Fold 5/5\n",
      "[FEMALE_<24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (5009, 24)\n",
      "[FEMALE_<24 (Kaggle)] Best iteration: 297\n",
      "\n",
      "[FEMALE_<24 (Kaggle)] OOF Accuracy: 0.9056 | Macro F1: 0.7536\n",
      "[FEMALE_<24 (Kaggle)] Best iterations: [308, 366, 300, 315, 297] | Median: 308\n",
      "\n",
      "[FEMALE_>=24 (Kaggle)] Fold 1/5\n",
      "[FEMALE_>=24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (5059, 24)\n",
      "[FEMALE_>=24 (Kaggle)] Best iteration: 344\n",
      "\n",
      "[FEMALE_>=24 (Kaggle)] Fold 2/5\n",
      "[FEMALE_>=24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (5058, 24)\n",
      "[FEMALE_>=24 (Kaggle)] Best iteration: 223\n",
      "\n",
      "[FEMALE_>=24 (Kaggle)] Fold 3/5\n",
      "[FEMALE_>=24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (5058, 24)\n",
      "[FEMALE_>=24 (Kaggle)] Best iteration: 219\n",
      "\n",
      "[FEMALE_>=24 (Kaggle)] Fold 4/5\n",
      "[FEMALE_>=24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (5058, 24)\n",
      "[FEMALE_>=24 (Kaggle)] Best iteration: 220\n",
      "\n",
      "[FEMALE_>=24 (Kaggle)] Fold 5/5\n",
      "[FEMALE_>=24 (Kaggle)] SMOTE applied (k=5) to Overweight I/II -> size (5059, 24)\n",
      "[FEMALE_>=24 (Kaggle)] Best iteration: 213\n",
      "\n",
      "[FEMALE_>=24 (Kaggle)] OOF Accuracy: 0.9204 | Macro F1: 0.6595\n",
      "[FEMALE_>=24 (Kaggle)] Best iterations: [344, 223, 219, 220, 213] | Median: 220\n",
      "\n",
      "âœ… Overall Accuracy on Kaggle_test: 0.90565\n",
      "\n",
      "=== Confusion Matrix (counts) ===\n",
      "Predicted â†’\n",
      "True â†“\n",
      "Insufficient_Weight   :  622 |   28 |    3 |    0 |    0 |    0 |    0\n",
      "Normal_Weight         :   44 |  646 |   40 |    6 |    1 |    0 |    0\n",
      "Overweight_Level_I    :    5 |   53 |  444 |   71 |   10 |    0 |    0\n",
      "Overweight_Level_II   :    0 |   16 |   60 |  520 |   42 |    3 |    0\n",
      "Obesity_Type_I        :    1 |    1 |   14 |   47 |  620 |   17 |    3\n",
      "Obesity_Type_II       :    0 |    0 |    2 |    5 |   19 |  819 |    0\n",
      "Obesity_Type_III      :    0 |    0 |    1 |    0 |    1 |    0 | 1061\n",
      "\n",
      "=== Confusion Matrix (row-normalized) ===\n",
      "Insufficient_Weight   : 0.95 | 0.04 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00\n",
      "Normal_Weight         : 0.06 | 0.88 | 0.05 | 0.01 | 0.00 | 0.00 | 0.00\n",
      "Overweight_Level_I    : 0.01 | 0.09 | 0.76 | 0.12 | 0.02 | 0.00 | 0.00\n",
      "Overweight_Level_II   : 0.00 | 0.02 | 0.09 | 0.81 | 0.07 | 0.00 | 0.00\n",
      "Obesity_Type_I        : 0.00 | 0.00 | 0.02 | 0.07 | 0.88 | 0.02 | 0.00\n",
      "Obesity_Type_II       : 0.00 | 0.00 | 0.00 | 0.01 | 0.02 | 0.97 | 0.00\n",
      "Obesity_Type_III      : 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00\n",
      "\n",
      "=== Per-class metrics ===\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight     0.9256    0.9525    0.9389       653\n",
      "      Normal_Weight     0.8683    0.8765    0.8724       737\n",
      " Overweight_Level_I     0.7872    0.7616    0.7742       583\n",
      "Overweight_Level_II     0.8012    0.8112    0.8062       641\n",
      "     Obesity_Type_I     0.8947    0.8819    0.8883       703\n",
      "    Obesity_Type_II     0.9762    0.9692    0.9727       845\n",
      "   Obesity_Type_III     0.9972    0.9981    0.9976      1063\n",
      "\n",
      "           accuracy                         0.9056      5225\n",
      "          macro avg     0.8929    0.8930    0.8929      5225\n",
      "       weighted avg     0.9054    0.9056    0.9055      5225\n",
      "\n",
      "\n",
      "=== Per-class accuracy (diagonal/row total) ===\n",
      "Insufficient_Weight    | Correct: 622 / 653 |  95.25%\n",
      "Normal_Weight          | Correct: 646 / 737 |  87.65%\n",
      "Overweight_Level_I     | Correct: 444 / 583 |  76.16%\n",
      "Overweight_Level_II    | Correct: 520 / 641 |  81.12%\n",
      "Obesity_Type_I         | Correct: 620 / 703 |  88.19%\n",
      "Obesity_Type_II        | Correct: 819 / 845 |  96.92%\n",
      "Obesity_Type_III       | Correct: 1061 / 1063 |  99.81%\n",
      "\n",
      "=== Most common confusions (true â†’ predicted) ===\n",
      "Overweight_Level_I        â†’ Overweight_Level_II       | Count:  71 | Row%:  12.2\n",
      "Overweight_Level_II       â†’ Overweight_Level_I        | Count:  60 | Row%:   9.4\n",
      "Overweight_Level_I        â†’ Normal_Weight             | Count:  53 | Row%:   9.1\n",
      "Obesity_Type_I            â†’ Overweight_Level_II       | Count:  47 | Row%:   6.7\n",
      "Normal_Weight             â†’ Insufficient_Weight       | Count:  44 | Row%:   6.0\n",
      "Overweight_Level_II       â†’ Obesity_Type_I            | Count:  42 | Row%:   6.6\n",
      "Normal_Weight             â†’ Overweight_Level_I        | Count:  40 | Row%:   5.4\n",
      "Insufficient_Weight       â†’ Normal_Weight             | Count:  28 | Row%:   4.3\n",
      "Obesity_Type_II           â†’ Obesity_Type_I            | Count:  19 | Row%:   2.2\n",
      "Obesity_Type_I            â†’ Obesity_Type_II           | Count:  17 | Row%:   2.4\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Evaluate on Kaggle_test.csv (with ground truth)\n",
    "# ==============================================\n",
    "if not os.path.exists(KAGGLE_TEST_PATH):\n",
    "    print(f\"\\n[Warn] {KAGGLE_TEST_PATH} not found. Skipping Kaggle_test evaluation.\")\n",
    "else:\n",
    "    kdf = pd.read_csv(KAGGLE_TEST_PATH)\n",
    "    if \"WeightCategory\" not in kdf.columns:\n",
    "        raise KeyError(\"Kaggle_test.csv must contain 'WeightCategory'.\")\n",
    "\n",
    "    y_true = kdf[\"WeightCategory\"].copy()\n",
    "    X_k = kdf.drop(columns=[\"WeightCategory\"], errors=\"ignore\").copy()\n",
    "    if id_col and id_col in X_k.columns:\n",
    "        X_k.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "    for c in [\"MTRANS\", \"SMOKE\"]:\n",
    "        if c in X_k.columns:\n",
    "            X_k.drop(columns=[c], inplace=True)\n",
    "    X_k = add_bmi(X_k)\n",
    "\n",
    "    gender_col_k = detect_gender_column(X_k)\n",
    "    if gender_col_k is None:\n",
    "        raise ValueError(\"Could not detect a gender column in Kaggle_test.csv\")\n",
    "    age_col_k = detect_age_column(X_k)\n",
    "    if age_col_k is None:\n",
    "        raise ValueError(\"Could not detect an Age column in Kaggle_test.csv\")\n",
    "    X_k = add_age_features(X_k, age_col_k)\n",
    "\n",
    "    km_k, kf_k = split_by_gender(X_k[gender_col_k])\n",
    "    ky_k = X_k[\"AgeGroup\"] == \"<24\"\n",
    "    ko_k = ~ky_k\n",
    "\n",
    "    kaggle_pred_proba = np.zeros((len(X_k), len(classes)), dtype=np.float32)\n",
    "\n",
    "    def pick_cols_k(df):\n",
    "        use_cols = [c for c in X.columns.tolist() if c in df.columns]\n",
    "        missing = [c for c in X.columns.tolist() if c not in df.columns]\n",
    "        tmp = df[use_cols].copy()\n",
    "        for m in missing:\n",
    "            tmp[m] = np.nan\n",
    "        return tmp[X.columns.tolist()]\n",
    "\n",
    "    def infer_group(train_mask, kaggle_mask, name):\n",
    "        if train_mask.sum() == 0 or kaggle_mask.sum() == 0:\n",
    "            print(f\"[Kaggle] Skip group {name}: train={int(train_mask.sum())}, eval={int(kaggle_mask.sum())}\")\n",
    "            return\n",
    "        Xg = X.loc[train_mask, :]\n",
    "        yg = y_enc[train_mask]\n",
    "        Xkg = pick_cols_k(X_k.loc[kaggle_mask, :])\n",
    "        oof_g, pred_g = train_group_and_predict(Xg, yg, Xkg, f\"{name} (Kaggle)\")\n",
    "        kaggle_pred_proba[np.where(kaggle_mask)[0]] = pred_g\n",
    "\n",
    "    infer_group(train_male_mask & train_young_mask, km_k & ky_k, \"MALE_<24\")\n",
    "    infer_group(train_male_mask & train_old_mask,   km_k & ko_k, \"MALE_>=24\")\n",
    "    infer_group(train_female_mask & train_young_mask, kf_k & ky_k, \"FEMALE_<24\")\n",
    "    infer_group(train_female_mask & train_old_mask,   kf_k & ko_k, \"FEMALE_>=24\")\n",
    "\n",
    "    kaggle_pred_idx = np.argmax(kaggle_pred_proba, axis=1)\n",
    "    y_pred = le.inverse_transform(kaggle_pred_idx)\n",
    "\n",
    "    overall_acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"\\nâœ… Overall Accuracy on Kaggle_test: {overall_acc:.5f}\")\n",
    "\n",
    "    order = [\n",
    "        'Insufficient_Weight',\n",
    "        'Normal_Weight',\n",
    "        'Overweight_Level_I',\n",
    "        'Overweight_Level_II',\n",
    "        'Obesity_Type_I',\n",
    "        'Obesity_Type_II',\n",
    "        'Obesity_Type_III'\n",
    "    ]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=order)\n",
    "    cm_norm = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    print(\"\\n=== Confusion Matrix (counts) ===\")\n",
    "    print(\"Predicted â†’\")\n",
    "    print(\"True â†“\")\n",
    "    for i, true_class in enumerate(order):\n",
    "        row = \" | \".join(f\"{cm[i, j]:4d}\" for j in range(len(order)))\n",
    "        print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "    print(\"\\n=== Confusion Matrix (row-normalized) ===\")\n",
    "    for i, true_class in enumerate(order):\n",
    "        row = \" | \".join(f\"{cm_norm[i, j]:.2f}\" for j in range(len(order)))\n",
    "        print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "    print(\"\\n=== Per-class metrics ===\")\n",
    "    try:\n",
    "        print(classification_report(y_true, y_pred, labels=order, target_names=order, digits=4, zero_division=0))\n",
    "    except Exception as e:\n",
    "        print(f\"[Info] classification_report fallback: {e}\")\n",
    "        print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "    print(\"\\n=== Per-class accuracy (diagonal/row total) ===\")\n",
    "    for i, c in enumerate(order):\n",
    "        total = cm[i].sum()\n",
    "        correct = cm[i, i]\n",
    "        acc = correct / total if total > 0 else 0.0\n",
    "        print(f\"{c:<22} | Correct: {correct:3d} / {total:3d} | {acc*100:6.2f}%\")\n",
    "\n",
    "    print(\"\\n=== Most common confusions (true â†’ predicted) ===\")\n",
    "    pairs = []\n",
    "    for i, t in enumerate(order):\n",
    "        for j, p in enumerate(order):\n",
    "            if i == j or cm[i, j] == 0:\n",
    "                continue\n",
    "            pairs.append((cm[i, j], t, p, cm_norm[i, j]))\n",
    "    pairs = sorted(pairs, key=lambda x: (-x[0], -x[3]))\n",
    "    for cnt, true_label, pred_label, norm_val in pairs[:10]:\n",
    "        print(f\"{true_label:25} â†’ {pred_label:25} | Count: {cnt:3d} | Row%: {norm_val*100:5.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2934e77b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
