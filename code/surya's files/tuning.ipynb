{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197913be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# End-to-end (Gender-Specific Models) + BMI + Randomized Search:\n",
    "# Load → Detect ID/Target/Gender → Drop MTRANS/SCC → +BMI →\n",
    "# Split by Gender → (Per-gender) Randomized Search + 5-Fold XGB (ES) →\n",
    "# Predict test → submission.csv → Evaluate on Kaggle_test.csv\n",
    "# ==============================================\n",
    "\n",
    "# -------- Imports --------\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.base import clone\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# -------- Paths --------\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "SAMPLE_SUB_PATH = \"sample_submission.csv\"\n",
    "KAGGLE_TEST_PATH = \"Kaggle_test.csv\"   # must contain WeightCategory ground truth\n",
    "\n",
    "# -------- Seeds / folds / speed --------\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "N_JOBS = -1\n",
    "NUM_CLASSES_EXPECTED = 7\n",
    "USE_GPU = True  # <— turn this on\n",
    "\n",
    "\n",
    "# -------- Randomized Search Config --------\n",
    "N_TRIALS = 30            # increase (e.g., 60–100) for more thorough tuning\n",
    "EARLY_STOP = 200\n",
    "MAX_BOOST_ROUND = 20000\n",
    "BASE_SEED = 1337\n",
    "\n",
    "random.seed(BASE_SEED)\n",
    "np.random.seed(BASE_SEED)\n",
    "\n",
    "# Optional: GPU toggle (falls back to CPU if not available)\n",
    "USE_GPU = False\n",
    "\n",
    "# -------- Helpers --------\n",
    "def norm_col(s: str) -> str:\n",
    "    if s is None: return s\n",
    "    return str(s).replace(\"\\ufeff\", \"\").strip().lower()\n",
    "\n",
    "def build_norm_map(cols):\n",
    "    fwd = {c: norm_col(c) for c in cols}\n",
    "    rev = {}\n",
    "    for orig, n in fwd.items():\n",
    "        if n not in rev:\n",
    "            rev[n] = orig\n",
    "    return fwd, rev\n",
    "\n",
    "def find_id_and_label(sample_sub, train, test):\n",
    "    ss_fwd, ss_rev = build_norm_map(sample_sub.columns)\n",
    "    tr_fwd, tr_rev = build_norm_map(train.columns)\n",
    "    te_fwd, te_rev = build_norm_map(test.columns)\n",
    "\n",
    "    ss_norm_cols = [ss_fwd[c] for c in sample_sub.columns]\n",
    "    tr_norm_cols = [tr_fwd[c] for c in train.columns]\n",
    "    te_norm_cols = [te_fwd[c] for c in test.columns]\n",
    "\n",
    "    id_norm, label_norm = None, None\n",
    "    if len(ss_norm_cols) == 2:\n",
    "        c1, c2 = ss_norm_cols\n",
    "        if c1 in te_norm_cols and c2 not in te_norm_cols:\n",
    "            id_norm, label_norm = c1, c2\n",
    "        elif c2 in te_norm_cols and c1 not in te_norm_cols:\n",
    "            id_norm, label_norm = c2, c1\n",
    "        else:\n",
    "            if c1 in te_norm_cols and c1 in tr_norm_cols:\n",
    "                id_norm, label_norm = c1, c2\n",
    "            elif c2 in te_norm_cols and c2 in tr_norm_cols:\n",
    "                id_norm, label_norm = c2, c1\n",
    "\n",
    "    if id_norm is None:\n",
    "        for cand in [\"id\", \"row_id\", \"index\", \"sample_id\"]:\n",
    "            if cand in te_norm_cols and cand in tr_norm_cols:\n",
    "                id_norm = cand\n",
    "                break\n",
    "\n",
    "    if label_norm is None:\n",
    "        candidates = [c for c in ss_norm_cols if c != id_norm]\n",
    "        if len(candidates) == 1:\n",
    "            label_norm = candidates[0]\n",
    "\n",
    "    if label_norm is None:\n",
    "        for cand in [\"label\", \"target\", \"class\", \"y\", \"weightcategory\", \"nobeyesdad\"]:\n",
    "            if cand in tr_norm_cols and cand != id_norm:\n",
    "                label_norm = cand\n",
    "                break\n",
    "\n",
    "    if label_norm is None:\n",
    "        for c in reversed(tr_norm_cols):\n",
    "            if c != id_norm:\n",
    "                label_norm = c\n",
    "                break\n",
    "\n",
    "    return {\n",
    "        \"id_norm\": id_norm,\n",
    "        \"label_norm\": label_norm,\n",
    "        \"id_in_train\": build_norm_map(train.columns)[1].get(id_norm, None),\n",
    "        \"id_in_test\": build_norm_map(test.columns)[1].get(id_norm, None),\n",
    "        \"id_in_sample\": build_norm_map(sample_sub.columns)[1].get(id_norm, None),\n",
    "        \"label_in_train\": build_norm_map(train.columns)[1].get(label_norm, None),\n",
    "        \"label_in_sample\": build_norm_map(sample_sub.columns)[1].get(label_norm, None),\n",
    "    }\n",
    "\n",
    "def infer_feature_types(df):\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def detect_gender_column(df):\n",
    "    candidates = [c for c in df.columns if norm_col(c) in {\"gender\",\"sex\"}]\n",
    "    if candidates:\n",
    "        return candidates[0]\n",
    "    for c in df.columns:\n",
    "        vals = pd.Series(df[c].dropna().astype(str).str.lower().str.strip()).unique()\n",
    "        if len(vals) in (2, 3):\n",
    "            if any(v.startswith(\"m\") for v in vals) and any(v.startswith(\"f\") for v in vals):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def split_by_gender(series):\n",
    "    s = series.astype(str).str.lower().str.strip()\n",
    "    male_mask = s.str.startswith((\"m\",\"1\",\"true\"))\n",
    "    female_mask = s.str.startswith((\"f\",\"0\",\"false\"))\n",
    "    if male_mask.sum()==0 and female_mask.sum()==0:\n",
    "        top = s.value_counts().index.tolist()\n",
    "        if len(top)>=2:\n",
    "            male_mask = s==top[0]\n",
    "            female_mask = s==top[1]\n",
    "    return male_mask, female_mask\n",
    "\n",
    "def add_bmi(df):\n",
    "    \"\"\"Compute BMI = Weight / (Height_m^2) with robust height-unit detection.\"\"\"\n",
    "    if (\"Weight\" in df.columns) and (\"Height\" in df.columns):\n",
    "        h = pd.to_numeric(df[\"Height\"], errors=\"coerce\")\n",
    "        height_m = np.where(np.nanmedian(h) > 3.0, h / 100.0, h)\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            w = pd.to_numeric(df[\"Weight\"], errors=\"coerce\")\n",
    "            bmi = w / (np.power(height_m, 2) + 1e-12)\n",
    "        df[\"BMI\"] = pd.Series(bmi).replace([np.inf, -np.inf], np.nan)\n",
    "    return df\n",
    "\n",
    "def round_age_inplace(df):\n",
    "    if \"Age\" in df.columns:\n",
    "        df[\"Age\"] = pd.to_numeric(df[\"Age\"], errors=\"coerce\").round().astype(\"Int64\")\n",
    "\n",
    "\n",
    "# ---- Randomized Search sampling helpers ----\n",
    "def sample_uniform(a, b):\n",
    "    return a + (b - a) * random.random()\n",
    "\n",
    "def sample_int(a, b):\n",
    "    return random.randint(a, b)\n",
    "\n",
    "def sample_loguniform(a, b):\n",
    "    la, lb = math.log(a), math.log(b)\n",
    "    return math.exp(la + (lb - la) * random.random())\n",
    "\n",
    "# ---- Preprocessor factory ----\n",
    "def make_preprocessor(X_frame):\n",
    "    num_cols, cat_cols = infer_feature_types(X_frame)\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    ])\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", ohe),\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=1.0,\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "# ---- Base XGB params (CPU/GPU aware) ----\n",
    "def make_xgb_base():\n",
    "    base = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": None,        # fill later\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"nthread\": N_JOBS,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "    }\n",
    "    if USE_GPU:\n",
    "        try:\n",
    "            # GPU path (works for xgboost <2 and >=2)\n",
    "            try:\n",
    "                from packaging import version\n",
    "                if version.parse(xgb.__version__) >= version.parse(\"2.0.0\"):\n",
    "                    base.update({\"device\": \"cuda\", \"tree_method\": \"hist\"})\n",
    "                else:\n",
    "                    base.update({\"tree_method\": \"gpu_hist\", \"predictor\": \"gpu_predictor\"})\n",
    "            except Exception:\n",
    "                base.update({\"tree_method\": \"gpu_hist\"})\n",
    "            base.update({\"max_bin\": 512})\n",
    "        except Exception:\n",
    "            base.update({\"tree_method\": \"hist\"})\n",
    "    else:\n",
    "        base.update({\"tree_method\": \"hist\"})\n",
    "    return base\n",
    "\n",
    "BASE_XGB = make_xgb_base()\n",
    "\n",
    "def sample_params():\n",
    "    return {\n",
    "        \"eta\":                sample_uniform(0.02, 0.07),    # learning rate\n",
    "        \"max_depth\":          sample_int(4, 10),\n",
    "        \"min_child_weight\":   sample_int(1, 8),\n",
    "        \"subsample\":          sample_uniform(0.6, 1.0),\n",
    "        \"colsample_bytree\":   sample_uniform(0.6, 1.0),\n",
    "        \"lambda\":             sample_loguniform(1e-3, 10.0), # L2\n",
    "        \"alpha\":              sample_loguniform(1e-3, 5.0),  # L1\n",
    "        \"gamma\":              sample_uniform(0.0, 5.0),\n",
    "        # \"max_delta_step\":   sample_int(0, 3),              # optional stabilizer\n",
    "    }\n",
    "\n",
    "def cv_score_for_params(Xg, yg, params, classes, group_name):\n",
    "    preprocessor = make_preprocessor(Xg)\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    f1s = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(Xg, yg), start=1):\n",
    "        X_tr, X_va = Xg.iloc[tr_idx], Xg.iloc[va_idx]\n",
    "        y_tr, y_va = yg[tr_idx], yg[va_idx]\n",
    "\n",
    "        prep = clone(preprocessor)\n",
    "        Xtr = prep.fit_transform(X_tr)\n",
    "        Xva = prep.transform(X_va)\n",
    "\n",
    "        dtrain = xgb.DMatrix(Xtr, label=y_tr)\n",
    "        dval   = xgb.DMatrix(Xva, label=y_va)\n",
    "\n",
    "        xgb_params = dict(BASE_XGB)\n",
    "        xgb_params[\"num_class\"] = len(classes)\n",
    "        xgb_params.update(params)\n",
    "\n",
    "        bst = xgb.train(\n",
    "            params=xgb_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=MAX_BOOST_ROUND,\n",
    "            evals=[(dtrain, \"train\"), (dval, \"valid\")],\n",
    "            early_stopping_rounds=EARLY_STOP,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        proba = bst.predict(dval, iteration_range=(0, int(bst.best_iteration + 1)))\n",
    "        pred  = np.argmax(proba, axis=1)\n",
    "        f1s.append(f1_score(y_va, pred, average=\"macro\"))\n",
    "\n",
    "    return float(np.mean(f1s))\n",
    "\n",
    "def randomized_search_xgb(Xg, yg, classes, group_name, n_trials=N_TRIALS):\n",
    "    best_score = -1.0\n",
    "    best_params = None\n",
    "    for t in range(1, n_trials + 1):\n",
    "        params = sample_params()\n",
    "        score = cv_score_for_params(Xg, yg, params, classes, group_name)\n",
    "        print(f\"[TUNE {group_name}] Trial {t:02d}/{n_trials} | macroF1={score:.4f} | params={params}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "    print(f\"[TUNE {group_name}] Best macroF1={best_score:.4f}\")\n",
    "    print(f\"[TUNE {group_name}] Best params={best_params}\")\n",
    "    return best_params\n",
    "\n",
    "def train_group_and_predict_with_params(X_grp, y_enc_grp, test_grp, group_name, classes, best_params, gender_col):\n",
    "    cols_to_use = [c for c in X_grp.columns if c != gender_col]\n",
    "    Xg = X_grp[cols_to_use].copy()\n",
    "    Xtestg = test_grp[cols_to_use].copy()\n",
    "\n",
    "    preprocessor = make_preprocessor(Xg)\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    oof_group = np.zeros((len(Xg), len(classes)), dtype=np.float32)\n",
    "    test_group_pred = np.zeros((len(Xtestg), len(classes)), dtype=np.float32)\n",
    "    fold_best = []\n",
    "\n",
    "    xgb_params = dict(BASE_XGB)\n",
    "    xgb_params[\"num_class\"] = len(classes)\n",
    "    xgb_params.update(best_params)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(Xg, y_enc_grp), start=1):\n",
    "        print(f\"\\n[{group_name}] Fold {fold}/{N_FOLDS}\")\n",
    "        X_tr, X_va = Xg.iloc[tr_idx], Xg.iloc[va_idx]\n",
    "        y_tr, y_va = y_enc_grp[tr_idx], y_enc_grp[va_idx]\n",
    "\n",
    "        prep = clone(preprocessor)\n",
    "        Xtr = prep.fit_transform(X_tr)\n",
    "        Xva = prep.transform(X_va)\n",
    "\n",
    "        dtrain = xgb.DMatrix(Xtr, label=y_tr)\n",
    "        dval   = xgb.DMatrix(Xva, label=y_va)\n",
    "\n",
    "        bst = xgb.train(\n",
    "            params=xgb_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=MAX_BOOST_ROUND,\n",
    "            evals=[(dtrain, \"train\"), (dval, \"valid\")],\n",
    "            early_stopping_rounds=EARLY_STOP,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        best_round = int(bst.best_iteration + 1)\n",
    "        fold_best.append(best_round)\n",
    "        print(f\"[{group_name}] Best iteration: {best_round}\")\n",
    "\n",
    "        oof_proba = bst.predict(dval, iteration_range=(0, best_round))\n",
    "        oof_group[va_idx] = oof_proba\n",
    "\n",
    "        # test preds\n",
    "        Xtest_tf = prep.transform(Xtestg)\n",
    "        dtest = xgb.DMatrix(Xtest_tf)\n",
    "        test_group_pred += bst.predict(dtest, iteration_range=(0, best_round)) / N_FOLDS\n",
    "\n",
    "    oof_labels = np.argmax(oof_group, axis=1)\n",
    "    acc_g = accuracy_score(y_enc_grp, oof_labels)\n",
    "    f1_g = f1_score(y_enc_grp, oof_labels, average=\"macro\")\n",
    "    print(f\"\\n[{group_name}] OOF Accuracy: {acc_g:.4f} | Macro F1: {f1_g:.4f}\")\n",
    "    print(f\"[{group_name}] Best iterations: {fold_best} | Median: {int(np.median(fold_best))}\")\n",
    "\n",
    "    return oof_group, test_group_pred\n",
    "\n",
    "# -------- Load data --------\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "# Drop not-used columns\n",
    "for c in [\"MTRANS\",\"SCC\"]:\n",
    "    if c in train.columns: train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:  test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# Add BMI feature\n",
    "train = add_bmi(train)\n",
    "test  = add_bmi(test)\n",
    "\n",
    "# after: train = add_bmi(train); test = add_bmi(test)\n",
    "round_age_inplace(train)\n",
    "round_age_inplace(test)\n",
    "\n",
    "\n",
    "# Detect headers\n",
    "info = find_id_and_label(sample_sub, train, test)\n",
    "ID_COL_TRAIN   = info[\"id_in_train\"]\n",
    "ID_COL_TEST    = info[\"id_in_test\"]\n",
    "ID_COL_SAMPLE  = info[\"id_in_sample\"]\n",
    "TARGET_COL     = info[\"label_in_train\"]\n",
    "LABEL_COL_SAMP = info[\"label_in_sample\"]\n",
    "\n",
    "if TARGET_COL is None:\n",
    "    raise ValueError(\"Could not detect the target column. Ensure sample_submission and train headers align.\")\n",
    "if LABEL_COL_SAMP is None:\n",
    "    ss_cols = list(sample_sub.columns)\n",
    "    others = [c for c in ss_cols if c != ID_COL_SAMPLE]\n",
    "    if len(others) == 1:\n",
    "        LABEL_COL_SAMP = others[0]\n",
    "    else:\n",
    "        raise ValueError(\"Could not detect label header in sample_submission.csv\")\n",
    "\n",
    "print(f\"[Detected] Target in train: '{TARGET_COL}', Label in sample_sub: '{LABEL_COL_SAMP}'\")\n",
    "if ID_COL_TRAIN and ID_COL_TEST:\n",
    "    print(f\"[Detected] ID in train: '{ID_COL_TRAIN}', ID in test: '{ID_COL_TEST}'\")\n",
    "\n",
    "# Target / Features\n",
    "y = train[TARGET_COL].copy()\n",
    "X = train.drop(columns=[TARGET_COL]).copy()\n",
    "if ID_COL_TRAIN in X.columns:\n",
    "    X.drop(columns=[ID_COL_TRAIN], inplace=True)\n",
    "\n",
    "test_features = test.copy()\n",
    "if ID_COL_TEST in test_features.columns:\n",
    "    test_ids = test_features[ID_COL_TEST].copy()\n",
    "    test_features.drop(columns=[ID_COL_TEST], inplace=True)\n",
    "else:\n",
    "    test_ids = pd.Series(np.arange(len(test_features)), name=\"id\")\n",
    "\n",
    "# Label encode target\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "classes = list(le.classes_)\n",
    "if len(classes) != NUM_CLASSES_EXPECTED:\n",
    "    print(f\"[Warn] Expected {NUM_CLASSES_EXPECTED} classes but found {len(classes)}. Proceeding.\")\n",
    "\n",
    "# Detect gender & split\n",
    "gender_col = detect_gender_column(pd.concat([X, test_features], axis=0))\n",
    "if gender_col is None:\n",
    "    raise ValueError(\"Could not detect a gender column (e.g., 'Gender' or 'SEX').\")\n",
    "\n",
    "male_mask, female_mask = split_by_gender(train[gender_col])\n",
    "test_male_mask, test_female_mask = split_by_gender(test_features[gender_col])\n",
    "print(f\"[Info] Train male rows: {int(male_mask.sum())} | female rows: {int(female_mask.sum())}\")\n",
    "print(f\"[Info] Test  male rows: {int(test_male_mask.sum())} | female rows: {int(test_female_mask.sum())}\")\n",
    "\n",
    "# Per-gender frames\n",
    "X_male = X[male_mask].reset_index(drop=True)\n",
    "y_male = y_enc[male_mask]\n",
    "X_female = X[female_mask].reset_index(drop=True)\n",
    "y_female = y_enc[female_mask]\n",
    "test_male = test_features[test_male_mask].reset_index(drop=True)\n",
    "test_female = test_features[test_female_mask].reset_index(drop=True)\n",
    "\n",
    "# ---- Randomized search per gender ----\n",
    "print(\"\\n=== Randomized Search: MALE ===\")\n",
    "best_params_male = randomized_search_xgb(X_male, y_male, classes, \"MALE\", n_trials=N_TRIALS)\n",
    "print(\"\\n=== Randomized Search: FEMALE ===\")\n",
    "best_params_female = randomized_search_xgb(X_female, y_female, classes, \"FEMALE\", n_trials=N_TRIALS)\n",
    "\n",
    "# ---- Train per gender using tuned params ----\n",
    "male_oof, male_test_pred = train_group_and_predict_with_params(\n",
    "    X_male, y_male, test_male, \"MALE\", classes, best_params_male, gender_col\n",
    ")\n",
    "female_oof, female_test_pred = train_group_and_predict_with_params(\n",
    "    X_female, y_female, test_female, \"FEMALE\", classes, best_params_female, gender_col\n",
    ")\n",
    "\n",
    "# ---- Combine OOF, print overall ----\n",
    "oof_full = np.zeros((len(X), len(classes)), dtype=np.float32)\n",
    "oof_full[male_mask.values] = male_oof\n",
    "oof_full[female_mask.values] = female_oof\n",
    "\n",
    "oof_labels = np.argmax(oof_full, axis=1)\n",
    "oof_acc = accuracy_score(y_enc, oof_labels)\n",
    "oof_f1 = f1_score(y_enc, oof_labels, average=\"macro\")\n",
    "print(\"\\n========== OVERALL OOF ==========\")\n",
    "print(f\"OOF Accuracy: {oof_acc:.4f} | OOF Macro F1: {oof_f1:.4f}\")\n",
    "try:\n",
    "    print(\"\\nOOF Classification Report:\\n\",\n",
    "          classification_report(y_enc, oof_labels, target_names=classes, zero_division=0))\n",
    "except Exception as e:\n",
    "    print(f\"[Info] Could not print classification report: {e}\")\n",
    "\n",
    "# ---- Predict test & build submission ----\n",
    "test_pred_proba = np.zeros((len(test_features), len(classes)), dtype=np.float32)\n",
    "test_pred_proba[test_male_mask.values] = male_test_pred\n",
    "test_pred_proba[test_female_mask.values] = female_test_pred\n",
    "test_pred_int = np.argmax(test_pred_proba, axis=1)\n",
    "test_pred_labels = le.inverse_transform(test_pred_int)\n",
    "\n",
    "ss_cols = list(sample_sub.columns)\n",
    "ID_HEADER = ID_COL_SAMPLE if (ID_COL_SAMPLE in sample_sub.columns) else None\n",
    "LABEL_HEADER = LABEL_COL_SAMP\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "if ID_HEADER is not None and (ID_COL_TEST in test.columns):\n",
    "    sub[ID_HEADER] = test[ID_COL_TEST].values\n",
    "elif ID_HEADER is not None:\n",
    "    sub[ID_HEADER] = np.arange(len(test_features))\n",
    "sub[LABEL_HEADER] = test_pred_labels\n",
    "\n",
    "for c in ss_cols:\n",
    "    if c not in sub.columns:\n",
    "        sub[c] = sample_sub[c].iloc[0] if len(sample_sub[c]) else None\n",
    "sub = sub[ss_cols]\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSaved submission.csv\")\n",
    "print(sub.head(10))\n",
    "\n",
    "# ==============================================\n",
    "# Evaluate on Kaggle_test.csv (with ground truth)\n",
    "# ==============================================\n",
    "if not os.path.exists(KAGGLE_TEST_PATH):\n",
    "    raise FileNotFoundError(f\"{KAGGLE_TEST_PATH} not found\")\n",
    "\n",
    "kdf = pd.read_csv(KAGGLE_TEST_PATH)\n",
    "if \"WeightCategory\" not in kdf.columns:\n",
    "    raise KeyError(\"Kaggle_test.csv must contain 'WeightCategory' as ground truth.\")\n",
    "\n",
    "y_true = kdf[\"WeightCategory\"].copy()\n",
    "X_k = kdf.drop(columns=[\"WeightCategory\"], errors=\"ignore\").copy()\n",
    "\n",
    "# drop same cols + add BMI (align to train processing)\n",
    "for c in [\"MTRANS\",\"SCC\"]:\n",
    "    if c in X_k.columns: X_k.drop(columns=[c], inplace=True)\n",
    "X_k = add_bmi(X_k)\n",
    "round_age_inplace(X_k)\n",
    "\n",
    "\n",
    "# detect gender on Kaggle_test\n",
    "gender_col_k = detect_gender_column(X_k)\n",
    "if gender_col_k is None:\n",
    "    raise ValueError(\"Could not detect a gender column in Kaggle_test.csv\")\n",
    "km_k, kf_k = split_by_gender(X_k[gender_col_k])\n",
    "\n",
    "# Helper: train on full group & infer on Kaggle subset using tuned params\n",
    "def train_full_and_predict_with_params(X_full, y_full, X_eval, name, params):\n",
    "    if len(X_eval) == 0 or len(X_full) == 0:\n",
    "        return np.zeros((len(X_eval), len(classes)), dtype=np.float32)\n",
    "\n",
    "    cols_to_use = [c for c in X_full.columns if c != gender_col]\n",
    "    Xf = X_full[cols_to_use].copy()\n",
    "    Xe = X_eval[cols_to_use].copy()\n",
    "\n",
    "    preprocessor = make_preprocessor(Xf)\n",
    "    prep = clone(preprocessor)\n",
    "    Xtr = prep.fit_transform(Xf)\n",
    "    Xev = prep.transform(Xe)\n",
    "\n",
    "    xgb_params = dict(BASE_XGB)\n",
    "    xgb_params[\"num_class\"] = len(classes)\n",
    "    xgb_params.update(params)\n",
    "\n",
    "    dtrain = xgb.DMatrix(Xtr, label=y_full)\n",
    "    dtest  = xgb.DMatrix(Xev)\n",
    "\n",
    "    bst = xgb.train(\n",
    "        params=xgb_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=MAX_BOOST_ROUND,\n",
    "        evals=[(dtrain, \"train\")],\n",
    "        early_stopping_rounds=EARLY_STOP,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    best_round = int(bst.best_iteration + 1)\n",
    "    proba = bst.predict(dtest, iteration_range=(0, best_round))\n",
    "    print(f\"[Kaggle] Trained {name} on {Xf.shape[0]} rows; infer {Xev.shape[0]} rows. Best iters={best_round}\")\n",
    "    return proba\n",
    "\n",
    "# split Kaggle set by gender\n",
    "Xk_male   = X_k.loc[km_k].reset_index(drop=True)\n",
    "Xk_female = X_k.loc[kf_k].reset_index(drop=True)\n",
    "\n",
    "# train on full train-group and predict Kaggle subsets\n",
    "kaggle_pred_proba = np.zeros((len(X_k), len(classes)), dtype=np.float32)\n",
    "kaggle_pred_proba[km_k.values] = train_full_and_predict_with_params(X_male, y_male, Xk_male, \"MALE (full)\", best_params_male)\n",
    "kaggle_pred_proba[kf_k.values] = train_full_and_predict_with_params(X_female, y_female, Xk_female, \"FEMALE (full)\", best_params_female)\n",
    "\n",
    "kaggle_pred_idx = np.argmax(kaggle_pred_proba, axis=1)\n",
    "y_pred = le.inverse_transform(kaggle_pred_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac0d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Accuracy & confusion matrix --------\n",
    "overall_acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\n✅ Overall Accuracy on Kaggle_test: {overall_acc:.5f}\")\n",
    "\n",
    "order = [\n",
    "    'Insufficient_Weight',\n",
    "    'Normal_Weight',\n",
    "    'Overweight_Level_I',\n",
    "    'Overweight_Level_II',\n",
    "    'Obesity_Type_I',\n",
    "    'Obesity_Type_II',\n",
    "    'Obesity_Type_III'\n",
    "]\n",
    "cm = confusion_matrix(y_true, y_pred, labels=order)\n",
    "cm_norm = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (counts) ===\")\n",
    "print(\"Predicted →\")\n",
    "print(\"True ↓\")\n",
    "for i, true_class in enumerate(order):\n",
    "    row = \" | \".join(f\"{cm[i, j]:4d}\" for j in range(len(order)))\n",
    "    print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (row-normalized) ===\")\n",
    "for i, true_class in enumerate(order):\n",
    "    row = \" | \".join(f\"{cm_norm[i, j]:.2f}\" for j in range(len(order)))\n",
    "    print(f\"{true_class:<22}: {row}\")\n",
    "\n",
    "print(\"\\n=== Per-class metrics ===\")\n",
    "try:\n",
    "    print(classification_report(y_true, y_pred, labels=order, target_names=order, digits=4, zero_division=0))\n",
    "except Exception as e:\n",
    "    print(f\"[Info] classification_report fallback: {e}\")\n",
    "    print(classification_report(y_true, y_pred, digits=4, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166399cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
