{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1773cf-38ad-47ca-84c3-b38193520458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Gender-split XGBoost + BMI + OHE + EarlyStopping\n",
    "# Optuna CV + CLASS WEIGHTS for Overweight I & II\n",
    "# GPU-enabled (gpu_hist) with safe CPU fallback\n",
    "# Prints trial/fold progress and GPU usage\n",
    "# Uses: train_combined.csv  → submission.csv\n",
    "# ==============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from contextlib import suppress\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "TRAIN_PATH = \"train_combined.csv\"\n",
    "TEST_PATH  = \"test.csv\"\n",
    "SAMPLE_SUB_PATH = \"sample_submission.csv\"\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_JOBS = -1\n",
    "N_FOLDS = 5\n",
    "EARLY_STOP = 200\n",
    "\n",
    "# Tuning budget\n",
    "TRIALS_MALE = 30\n",
    "TRIALS_FEMALE = 30\n",
    "\n",
    "# Multi-seed CV\n",
    "CV_SEEDS = [42, 2027, 1337]\n",
    "\n",
    "# >>> Class weights for Overweight classes <<<\n",
    "W_OVR1 = 1.6   # Overweight Level I\n",
    "W_OVR2 = 1.6   # Overweight Level II\n",
    "\n",
    "# >>> Progress / logging <<<\n",
    "PRINT_EVERY = 200          # print eval metric every N boosting rounds (set 0 to silence)\n",
    "SHOW_PARAMS_PER_TRIAL = True\n",
    "\n",
    "# >>> Prefer GPU if available <<<\n",
    "USE_GPU = True  # set False to force CPU\n",
    "\n",
    "GPU_STATUS = {\"requested\": USE_GPU, \"used_any\": False}\n",
    "\n",
    "def want_gpu_params():\n",
    "    # device is honored by XGBoost 2.x; ignored by older\n",
    "    return {\"tree_method\": \"gpu_hist\", \"predictor\": \"gpu_predictor\", \"device\": \"cuda\"}\n",
    "\n",
    "def apply_device(params, want_gpu=True):\n",
    "    p = dict(params)\n",
    "    if want_gpu:\n",
    "        p.update(want_gpu_params())\n",
    "    else:\n",
    "        p.update({\"tree_method\": \"hist\", \"predictor\": \"auto\"})\n",
    "    return p\n",
    "\n",
    "def make_dmatrix(X, y=None, weight=None, use_gpu=False):\n",
    "    \"\"\"Use fast DeviceQuantileDMatrix on GPU, fallback to classic DMatrix.\"\"\"\n",
    "    if use_gpu:\n",
    "        with suppress(Exception):\n",
    "            return xgb.DeviceQuantileDMatrix(X, label=y, weight=weight)\n",
    "    return xgb.DMatrix(X, label=y, weight=weight)\n",
    "\n",
    "def has_gpu(params):\n",
    "    return params.get(\"tree_method\") == \"gpu_hist\"\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def add_bmi(df):\n",
    "    if (\"Weight\" in df.columns) and (\"Height\" in df.columns):\n",
    "        h = df[\"Height\"].astype(float)\n",
    "        hm = np.where(h.median() > 3.0, h/100.0, h)\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            bmi = df[\"Weight\"].astype(float) / (hm**2 + 1e-12)\n",
    "        df[\"BMI\"] = pd.Series(bmi).replace([np.inf, -np.inf], np.nan).clip(10, 80)\n",
    "    return df\n",
    "\n",
    "def make_preprocessor(num_cols, cat_cols):\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", ohe),\n",
    "    ])\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, num_cols),\n",
    "            (\"cat\", cat_pipe, cat_cols)\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=1.0\n",
    "    )\n",
    "\n",
    "def detect_gender(df):\n",
    "    for c in df.columns:\n",
    "        if c.lower() in (\"gender\",\"sex\"):\n",
    "            return c\n",
    "    raise ValueError(\"Could not detect gender column (expected Gender or Sex).\")\n",
    "\n",
    "def infer_cols(df):\n",
    "    cat_cols = df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def norm_label(s: str) -> str:\n",
    "    s = str(s)\n",
    "    return \"\".join(ch for ch in s.lower() if ch.isalnum())\n",
    "\n",
    "OVERWEIGHT1_ALIASES = {\n",
    "    \"overweightleveli\",\"overweightlevel1\",\"overweight_level_i\",\"overweighti\"\n",
    "}\n",
    "OVERWEIGHT2_ALIASES = {\n",
    "    \"overweightlevelii\",\"overweightlevel2\",\"overweight_level_ii\",\"overweightii\"\n",
    "}\n",
    "\n",
    "def build_class_weights(y_series: pd.Series, w1=W_OVR1, w2=W_OVR2):\n",
    "    w = np.ones(len(y_series), dtype=np.float32)\n",
    "    y_norm = y_series.astype(str).map(norm_label)\n",
    "    w[(y_norm.isin(OVERWEIGHT1_ALIASES)).values] = float(w1)\n",
    "    w[(y_norm.isin(OVERWEIGHT2_ALIASES)).values] = float(w2)\n",
    "    return w\n",
    "\n",
    "def suggest_params(trial):\n",
    "    return {\n",
    "        \"eta\": trial.suggest_float(\"eta\", 0.02, 0.06, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 8),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 4.5),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.75, 0.95),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.70, 0.95),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.8, 2.5, log=True),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 0.6),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 0.4),\n",
    "        \"max_delta_step\": trial.suggest_float(\"max_delta_step\", 0.0, 3.0),\n",
    "        \"num_boost_round\": trial.suggest_int(\"num_boost_round\", 2000, 12000, step=1000),\n",
    "    }\n",
    "\n",
    "def _verbose_eval():\n",
    "    # return False to silence; or integer N to print every N rounds\n",
    "    return False if PRINT_EVERY <= 0 else PRINT_EVERY\n",
    "\n",
    "# ---------------- Tuning & Training (uses weights) ----------------\n",
    "def tune_group(Xg, yg_enc, y_labels_grp, label, num_class, trials):\n",
    "    num_cols, cat_cols = infer_cols(Xg)\n",
    "    weights_grp = build_class_weights(y_labels_grp)\n",
    "\n",
    "    def objective(trial):\n",
    "        hp = suggest_params(trial)\n",
    "        if SHOW_PARAMS_PER_TRIAL:\n",
    "            print(f\"\\n[{label}] Trial {trial.number+1}/{trials} | \"\n",
    "                  f\"eta={hp['eta']:.4f}, depth={hp['max_depth']}, \"\n",
    "                  f\"mcw={hp['min_child_weight']:.2f}, subsample={hp['subsample']:.2f}, \"\n",
    "                  f\"colsample={hp['colsample_bytree']:.2f}, rounds={hp['num_boost_round']}\")\n",
    "\n",
    "        scores_across_seeds = []\n",
    "        base_params = {\n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"num_class\": num_class,\n",
    "            \"eval_metric\": \"mlogloss\",\n",
    "            \"eta\": hp[\"eta\"],\n",
    "            \"max_depth\": int(hp[\"max_depth\"]),\n",
    "            \"min_child_weight\": float(hp[\"min_child_weight\"]),\n",
    "            \"subsample\": float(hp[\"subsample\"]),\n",
    "            \"colsample_bytree\": float(hp[\"colsample_bytree\"]),\n",
    "            \"reg_lambda\": float(hp[\"reg_lambda\"]),\n",
    "            \"reg_alpha\": float(hp[\"reg_alpha\"]),\n",
    "            \"gamma\": float(hp[\"gamma\"]),\n",
    "            \"max_delta_step\": float(hp[\"max_delta_step\"]),\n",
    "            \"nthread\": N_JOBS,\n",
    "            \"verbosity\": 0,\n",
    "        }\n",
    "\n",
    "        for cv_seed in CV_SEEDS:\n",
    "            skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=cv_seed)\n",
    "            fold_scores = []\n",
    "\n",
    "            for fold_id, (tr_idx, va_idx) in enumerate(skf.split(Xg, yg_enc), start=1):\n",
    "                print(f\"[{label}] Trial {trial.number+1}/{trials} • Seed {cv_seed} • Fold {fold_id}/{N_FOLDS}\")\n",
    "                X_tr, X_va = Xg.iloc[tr_idx], Xg.iloc[va_idx]\n",
    "                y_tr, y_va = yg_enc[tr_idx], yg_enc[va_idx]\n",
    "                w_tr = weights_grp[tr_idx]\n",
    "\n",
    "                pre = make_preprocessor(num_cols, cat_cols)\n",
    "                Xtr = pre.fit_transform(X_tr)\n",
    "                Xva = pre.transform(X_va)\n",
    "\n",
    "                params_try = apply_device(base_params, want_gpu=USE_GPU)\n",
    "                use_gpu = has_gpu(params_try)\n",
    "\n",
    "                dtrain = make_dmatrix(Xtr, y_tr, w_tr, use_gpu)\n",
    "                dvalid = make_dmatrix(Xva, y_va, None, use_gpu)\n",
    "\n",
    "                try:\n",
    "                    bst = xgb.train(\n",
    "                        params=params_try,\n",
    "                        dtrain=dtrain,\n",
    "                        num_boost_round=hp[\"num_boost_round\"],\n",
    "                        evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "                        early_stopping_rounds=EARLY_STOP,\n",
    "                        verbose_eval=_verbose_eval()\n",
    "                    )\n",
    "                    if use_gpu and not GPU_STATUS[\"used_any\"]:\n",
    "                        GPU_STATUS[\"used_any\"] = True\n",
    "                        print(f\"[{label}] ✅ GPU is being used (tree_method=gpu_hist, predictor=gpu_predictor).\")\n",
    "                except xgb.core.XGBoostError as e:\n",
    "                    if USE_GPU:\n",
    "                        print(f\"[{label}] ⚠️ GPU unavailable → CPU fallback. Reason: {e}\")\n",
    "                    params_cpu = apply_device(base_params, want_gpu=False)\n",
    "                    dtrain = make_dmatrix(Xtr, y_tr, w_tr, use_gpu=False)\n",
    "                    dvalid = make_dmatrix(Xva, y_va, None, use_gpu=False)\n",
    "                    bst = xgb.train(\n",
    "                        params=params_cpu,\n",
    "                        dtrain=dtrain,\n",
    "                        num_boost_round=hp[\"num_boost_round\"],\n",
    "                        evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "                        early_stopping_rounds=EARLY_STOP,\n",
    "                        verbose_eval=_verbose_eval()\n",
    "                    )\n",
    "\n",
    "                pred_va = bst.predict(dvalid, iteration_range=(0, bst.best_iteration + 1))\n",
    "                y_hat = np.argmax(pred_va, axis=1)\n",
    "                acc = accuracy_score(y_va, y_hat)\n",
    "                fold_scores.append(acc)\n",
    "                print(f\"[{label}]   Fold {fold_id} Acc={acc:.5f} | best_iter={bst.best_iteration+1}\")\n",
    "\n",
    "            mean_seed_score = float(np.mean(fold_scores))\n",
    "            scores_across_seeds.append(mean_seed_score)\n",
    "            trial.report(np.mean(scores_across_seeds), step=len(scores_across_seeds))\n",
    "\n",
    "        mean_score = float(np.mean(scores_across_seeds))\n",
    "        print(f\"[{label}] Trial {trial.number+1} mean Acc over seeds: {mean_score:.5f}\")\n",
    "        return mean_score\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    print(f\"\\n[{label}] Robust tuning (weighted): {trials} trials × {len(CV_SEEDS)} seeds × {N_FOLDS}-fold\")\n",
    "    print(f\"[INFO] XGBoost {xgb.__version__} | GPU requested: {USE_GPU}\")\n",
    "    study.optimize(objective, n_trials=trials, show_progress_bar=True)\n",
    "\n",
    "    print(f\"[{label}] Best CV Acc: {study.best_value:.5f}\")\n",
    "    print(f\"[{label}] Best params:\\n{study.best_params}\")\n",
    "    return study.best_params\n",
    "\n",
    "def train_cv_predict(Xg, yg_enc, y_labels_grp, Xtestg, params, num_class, label):\n",
    "    num_cols, cat_cols = infer_cols(Xg)\n",
    "    weights_grp = build_class_weights(y_labels_grp)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof = np.zeros((len(Xg), num_class), dtype=np.float32)\n",
    "    test_pred = np.zeros((len(Xtestg), num_class), dtype=np.float32)\n",
    "    best_iters = []\n",
    "\n",
    "    base_params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": num_class,\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"nthread\": N_JOBS,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "        \"verbosity\": 0,\n",
    "        \"eta\": float(params[\"eta\"]),\n",
    "        \"max_depth\": int(params[\"max_depth\"]),\n",
    "        \"min_child_weight\": float(params[\"min_child_weight\"]),\n",
    "        \"subsample\": float(params[\"subsample\"]),\n",
    "        \"colsample_bytree\": float(params[\"colsample_bytree\"]),\n",
    "        \"reg_lambda\": float(params[\"reg_lambda\"]),\n",
    "        \"reg_alpha\": float(params[\"reg_alpha\"]),\n",
    "        \"gamma\": float(params[\"gamma\"]),\n",
    "        \"max_delta_step\": float(params[\"max_delta_step\"]),\n",
    "    }\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(Xg, yg_enc), start=1):\n",
    "        print(f\"[{label}] Final CV • Fold {fold}/{N_FOLDS}\")\n",
    "        X_tr, X_va = Xg.iloc[tr_idx], Xg.iloc[va_idx]\n",
    "        y_tr, y_va = yg_enc[tr_idx], yg_enc[va_idx]\n",
    "        w_tr = weights_grp[tr_idx]\n",
    "\n",
    "        pre = make_preprocessor(num_cols, cat_cols)\n",
    "        Xtr = pre.fit_transform(X_tr)\n",
    "        Xva = pre.transform(X_va)\n",
    "        Xte = pre.transform(Xtestg)\n",
    "\n",
    "        params_try = apply_device(base_params, want_gpu=USE_GPU)\n",
    "        use_gpu = has_gpu(params_try)\n",
    "\n",
    "        dtrain = make_dmatrix(Xtr, y_tr, w_tr, use_gpu)\n",
    "        dvalid = make_dmatrix(Xva, y_va, None, use_gpu)\n",
    "        dtest  = make_dmatrix(Xte, None, None, use_gpu)\n",
    "\n",
    "        try:\n",
    "            bst = xgb.train(\n",
    "                params=params_try,\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=int(params[\"num_boost_round\"]),\n",
    "                evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "                early_stopping_rounds=EARLY_STOP,\n",
    "                verbose_eval=_verbose_eval()\n",
    "            )\n",
    "            if use_gpu and not GPU_STATUS[\"used_any\"]:\n",
    "                GPU_STATUS[\"used_any\"] = True\n",
    "                print(f\"[{label}] ✅ GPU is being used (tree_method=gpu_hist, predictor=gpu_predictor).\")\n",
    "        except xgb.core.XGBoostError as e:\n",
    "            if USE_GPU:\n",
    "                print(f\"[{label}] ⚠️ GPU unavailable → CPU fallback. Reason: {e}\")\n",
    "            params_cpu = apply_device(base_params, want_gpu=False)\n",
    "            dtrain = make_dmatrix(Xtr, y_tr, w_tr, use_gpu=False)\n",
    "            dvalid = make_dmatrix(Xva, y_va, None, use_gpu=False)\n",
    "            dtest  = make_dmatrix(Xte, None, None, use_gpu=False)\n",
    "            bst = xgb.train(\n",
    "                params=params_cpu,\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=int(params[\"num_boost_round\"]),\n",
    "                evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "                early_stopping_rounds=EARLY_STOP,\n",
    "                verbose_eval=_verbose_eval()\n",
    "            )\n",
    "\n",
    "        best_iters.append(int(bst.best_iteration + 1))\n",
    "        oof[va_idx] = bst.predict(dvalid, iteration_range=(0, bst.best_iteration + 1))\n",
    "        test_pred += bst.predict(dtest, iteration_range=(0, bst.best_iteration + 1)) / N_FOLDS\n",
    "\n",
    "        print(f\"[{label}]   Fold {fold} best_iter={best_iters[-1]}\")\n",
    "\n",
    "    y_oof = np.argmax(oof, axis=1)\n",
    "    print(f\"[{label}] OOF Acc: {accuracy_score(yg_enc, y_oof):.5f} | \"\n",
    "          f\"OOF F1: {f1_score(yg_enc, y_oof, average='macro'):.5f} | \"\n",
    "          f\"median best_iter: {int(np.median(best_iters))}\")\n",
    "    return test_pred\n",
    "\n",
    "# ==============================================================\n",
    "# Load & Prepare\n",
    "# ==============================================================\n",
    "\n",
    "print(f\"[INFO] XGBoost version: {xgb.__version__} | USE_GPU={USE_GPU}\")\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "# Drop columns you don’t want\n",
    "for c in [\"MTRANS\",\"SMOKE\"]:\n",
    "    if c in train.columns: train.drop(columns=[c], inplace=True)\n",
    "    if c in test.columns:  test.drop(columns=[c], inplace=True)\n",
    "\n",
    "# Add BMI\n",
    "train = add_bmi(train)\n",
    "test  = add_bmi(test)\n",
    "\n",
    "# Detect ID/Target\n",
    "ID_COL, TARGET_COL = sample_sub.columns[0], sample_sub.columns[1]\n",
    "\n",
    "y = train[TARGET_COL].copy()\n",
    "X = train.drop(columns=[TARGET_COL, ID_COL], errors=\"ignore\")\n",
    "test_ids = test[ID_COL]\n",
    "test_X = test.drop(columns=[ID_COL], errors=\"ignore\")\n",
    "\n",
    "# Label encode target\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "classes = le.classes_\n",
    "num_class = len(classes)\n",
    "\n",
    "# Gender split\n",
    "gender_col = detect_gender(X)\n",
    "male_mask   = X[gender_col].astype(str).str.lower().str.startswith((\"m\"))\n",
    "female_mask = X[gender_col].astype(str).str.lower().str.startswith((\"f\"))\n",
    "test_male_mask   = test_X[gender_col].astype(str).str.lower().str.startswith((\"m\"))\n",
    "test_female_mask = test_X[gender_col].astype(str).str.lower().str.startswith((\"f\"))\n",
    "\n",
    "print(f\"[INFO] Train: males={male_mask.sum()}, females={female_mask.sum()}\")\n",
    "print(f\"[INFO] Test:  males={test_male_mask.sum()}, females={test_female_mask.sum()}\")\n",
    "\n",
    "# Split: X, y (encoded), AND the original string labels for weight computation\n",
    "X_male     = X[male_mask].reset_index(drop=True)\n",
    "y_male_enc = y_enc[male_mask]\n",
    "y_male_lbl = y[male_mask].reset_index(drop=True)\n",
    "\n",
    "X_female     = X[female_mask].reset_index(drop=True)\n",
    "y_female_enc = y_enc[female_mask]\n",
    "y_female_lbl = y[female_mask].reset_index(drop=True)\n",
    "\n",
    "test_male   = test_X[test_male_mask].reset_index(drop=True)\n",
    "test_female = test_X[test_female_mask].reset_index(drop=True)\n",
    "\n",
    "# ==============================================================\n",
    "# Tune (weighted) per gender\n",
    "# ==============================================================\n",
    "\n",
    "best_male   = tune_group(X_male, y_male_enc, y_male_lbl, \"MALE\",   num_class, TRIALS_MALE)\n",
    "best_female = tune_group(X_female, y_female_enc, y_female_lbl, \"FEMALE\", num_class, TRIALS_FEMALE)\n",
    "\n",
    "# ==============================================================\n",
    "# Train with best params and predict (5-fold avg, weighted)\n",
    "# ==============================================================\n",
    "\n",
    "pred_male   = train_cv_predict(X_male, y_male_enc, y_male_lbl, test_male,   best_male,   num_class, \"MALE\")\n",
    "pred_female = train_cv_predict(X_female, y_female_enc, y_female_lbl, test_female, best_female, num_class, \"FEMALE\")\n",
    "\n",
    "# Merge predictions and build submission\n",
    "final_proba = np.zeros((len(test_X), num_class), dtype=np.float32)\n",
    "final_proba[test_male_mask.values]   = pred_male\n",
    "final_proba[test_female_mask.values] = pred_female\n",
    "final_pred = le.inverse_transform(np.argmax(final_proba, axis=1))\n",
    "\n",
    "sub = pd.DataFrame({ID_COL: test_ids, TARGET_COL: final_pred})\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSaved submission.csv ✅\")\n",
    "print(sub.head())\n",
    "\n",
    "# Final GPU summary\n",
    "if GPU_STATUS[\"requested\"]:\n",
    "    print(f\"\\n[SUMMARY] GPU requested ✔ | GPU actually used at least once: {GPU_STATUS['used_any']}\")\n",
    "else:\n",
    "    print(\"\\n[SUMMARY] GPU not requested (forced CPU).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c238656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2a862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
